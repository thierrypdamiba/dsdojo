{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Maximal Marginal Relevance (MMR) Reranking\n",
    "\n",
    "## 🎯 Objectives\n",
    "\n",
    "In this notebook, you'll learn:\n",
    "- What Maximal Marginal Relevance (MMR) is and why it matters\n",
    "- How to implement MMR reranking to reduce redundancy\n",
    "- When MMR helps vs when it can hurt search quality\n",
    "- How to tune the λ parameter for different query types\n",
    "- Measuring diversity vs relevance trade-offs\n",
    "- Applying MMR to both dense and hybrid search results\n",
    "\n",
    "## 📋 Prerequisites\n",
    "\n",
    "- Collection from Notebook 2 (hybrid search)\n",
    "- Understanding of vector similarity and search results\n",
    "- Basic linear algebra (cosine similarity, vector operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "from utils import (\n",
    "    get_qdrant_client, create_sample_dataset, search_dense, \n",
    "    search_hybrid_fusion, mmr_rerank, print_search_results,\n",
    "    calculate_redundancy, print_system_info\n",
    ")\n",
    "\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "print_system_info()\n",
    "print(\"\\n🎯 MMR Reranking Workshop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📦 Install Dependencies (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if running in a fresh environment\n",
    "# !pip install qdrant-client numpy pandas matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚙️ Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "COLLECTION_NAME = \"workshop_hybrid\"  # Reusing from Notebook 2\n",
    "BACKUP_COLLECTION = \"workshop_fundamentals\"  # Fallback from Notebook 1\n",
    "\n",
    "# Connect to Qdrant\n",
    "client = get_qdrant_client()\n",
    "\n",
    "# Ensure collection exists and has data\n",
    "try:\n",
    "    info = client.get_collection(active_collection)\n",
    "except Exception:\n",
    "    info = None\n",
    "\n",
    "needs_population = False\n",
    "if info is None:\n",
    "    needs_population = True\n",
    "else:\n",
    "    print(\"❌ No suitable collection found. Please run Notebook 1 or 2 first.\")\n",
    "    raise Exception(\"Collection not found\")\n",
    "\n",
    "if needs_population:\n",
    "    print(f\"✅ Creating and populating demo collection: {active_collection}\")\n",
    "    ensure_collection(\n",
    "        client,\n",
    "        active_collection,\n",
    "        VectorParams(size=384, distance=Distance.COSINE),\n",
    "        force_recreate=True,\n",
    "    )\n",
    "    df = create_sample_dataset(size=200, seed=42)\n",
    "    vectors = np.random.randn(len(df), 384)\n",
    "    vectors = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    upsert_points_batch(\n",
    "        client,\n",
    "        active_collection,\n",
    "        df,\n",
    "        vectors,\n",
    "        [\"text\", \"category\", \"lang\", \"timestamp\"],\n",
    "        batch_size=100,\n",
    "    )\n",
    "    print(f\"✅ Added {len(df)} documents to {active_collection}\")\n",
    "else:\n",
    "    print(f\"✅ Using existing collection: {active_collection}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 MMR Theory & Intuition\n",
    "\n",
    "### What is Maximal Marginal Relevance?\n",
    "\n",
    "MMR balances two competing objectives:\n",
    "1. **Relevance**: How well does each item match the query?\n",
    "2. **Diversity**: How different is each item from already selected items?\n",
    "\n",
    "**MMR Score = λ × Relevance - (1-λ) × Max_Similarity_to_Selected**\n",
    "\n",
    "Where:\n",
    "- λ ∈ [0,1]: Controls relevance vs diversity trade-off\n",
    "- λ = 1: Pure relevance (no diversity)\n",
    "- λ = 0: Pure diversity (no relevance)\n",
    "- λ = 0.5: Balanced approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_mmr_visually():\n",
    "    \"\"\"Visual explanation of MMR concept\"\"\"\n",
    "    print(\"🎯 MMR Intuition Example\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    print(\"\\n📍 Scenario: Query 'machine learning'\")\n",
    "    print(\"\\n🔍 Top 5 similarity results (without MMR):\")\n",
    "    print(\"   1. [0.95] Machine learning algorithms and techniques\")\n",
    "    print(\"   2. [0.94] ML algorithms for data science\")\n",
    "    print(\"   3. [0.93] Machine learning model training\")\n",
    "    print(\"   4. [0.92] Supervised machine learning methods\")\n",
    "    print(\"   5. [0.91] ML classification and regression\")\n",
    "    \n",
    "    print(\"\\n❌ Problem: All results are very similar!\")\n",
    "    print(\"   • High relevance but low diversity\")\n",
    "    print(\"   • User gets repetitive information\")\n",
    "    print(\"   • Misses broader context\")\n",
    "    \n",
    "    print(\"\\n✅ With MMR (λ=0.5):\")\n",
    "    print(\"   1. [0.95] Machine learning algorithms and techniques\")\n",
    "    print(\"   2. [0.85] Neural network architecture patterns\")\n",
    "    print(\"   3. [0.82] Data preprocessing for ML pipelines\")\n",
    "    print(\"   4. [0.79] ML model deployment strategies\")\n",
    "    print(\"   5. [0.76] Performance metrics and evaluation\")\n",
    "    \n",
    "    print(\"\\n✅ Benefits:\")\n",
    "    print(\"   • Still relevant to 'machine learning'\")\n",
    "    print(\"   • Covers different aspects of ML\")\n",
    "    print(\"   • More informative for users\")\n",
    "    print(\"   • Better coverage of the domain\")\n",
    "\n",
    "explain_mmr_visually()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Get Baseline Search Results\n",
    "\n",
    "Let's get a large set of candidates to rerank with MMR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_diverse_queries() -> List[Dict]:\n",
    "    \"\"\"Create queries that benefit from MMR in different ways\"\"\"\n",
    "    queries = [\n",
    "        # Ambiguous queries (should benefit from MMR)\n",
    "        {\n",
    "            \"text\": \"optimization\",\n",
    "            \"type\": \"ambiguous\",\n",
    "            \"description\": \"Could mean database, algorithm, or business optimization\"\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"configuration setup\", \n",
    "            \"type\": \"ambiguous\",\n",
    "            \"description\": \"Could apply to various systems and tools\"\n",
    "        },\n",
    "        # Specific queries (might be hurt by high diversity)\n",
    "        {\n",
    "            \"text\": \"password reset procedure\",\n",
    "            \"type\": \"specific\",\n",
    "            \"description\": \"Very specific need, diversity might hurt\"\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"HNSW algorithm implementation\",\n",
    "            \"type\": \"specific\", \n",
    "            \"description\": \"Technical and specific\"\n",
    "        },\n",
    "        # Broad queries (should benefit from MMR)\n",
    "        {\n",
    "            \"text\": \"customer support help\",\n",
    "            \"type\": \"broad\",\n",
    "            \"description\": \"Broad topic with many subtypes\"\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"software development process\",\n",
    "            \"type\": \"broad\",\n",
    "            \"description\": \"Wide domain with many aspects\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return queries\n",
    "\n",
    "# Create test queries\n",
    "test_queries = create_diverse_queries()\n",
    "\n",
    "print(\"🎯 Test Queries for MMR Analysis:\")\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"{i}. [{query['type']:>10}] '{query['text']}'\")\n",
    "    print(f\"   💭 {query['description']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 Baseline Search Function\n",
    "\n",
    "Get initial candidates for MMR reranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "\n",
    "# Baseline candidate retrieval (independent and robust)\n",
    "def get_baseline_candidates(query_text: str, limit: int = 50) -> Tuple[List, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Get baseline search results and vectors for MMR reranking.\n",
    "    Falls back gracefully if vectors are missing.\n",
    "    \"\"\"\n",
    "    global client, active_collection\n",
    "\n",
    "    # Inspect vector config\n",
    "    collection_info = client.get_collection(active_collection)\n",
    "    vector_config = collection_info.config.params.vectors\n",
    "    is_hybrid = isinstance(vector_config, dict) and \"text_dense\" in vector_config\n",
    "\n",
    "    if is_hybrid:\n",
    "        # Hybrid: use mock dense/sparse for demo\n",
    "        dense_vector = np.random.randn(384)\n",
    "        dense_vector /= np.linalg.norm(dense_vector)\n",
    "        words = query_text.lower().split()\n",
    "        sparse_vector = {i: 1.0 / max(1, len(words)) for i in range(len(words))}\n",
    "\n",
    "        results = search_hybrid_fusion(\n",
    "            client=client,\n",
    "            collection_name=active_collection,\n",
    "            dense_vector=dense_vector,\n",
    "            sparse_vector=sparse_vector,\n",
    "            dense_weight=0.6,\n",
    "            limit=limit * 2,\n",
    "            final_limit=limit,\n",
    "        )\n",
    "\n",
    "        point_ids = [r.id for r in results]\n",
    "        points = client.retrieve(\n",
    "            collection_name=active_collection,\n",
    "            ids=point_ids,\n",
    "            with_vectors=[\"text_dense\"],\n",
    "        )\n",
    "        candidate_vectors = []\n",
    "        for p in points:\n",
    "            if getattr(p, \"vector\", None) and \"text_dense\" in p.vector:\n",
    "                candidate_vectors.append(p.vector[\"text_dense\"])\n",
    "            else:\n",
    "                v = np.random.randn(384)\n",
    "                candidate_vectors.append(v / np.linalg.norm(v))\n",
    "        candidate_vectors = np.array(candidate_vectors)\n",
    "        query_vector = dense_vector\n",
    "    else:\n",
    "        # Single-vector collection\n",
    "        vec_size = int(getattr(vector_config, \"size\", 384) or 384)\n",
    "        query_vector = np.random.randn(vec_size)\n",
    "        query_vector /= np.linalg.norm(query_vector)\n",
    "\n",
    "        results = search_dense(\n",
    "            client=client,\n",
    "            collection_name=active_collection,\n",
    "            query_vector=query_vector,\n",
    "            limit=limit,\n",
    "            with_payload=True,\n",
    "            with_vectors=True,\n",
    "        )\n",
    "\n",
    "        candidate_vectors = []\n",
    "        for r in results:\n",
    "            vec = None\n",
    "            if hasattr(r, \"vector\") and r.vector is not None:\n",
    "                if isinstance(r.vector, dict):\n",
    "                    vec = next(iter(r.vector.values()))\n",
    "                else:\n",
    "                    vec = r.vector\n",
    "            if vec is None:\n",
    "                v = np.random.randn(vec_size)\n",
    "                vec = v / np.linalg.norm(v)\n",
    "            candidate_vectors.append(vec)\n",
    "        candidate_vectors = np.array(candidate_vectors)\n",
    "\n",
    "    print(f\"📊 Retrieved {len(results)} candidates with {candidate_vectors.shape[1]}D vectors\")\n",
    "    return results, candidate_vectors, query_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Implement MMR Reranking\n",
    "\n",
    "Now let's apply MMR to rerank our candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMR implementation is already in utils.py, let's test it\n",
    "\n",
    "def compare_mmr_results(query_text: str, lambda_values: List[float] = [0.2, 0.5, 0.8], k: int = 10):\n",
    "    \"\"\"Compare MMR results with different lambda values\"\"\"\n",
    "    print(f\"\\n🎯 MMR Comparison: '{query_text}'\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get baseline candidates\n",
    "    candidates, candidate_vectors, query_vector = get_baseline_candidates(query_text, limit=50)\n",
    "    \n",
    "    if len(candidates) == 0:\n",
    "        print(\"⚠️ No candidates found\")\n",
    "        return\n",
    "    \n",
    "    results_by_lambda = {}\n",
    "    \n",
    "    # Original ranking (no MMR)\n",
    "    original_top_k = candidates[:k]\n",
    "    results_by_lambda['original'] = original_top_k\n",
    "    \n",
    "    print(f\"\\n🔍 ORIGINAL (No MMR):\")\n",
    "    for i, result in enumerate(original_top_k[:5]):\n",
    "        print(f\"  {i+1}. [{result.score:.3f}] {result.payload['text'][:55]}...\")\n",
    "    \n",
    "    # Calculate original redundancy\n",
    "    original_texts = [r.payload['text'] for r in original_top_k]\n",
    "    original_redundancy = calculate_redundancy(original_texts)\n",
    "    print(f\"     📊 Redundancy: {original_redundancy:.3f}\")\n",
    "    \n",
    "    # MMR with different lambda values\n",
    "    for lambda_val in lambda_values:\n",
    "        print(f\"\\n🎚️ MMR (λ={lambda_val}):\")\n",
    "        \n",
    "        mmr_results = mmr_rerank(\n",
    "            query_vector=query_vector,\n",
    "            candidate_vectors=candidate_vectors[:len(candidates)],\n",
    "            candidate_results=candidates,\n",
    "            lambda_param=lambda_val,\n",
    "            k=k\n",
    "        )\n",
    "        \n",
    "        results_by_lambda[lambda_val] = mmr_results\n",
    "        \n",
    "        for i, result in enumerate(mmr_results[:5]):\n",
    "            print(f\"  {i+1}. [{result.score:.3f}] {result.payload['text'][:55]}...\")\n",
    "        \n",
    "        # Calculate MMR redundancy\n",
    "        mmr_texts = [r.payload['text'] for r in mmr_results]\n",
    "        mmr_redundancy = calculate_redundancy(mmr_texts)\n",
    "        \n",
    "        # Calculate category diversity\n",
    "        categories = [r.payload.get('category', 'unknown') for r in mmr_results]\n",
    "        unique_categories = len(set(categories))\n",
    "        category_diversity = unique_categories / len(categories) if categories else 0\n",
    "        \n",
    "        print(f\"     📊 Redundancy: {mmr_redundancy:.3f} (vs {original_redundancy:.3f} original)\")\n",
    "        print(f\"     📂 Category diversity: {category_diversity:.3f} ({unique_categories}/{len(categories)} unique)\")\n",
    "        \n",
    "        # Show improvement\n",
    "        if mmr_redundancy < original_redundancy:\n",
    "            improvement = (original_redundancy - mmr_redundancy) / original_redundancy * 100\n",
    "            print(f\"     ✅ {improvement:.1f}% redundancy reduction\")\n",
    "    \n",
    "    return results_by_lambda\n",
    "\n",
    "# Test MMR on our first query\n",
    "mmr_comparison = compare_mmr_results(test_queries[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Comprehensive MMR Analysis\n",
    "\n",
    "Let's analyze MMR performance across all our test queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_mmr_performance(queries: List[Dict], lambda_values: List[float] = [0.2, 0.5, 0.8]):\n",
    "    \"\"\"Comprehensive MMR analysis across multiple queries\"\"\"\n",
    "    analysis_results = []\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\n🔍 Analyzing: '{query['text']}' ({query['type']})\")\n",
    "        \n",
    "        try:\n",
    "            # Get candidates\n",
    "            candidates, candidate_vectors, query_vector = get_baseline_candidates(\n",
    "                query[\"text\"], limit=30\n",
    "            )\n",
    "            \n",
    "            if len(candidates) < 5:\n",
    "                print(f\"   ⚠️ Insufficient candidates ({len(candidates)})\")\n",
    "                continue\n",
    "            \n",
    "            k = min(10, len(candidates))\n",
    "            original_results = candidates[:k]\n",
    "            \n",
    "            # Calculate baseline metrics\n",
    "            original_texts = [r.payload['text'] for r in original_results]\n",
    "            original_redundancy = calculate_redundancy(original_texts)\n",
    "            \n",
    "            original_categories = [r.payload.get('category', 'unknown') for r in original_results]\n",
    "            original_category_diversity = len(set(original_categories)) / len(original_categories)\n",
    "            original_top_score = original_results[0].score if original_results else 0\n",
    "            \n",
    "            query_analysis = {\n",
    "                'query': query['text'],\n",
    "                'type': query['type'],\n",
    "                'original_redundancy': original_redundancy,\n",
    "                'original_category_diversity': original_category_diversity,\n",
    "                'original_top_score': original_top_score\n",
    "            }\n",
    "            \n",
    "            # Test different lambda values\n",
    "            for lambda_val in lambda_values:\n",
    "                mmr_results = mmr_rerank(\n",
    "                    query_vector=query_vector,\n",
    "                    candidate_vectors=candidate_vectors[:len(candidates)],\n",
    "                    candidate_results=candidates,\n",
    "                    lambda_param=lambda_val,\n",
    "                    k=k\n",
    "                )\n",
    "                \n",
    "                # Calculate MMR metrics\n",
    "                mmr_texts = [r.payload['text'] for r in mmr_results]\n",
    "                mmr_redundancy = calculate_redundancy(mmr_texts)\n",
    "                \n",
    "                mmr_categories = [r.payload.get('category', 'unknown') for r in mmr_results]\n",
    "                mmr_category_diversity = len(set(mmr_categories)) / len(mmr_categories)\n",
    "                mmr_top_score = mmr_results[0].score if mmr_results else 0\n",
    "                \n",
    "                # Calculate overlaps with original\n",
    "                original_ids = {r.id for r in original_results}\n",
    "                mmr_ids = {r.id for r in mmr_results}\n",
    "                overlap_ratio = len(original_ids & mmr_ids) / len(original_ids)\n",
    "                \n",
    "                # Store results\n",
    "                query_analysis[f'mmr_{lambda_val}_redundancy'] = mmr_redundancy\n",
    "                query_analysis[f'mmr_{lambda_val}_category_diversity'] = mmr_category_diversity\n",
    "                query_analysis[f'mmr_{lambda_val}_top_score'] = mmr_top_score\n",
    "                query_analysis[f'mmr_{lambda_val}_overlap'] = overlap_ratio\n",
    "                \n",
    "                redundancy_change = (original_redundancy - mmr_redundancy) / original_redundancy * 100 if original_redundancy > 0 else 0\n",
    "                query_analysis[f'mmr_{lambda_val}_redundancy_improvement'] = redundancy_change\n",
    "                \n",
    "                print(f\"   λ={lambda_val}: redundancy {redundancy_change:+.1f}%, diversity {mmr_category_diversity:.2f}, overlap {overlap_ratio:.2f}\")\n",
    "            \n",
    "            analysis_results.append(query_analysis)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error analyzing query: {e}\")\n",
    "    \n",
    "    return analysis_results\n",
    "\n",
    "# Run comprehensive analysis\n",
    "print(\"🚀 Running comprehensive MMR analysis...\")\n",
    "analysis_results = analyze_mmr_performance(test_queries)\n",
    "\n",
    "# Create summary DataFrame\n",
    "if analysis_results:\n",
    "    results_df = pd.DataFrame(analysis_results)\n",
    "    print(f\"\\n📊 Analysis complete: {len(results_df)} queries analyzed\")\n",
    "else:\n",
    "    print(\"⚠️ No analysis results generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 MMR Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'results_df' in locals() and len(results_df) > 0:\n",
    "    # Create comprehensive visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('MMR Reranking Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    lambda_values = [0.2, 0.5, 0.8]\n",
    "    colors = ['#e74c3c', '#f39c12', '#27ae60']  # Red, Orange, Green\n",
    "    \n",
    "    # 1. Redundancy Improvement by Lambda\n",
    "    ax1 = axes[0, 0]\n",
    "    lambda_improvements = []\n",
    "    for lam in lambda_values:\n",
    "        col = f'mmr_{lam}_redundancy_improvement'\n",
    "        if col in results_df.columns:\n",
    "            improvements = results_df[col].values\n",
    "            lambda_improvements.append(improvements)\n",
    "    \n",
    "    if lambda_improvements:\n",
    "        positions = np.arange(len(lambda_values))\n",
    "        bp = ax1.boxplot(lambda_improvements, positions=positions, patch_artist=True)\n",
    "        \n",
    "        for patch, color in zip(bp['boxes'], colors):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.7)\n",
    "        \n",
    "        ax1.set_xticklabels([f'λ={lam}' for lam in lambda_values])\n",
    "        ax1.set_ylabel('Redundancy Improvement (%)')\n",
    "        ax1.set_title('Redundancy Reduction by Lambda')\n",
    "        ax1.axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # 2. Category Diversity Improvement\n",
    "    ax2 = axes[0, 1]\n",
    "    query_types = results_df['type'].unique()\n",
    "    x_pos = np.arange(len(query_types))\n",
    "    width = 0.25\n",
    "    \n",
    "    original_diversity = []\n",
    "    mmr_diversity_by_lambda = {lam: [] for lam in lambda_values}\n",
    "    \n",
    "    for qtype in query_types:\n",
    "        subset = results_df[results_df['type'] == qtype]\n",
    "        original_diversity.append(subset['original_category_diversity'].mean())\n",
    "        \n",
    "        for lam in lambda_values:\n",
    "            col = f'mmr_{lam}_category_diversity'\n",
    "            if col in subset.columns:\n",
    "                mmr_diversity_by_lambda[lam].append(subset[col].mean())\n",
    "            else:\n",
    "                mmr_diversity_by_lambda[lam].append(0)\n",
    "    \n",
    "    # Plot bars\n",
    "    ax2.bar(x_pos - width, original_diversity, width, label='Original', color='#95a5a6', alpha=0.7)\n",
    "    \n",
    "    for i, lam in enumerate(lambda_values):\n",
    "        ax2.bar(x_pos + (i * width), mmr_diversity_by_lambda[lam], width, \n",
    "               label=f'MMR λ={lam}', color=colors[i], alpha=0.7)\n",
    "    \n",
    "    ax2.set_ylabel('Category Diversity')\n",
    "    ax2.set_title('Category Diversity by Query Type')\n",
    "    ax2.set_xticks(x_pos)\n",
    "    ax2.set_xticklabels(query_types, rotation=45)\n",
    "    ax2.legend()\n",
    "    \n",
    "    # 3. Top Score Preservation\n",
    "    ax3 = axes[1, 0]\n",
    "    score_ratios_by_lambda = []\n",
    "    \n",
    "    for lam in lambda_values:\n",
    "        score_col = f'mmr_{lam}_top_score'\n",
    "        if score_col in results_df.columns:\n",
    "            # Calculate score retention ratio\n",
    "            score_ratios = results_df[score_col] / results_df['original_top_score']\n",
    "            score_ratios = score_ratios[np.isfinite(score_ratios)]  # Remove inf/nan\n",
    "            score_ratios_by_lambda.append(score_ratios)\n",
    "    \n",
    "    if score_ratios_by_lambda:\n",
    "        bp2 = ax3.boxplot(score_ratios_by_lambda, patch_artist=True)\n",
    "        for patch, color in zip(bp2['boxes'], colors):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.7)\n",
    "        \n",
    "        ax3.set_xticklabels([f'λ={lam}' for lam in lambda_values])\n",
    "        ax3.set_ylabel('Top Score Retention Ratio')\n",
    "        ax3.set_title('Relevance Preservation')\n",
    "        ax3.axhline(y=1.0, color='black', linestyle='--', alpha=0.3, label='Perfect retention')\n",
    "    \n",
    "    # 4. Result Overlap with Original\n",
    "    ax4 = axes[1, 1]\n",
    "    overlap_data = []\n",
    "    \n",
    "    for lam in lambda_values:\n",
    "        overlap_col = f'mmr_{lam}_overlap'\n",
    "        if overlap_col in results_df.columns:\n",
    "            overlap_data.append(results_df[overlap_col].values)\n",
    "    \n",
    "    if overlap_data:\n",
    "        bp3 = ax4.boxplot(overlap_data, patch_artist=True)\n",
    "        for patch, color in zip(bp3['boxes'], colors):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.7)\n",
    "        \n",
    "        ax4.set_xticklabels([f'λ={lam}' for lam in lambda_values])\n",
    "        ax4.set_ylabel('Overlap with Original (%)')\n",
    "        ax4.set_title('Result Set Stability')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n📊 MMR Analysis Summary:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for lam in lambda_values:\n",
    "        improvement_col = f'mmr_{lam}_redundancy_improvement'\n",
    "        if improvement_col in results_df.columns:\n",
    "            avg_improvement = results_df[improvement_col].mean()\n",
    "            print(f\"\\nλ={lam}:\")\n",
    "            print(f\"   📉 Avg redundancy reduction: {avg_improvement:.1f}%\")\n",
    "            \n",
    "            diversity_col = f'mmr_{lam}_category_diversity'\n",
    "            if diversity_col in results_df.columns:\n",
    "                avg_diversity = results_df[diversity_col].mean()\n",
    "                orig_diversity = results_df['original_category_diversity'].mean()\n",
    "                diversity_improvement = (avg_diversity - orig_diversity) / orig_diversity * 100\n",
    "                print(f\"   📈 Avg diversity improvement: {diversity_improvement:+.1f}%\")\n",
    "            \n",
    "            overlap_col = f'mmr_{lam}_overlap'\n",
    "            if overlap_col in results_df.columns:\n",
    "                avg_overlap = results_df[overlap_col].mean()\n",
    "                print(f\"   🔄 Avg result overlap: {avg_overlap:.1%}\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ No data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 When MMR Helps vs When It Hurts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_mmr_effectiveness():\n",
    "    \"\"\"Analyze when MMR helps vs hurts based on query characteristics\"\"\"\n",
    "    \n",
    "    if 'results_df' not in locals() or len(results_df) == 0:\n",
    "        print(\"⚠️ No analysis data available\")\n",
    "        return\n",
    "    \n",
    "    print(\"🎯 When MMR Helps vs Hurts\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Group by query type\n",
    "    for query_type in results_df['type'].unique():\n",
    "        subset = results_df[results_df['type'] == query_type]\n",
    "        print(f\"\\n📋 {query_type.upper()} Queries:\")\n",
    "        \n",
    "        for _, row in subset.iterrows():\n",
    "            query_text = row['query']\n",
    "            print(f\"\\n   Query: '{query_text}'\")\n",
    "            \n",
    "            # Find best lambda for this query\n",
    "            best_lambda = None\n",
    "            best_improvement = -float('inf')\n",
    "            \n",
    "            lambda_performances = []\n",
    "            for lam in [0.2, 0.5, 0.8]:\n",
    "                improvement_col = f'mmr_{lam}_redundancy_improvement'\n",
    "                if improvement_col in row:\n",
    "                    improvement = row[improvement_col]\n",
    "                    lambda_performances.append((lam, improvement))\n",
    "                    \n",
    "                    if improvement > best_improvement:\n",
    "                        best_improvement = improvement\n",
    "                        best_lambda = lam\n",
    "            \n",
    "            # Show performance for each lambda\n",
    "            for lam, improvement in lambda_performances:\n",
    "                status = \"✅\" if improvement > 5 else \"⚠️\" if improvement > 0 else \"❌\"\n",
    "                overlap_col = f'mmr_{lam}_overlap'\n",
    "                overlap = row[overlap_col] if overlap_col in row else 0\n",
    "                \n",
    "                print(f\"     λ={lam}: {status} {improvement:+.1f}% redundancy, {overlap:.1%} overlap\")\n",
    "            \n",
    "            # Recommendation\n",
    "            if best_improvement > 10:\n",
    "                print(f\"     💡 Recommendation: Use λ={best_lambda} (strong improvement)\")\n",
    "            elif best_improvement > 0:\n",
    "                print(f\"     💡 Recommendation: Use λ={best_lambda} (modest improvement)\")\n",
    "            else:\n",
    "                print(f\"     💡 Recommendation: Skip MMR (no improvement)\")\n",
    "    \n",
    "    # Overall recommendations\n",
    "    print(f\"\\n📚 General Guidelines:\")\n",
    "    print(f\"\\n🎯 AMBIGUOUS/BROAD Queries:\")\n",
    "    print(f\"   • MMR typically helps (λ=0.3-0.5)\")\n",
    "    print(f\"   • Benefits: Reduces repetitive results\")\n",
    "    print(f\"   • Benefits: Covers different aspects\")\n",
    "    \n",
    "    print(f\"\\n🎯 SPECIFIC Queries:\")\n",
    "    print(f\"   • MMR may hurt (use λ=0.7-0.8 if needed)\")\n",
    "    print(f\"   • Risk: May demote the most relevant results\")\n",
    "    print(f\"   • Alternative: Use score threshold instead\")\n",
    "    \n",
    "    print(f\"\\n🎯 RECOMMENDED λ VALUES:\")\n",
    "    print(f\"   • λ=0.8: Slight diversity, preserve relevance\")\n",
    "    print(f\"   • λ=0.5: Balanced relevance and diversity\")\n",
    "    print(f\"   • λ=0.3: Strong diversity, some relevance loss\")\n",
    "    print(f\"   • λ=0.1: Maximum diversity (rarely recommended)\")\n",
    "\n",
    "analyze_mmr_effectiveness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Server-Side MMR (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_server_side_mmr():\n",
    "    \"\"\"Test server-side MMR if available in Qdrant\"\"\"\n",
    "    print(\"🔄 Testing Server-Side MMR...\")\n",
    "    \n",
    "    try:\n",
    "        # This is hypothetical - actual server-side MMR API may differ\n",
    "        # from qdrant_client.models import SearchRequest, RerankRequest\n",
    "        \n",
    "        print(\"⚠️  Server-side MMR not yet available in standard Qdrant client\")\n",
    "        print(\"\\n📝 Expected future API might look like:\")\n",
    "        print(\"\"\"    \n",
    "    # Hypothetical server-side MMR API\n",
    "    results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_vector,\n",
    "        limit=50,  # Get candidates\n",
    "        rerank={\n",
    "            \"method\": \"mmr\",\n",
    "            \"lambda\": 0.5,\n",
    "            \"final_limit\": 10\n",
    "        }\n",
    "    )\n",
    "    \"\"\")\n",
    "        \n",
    "        print(\"\\n✅ Current approach: Client-side MMR (as demonstrated above)\")\n",
    "        print(\"   • More flexible parameter tuning\")\n",
    "        print(\"   • Works with any vector similarity metric\")\n",
    "        print(\"   • Can combine with other reranking methods\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Server-side MMR test failed: {e}\")\n",
    "\n",
    "test_server_side_mmr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎛️ Parameter Tuning Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mmr_tuning_guide():\n",
    "    \"\"\"Create a comprehensive guide for MMR parameter tuning\"\"\"\n",
    "    \n",
    "    print(\"🎛️ MMR Parameter Tuning Guide\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Lambda tuning guide\n",
    "    lambda_guide = {\n",
    "        \"0.9-1.0\": {\n",
    "            \"focus\": \"Pure Relevance\", \n",
    "            \"use_case\": \"Specific technical queries\",\n",
    "            \"example\": \"'password reset steps'\",\n",
    "            \"pros\": \"Preserves exact relevance ranking\",\n",
    "            \"cons\": \"No diversity benefit\"\n",
    "        },\n",
    "        \"0.7-0.8\": {\n",
    "            \"focus\": \"Relevance-Heavy\",\n",
    "            \"use_case\": \"Domain-specific searches\", \n",
    "            \"example\": \"'HNSW algorithm optimization'\",\n",
    "            \"pros\": \"Slight diversity, high relevance\",\n",
    "            \"cons\": \"Limited diversity gains\"\n",
    "        },\n",
    "        \"0.4-0.6\": {\n",
    "            \"focus\": \"Balanced\",\n",
    "            \"use_case\": \"General-purpose search\",\n",
    "            \"example\": \"'machine learning techniques'\", \n",
    "            \"pros\": \"Good relevance-diversity trade-off\",\n",
    "            \"cons\": \"May need query-specific tuning\"\n",
    "        },\n",
    "        \"0.2-0.3\": {\n",
    "            \"focus\": \"Diversity-Heavy\",\n",
    "            \"use_case\": \"Exploratory/research queries\",\n",
    "            \"example\": \"'business optimization strategies'\",\n",
    "            \"pros\": \"High diversity, broad coverage\", \n",
    "            \"cons\": \"May sacrifice top relevance\"\n",
    "        },\n",
    "        \"0.0-0.1\": {\n",
    "            \"focus\": \"Pure Diversity\",\n",
    "            \"use_case\": \"Brainstorming/discovery\",\n",
    "            \"example\": \"'innovation approaches'\",\n",
    "            \"pros\": \"Maximum diversity\",\n",
    "            \"cons\": \"Poor relevance, rarely useful\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"\\n📊 Lambda (λ) Value Guide:\")\n",
    "    for lambda_range, info in lambda_guide.items():\n",
    "        print(f\"\\n🎯 λ = {lambda_range}: {info['focus']}\")\n",
    "        print(f\"   Use case: {info['use_case']}\")\n",
    "        print(f\"   Example: {info['example']}\")\n",
    "        print(f\"   ✅ Pros: {info['pros']}\")\n",
    "        print(f\"   ⚠️  Cons: {info['cons']}\")\n",
    "    \n",
    "    print(f\"\\n🔧 Tuning Process:\")\n",
    "    print(f\"\\n1️⃣ BASELINE ANALYSIS:\")\n",
    "    print(f\"   • Measure redundancy in top-10 results\")\n",
    "    print(f\"   • Check category/topic diversity\")\n",
    "    print(f\"   • Note query type (specific/broad/ambiguous)\")\n",
    "    \n",
    "    print(f\"\\n2️⃣ LAMBDA SELECTION:\")\n",
    "    print(f\"   • Start with λ=0.5 (balanced)\")\n",
    "    print(f\"   • If high redundancy → decrease λ (more diversity)\")\n",
    "    print(f\"   • If low relevance → increase λ (more relevance)\")\n",
    "    print(f\"   • If specific queries → use λ≥0.7\")\n",
    "    \n",
    "    print(f\"\\n3️⃣ EVALUATION METRICS:\")\n",
    "    print(f\"   • Redundancy score (target: <0.3 for diverse results)\")\n",
    "    print(f\"   • Top-1 relevance retention (target: >90%)\")\n",
    "    print(f\"   • Category diversity (target: context-dependent)\")\n",
    "    print(f\"   • User satisfaction (A/B testing)\")\n",
    "    \n",
    "    print(f\"\\n4️⃣ PRODUCTION CONSIDERATIONS:\")\n",
    "    print(f\"   • Cache MMR results for popular queries\")\n",
    "    print(f\"   • Consider query-adaptive λ values\")\n",
    "    print(f\"   • Monitor latency impact (MMR adds computation)\")\n",
    "    print(f\"   • A/B test against non-MMR results\")\n",
    "    \n",
    "    # Create decision tree\n",
    "    print(f\"\\n🌳 Decision Tree:\")\n",
    "    decision_tree = '''\n",
    "    Query Intent?\n",
    "    ├── Specific/Technical\n",
    "    │   ├── High precision needed? → λ=0.8-0.9\n",
    "    │   └── Some diversity OK? → λ=0.6-0.7  \n",
    "    ├── General/Broad\n",
    "    │   ├── Balanced results? → λ=0.4-0.6\n",
    "    │   └── Diverse exploration? → λ=0.2-0.4\n",
    "    └── Ambiguous/Multi-meaning\n",
    "        ├── Cover all aspects? → λ=0.2-0.4\n",
    "        └── Balanced approach? → λ=0.4-0.5\n",
    "    '''\n",
    "    print(decision_tree)\n",
    "\n",
    "create_mmr_tuning_guide()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Final Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final collection stats\n",
    "final_info = client.get_collection(active_collection)\n",
    "\n",
    "print(\"🎉 MMR Reranking Summary\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"\\n📚 Collection: {active_collection}\")\n",
    "print(f\"   📊 Total points: {final_info.points_count}\")\n",
    "print(f\"   🎯 Analysis queries: {len(test_queries)}\")\n",
    "\n",
    "print(f\"\\n🔄 MMR Implementation:\")\n",
    "print(\"   ✅ Client-side MMR reranking\")\n",
    "print(\"   ✅ Configurable λ parameter (0.0-1.0)\")\n",
    "print(\"   ✅ Works with any vector similarity metric\")\n",
    "print(\"   ✅ Integrates with hybrid search results\")\n",
    "\n",
    "if 'results_df' in locals() and len(results_df) > 0:\n",
    "    # Calculate overall improvements\n",
    "    lambda_05_improvement = results_df['mmr_0.5_redundancy_improvement'].mean()\n",
    "    best_improvements = []\n",
    "    \n",
    "    for lam in [0.2, 0.5, 0.8]:\n",
    "        col = f'mmr_{lam}_redundancy_improvement'\n",
    "        if col in results_df.columns:\n",
    "            best_improvements.append(results_df[col].max())\n",
    "    \n",
    "    print(f\"\\n📈 Performance Results:\")\n",
    "    print(f\"   📉 Average redundancy reduction (λ=0.5): {lambda_05_improvement:.1f}%\")\n",
    "    print(f\"   🏆 Best single-query improvement: {max(best_improvements):.1f}%\")\n",
    "    \n",
    "    # Query type performance\n",
    "    print(f\"\\n📋 By Query Type:\")\n",
    "    for qtype in results_df['type'].unique():\n",
    "        subset = results_df[results_df['type'] == qtype]\n",
    "        avg_improvement = subset['mmr_0.5_redundancy_improvement'].mean()\n",
    "        print(f\"   {qtype:>12}: {avg_improvement:+.1f}% redundancy reduction\")\n",
    "\n",
    "print(f\"\\n🎯 Key Takeaways:\")\n",
    "print(\"   🔹 MMR reduces redundancy while preserving relevance\")\n",
    "print(\"   🔹 λ parameter controls relevance vs diversity trade-off\")\n",
    "print(\"   🔹 Most effective for broad/ambiguous queries\")\n",
    "print(\"   🔹 Use λ≥0.7 for specific technical queries\")\n",
    "print(\"   🔹 Use λ≤0.4 for exploratory/research queries\")\n",
    "print(\"   🔹 A/B test to find optimal λ for your domain\")\n",
    "\n",
    "print(f\"\\n🚀 Ready for Notebook 4: HNSW Index Health!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎮 Stretch Goals (Optional)\n",
    "\n",
    "Advanced MMR techniques and optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 Query-Adaptive Lambda\n",
    "\n",
    "Implement automatic lambda selection based on query characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_lambda_selection(query_text: str, baseline_results: List) -> float:\n",
    "    \"\"\"Automatically select lambda based on query and initial results\"\"\"\n",
    "    \n",
    "    # Start with balanced approach\n",
    "    base_lambda = 0.5\n",
    "    adjustments = []\n",
    "    \n",
    "    # Query length analysis\n",
    "    word_count = len(query_text.split())\n",
    "    if word_count <= 2:\n",
    "        adjustments.append((0.2, \"Short query → favor relevance\"))\n",
    "    elif word_count >= 6:\n",
    "        adjustments.append((-0.1, \"Long query → more diversity OK\"))\n",
    "    \n",
    "    # Technical term detection\n",
    "    technical_terms = [\n",
    "        'algorithm', 'api', 'database', 'function', 'method', 'class',\n",
    "        'variable', 'parameter', 'configuration', 'installation', 'setup'\n",
    "    ]\n",
    "    technical_score = sum(1 for term in technical_terms if term.lower() in query_text.lower())\n",
    "    if technical_score > 0:\n",
    "        adjustment = min(0.3, technical_score * 0.1)\n",
    "        adjustments.append((adjustment, f\"Technical terms → favor relevance\"))\n",
    "    \n",
    "    # Baseline results analysis\n",
    "    if len(baseline_results) >= 5:\n",
    "        # Check score distribution\n",
    "        scores = [r.score for r in baseline_results[:10]]\n",
    "        score_std = np.std(scores)\n",
    "        \n",
    "        if score_std < 0.1:  # Very similar scores\n",
    "            adjustments.append((-0.2, \"Similar scores → increase diversity\"))\n",
    "        \n",
    "        # Check category diversity\n",
    "        categories = [r.payload.get('category', 'unknown') for r in baseline_results[:10]]\n",
    "        unique_ratio = len(set(categories)) / len(categories)\n",
    "        \n",
    "        if unique_ratio > 0.8:  # Already diverse\n",
    "            adjustments.append((0.1, \"Already diverse → preserve relevance\"))\n",
    "        elif unique_ratio < 0.3:  # Very redundant\n",
    "            adjustments.append((-0.3, \"Low diversity → increase MMR diversity\"))\n",
    "    \n",
    "    # Apply adjustments\n",
    "    final_lambda = base_lambda\n",
    "    print(f\"\\n🤖 Adaptive Lambda for: '{query_text}'\")\n",
    "    print(f\"   Base λ: {base_lambda}\")\n",
    "    \n",
    "    for adjustment, reason in adjustments:\n",
    "        final_lambda += adjustment\n",
    "        print(f\"   {adjustment:+.2f}: {reason}\")\n",
    "    \n",
    "    # Clamp to valid range\n",
    "    final_lambda = max(0.1, min(0.9, final_lambda))\n",
    "    \n",
    "    print(f\"   Final λ: {final_lambda:.2f}\")\n",
    "    return final_lambda\n",
    "\n",
    "# Test adaptive lambda on our queries\n",
    "print(\"🤖 Adaptive Lambda Selection Examples:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for query in test_queries[:3]:\n",
    "    try:\n",
    "        candidates, vectors, qvec = get_baseline_candidates(query[\"text\"], limit=20)\n",
    "        adaptive_lambda = adaptive_lambda_selection(query[\"text\"], candidates)\n",
    "        \n",
    "        print(f\"\\n   Recommended for '{query['text']}': λ={adaptive_lambda:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   Error processing '{query['text']}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⚡ Hybrid MMR: Multiple Vector Types\n",
    "\n",
    "Apply MMR using different vector spaces for relevance vs diversity calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_mmr_rerank(query_dense: np.ndarray, query_sparse: Dict[int, float],\n",
    "                     candidates: List, candidate_dense: np.ndarray, \n",
    "                     candidate_sparse: List[Dict[int, float]], \n",
    "                     lambda_param: float = 0.5, k: int = 10) -> List:\n",
    "    \"\"\"MMR using dense vectors for relevance, sparse for diversity (or vice versa)\"\"\"\n",
    "    \n",
    "    if len(candidates) == 0:\n",
    "        return []\n",
    "    \n",
    "    selected_indices = []\n",
    "    remaining_indices = list(range(len(candidates)))\n",
    "    \n",
    "    # Normalize dense vectors\n",
    "    query_dense_norm = query_dense / np.linalg.norm(query_dense)\n",
    "    candidate_dense_norm = candidate_dense / np.linalg.norm(candidate_dense, axis=1, keepdims=True)\n",
    "    \n",
    "    # Calculate relevance using dense vectors (semantic similarity)\n",
    "    relevance_scores = np.dot(candidate_dense_norm, query_dense_norm)\n",
    "    \n",
    "    # Start with most relevant\n",
    "    first_idx = np.argmax(relevance_scores)\n",
    "    selected_indices.append(first_idx)\n",
    "    remaining_indices.remove(first_idx)\n",
    "    \n",
    "    print(f\"🎯 Hybrid MMR: Using dense for relevance, dense for diversity\")\n",
    "    print(f\"   (In practice, you might use sparse vectors for diversity calculation)\")\n",
    "    \n",
    "    # Iteratively select based on hybrid MMR\n",
    "    for _ in range(min(k - 1, len(remaining_indices))):\n",
    "        mmr_scores = []\n",
    "        \n",
    "        for idx in remaining_indices:\n",
    "            # Relevance from dense vectors\n",
    "            relevance = relevance_scores[idx]\n",
    "            \n",
    "            # Diversity from dense vectors (could use sparse here)\n",
    "            max_sim = 0\n",
    "            for sel_idx in selected_indices:\n",
    "                # Using dense similarity for diversity calculation\n",
    "                sim = np.dot(candidate_dense_norm[idx], candidate_dense_norm[sel_idx])\n",
    "                max_sim = max(max_sim, sim)\n",
    "            \n",
    "            # MMR score\n",
    "            mmr_score = lambda_param * relevance - (1 - lambda_param) * max_sim\n",
    "            mmr_scores.append((idx, mmr_score))\n",
    "        \n",
    "        # Select best MMR score\n",
    "        best_idx = max(mmr_scores, key=lambda x: x[1])[0]\n",
    "        selected_indices.append(best_idx)\n",
    "        remaining_indices.remove(best_idx)\n",
    "    \n",
    "    return [candidates[i] for i in selected_indices]\n",
    "\n",
    "# This is a conceptual implementation - in practice you'd need both vector types\n",
    "print(\"🔬 Hybrid MMR Concept:\")\n",
    "print(\"=\" * 30)\n",
    "print(\"\\n💡 Advanced MMR Ideas:\")\n",
    "print(\"   • Use dense vectors for relevance scoring\")\n",
    "print(\"   • Use sparse vectors for diversity calculation\")\n",
    "print(\"   • Combine topic modeling with vector similarity\")\n",
    "print(\"   • Apply different λ values for different content types\")\n",
    "print(\"   • Use metadata (categories, tags) for diversity constraints\")\n",
    "\n",
    "print(\"\\n🚧 Implementation Notes:\")\n",
    "print(\"   • Requires access to multiple vector representations\")\n",
    "print(\"   • Can significantly improve diversity in specialized domains\")\n",
    "print(\"   • Increases computational complexity\")\n",
    "print(\"   • Best tested with domain-specific evaluation metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧹 Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collection cleanup is optional since we're reusing existing collections\n",
    "print(f\"💾 Preserving collection: {active_collection}\")\n",
    "print(f\"\\n✨ Notebook 3 complete! Move on to 04_hnsw_health.ipynb\")\n",
    "print(f\"\\n🎯 Next: Learn how to maintain and optimize HNSW index performance!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
