{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3: Maximal Marginal Relevance (MMR) Reranking\n",
    "\n",
    "## üéØ Objectives\n",
    "\n",
    "In this notebook, you'll learn:\n",
    "- What Maximal Marginal Relevance (MMR) is and why it matters\n",
    "- How to implement MMR reranking to reduce redundancy\n",
    "- When MMR helps vs when it can hurt search quality\n",
    "- How to tune the Œª parameter for different query types\n",
    "- Measuring diversity vs relevance trade-offs\n",
    "- Applying MMR to both dense and hybrid search results\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "- Collection from Notebook 2 (hybrid search)\n",
    "- Understanding of vector similarity and search results\n",
    "- Basic linear algebra (cosine similarity, vector operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "from utils import (\n",
    "    get_qdrant_client, create_sample_dataset, search_dense, \n",
    "    search_hybrid_fusion, mmr_rerank, print_search_results,\n",
    "    calculate_redundancy, print_system_info\n",
    ")\n",
    "\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "print_system_info()\n",
    "print(\"\\nüéØ MMR Reranking Workshop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Install Dependencies (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if running in a fresh environment\n",
    "# !pip install qdrant-client numpy pandas matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "COLLECTION_NAME = \"workshop_hybrid\"  # Reusing from Notebook 2\n",
    "BACKUP_COLLECTION = \"workshop_fundamentals\"  # Fallback from Notebook 1\n",
    "\n",
    "# Connect to Qdrant\n",
    "client = get_qdrant_client()\n",
    "\n",
    "# Ensure collection exists and has data\n",
    "try:\n",
    "    info = client.get_collection(active_collection)\n",
    "except Exception:\n",
    "    info = None\n",
    "\n",
    "needs_population = False\n",
    "if info is None:\n",
    "    needs_population = True\n",
    "else:\n",
    "    print(\"‚ùå No suitable collection found. Please run Notebook 1 or 2 first.\")\n",
    "    raise Exception(\"Collection not found\")\n",
    "\n",
    "if needs_population:\n",
    "    print(f\"‚úÖ Creating and populating demo collection: {active_collection}\")\n",
    "    ensure_collection(\n",
    "        client,\n",
    "        active_collection,\n",
    "        VectorParams(size=384, distance=Distance.COSINE),\n",
    "        force_recreate=True,\n",
    "    )\n",
    "    df = create_sample_dataset(size=200, seed=42)\n",
    "    vectors = np.random.randn(len(df), 384)\n",
    "    vectors = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    upsert_points_batch(\n",
    "        client,\n",
    "        active_collection,\n",
    "        df,\n",
    "        vectors,\n",
    "        [\"text\", \"category\", \"lang\", \"timestamp\"],\n",
    "        batch_size=100,\n",
    "    )\n",
    "    print(f\"‚úÖ Added {len(df)} documents to {active_collection}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Using existing collection: {active_collection}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† MMR Theory & Intuition\n",
    "\n",
    "### What is Maximal Marginal Relevance?\n",
    "\n",
    "MMR balances two competing objectives:\n",
    "1. **Relevance**: How well does each item match the query?\n",
    "2. **Diversity**: How different is each item from already selected items?\n",
    "\n",
    "**MMR Score = Œª √ó Relevance - (1-Œª) √ó Max_Similarity_to_Selected**\n",
    "\n",
    "Where:\n",
    "- Œª ‚àà [0,1]: Controls relevance vs diversity trade-off\n",
    "- Œª = 1: Pure relevance (no diversity)\n",
    "- Œª = 0: Pure diversity (no relevance)\n",
    "- Œª = 0.5: Balanced approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_mmr_visually():\n",
    "    \"\"\"Visual explanation of MMR concept\"\"\"\n",
    "    print(\"üéØ MMR Intuition Example\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    print(\"\\nüìç Scenario: Query 'machine learning'\")\n",
    "    print(\"\\nüîç Top 5 similarity results (without MMR):\")\n",
    "    print(\"   1. [0.95] Machine learning algorithms and techniques\")\n",
    "    print(\"   2. [0.94] ML algorithms for data science\")\n",
    "    print(\"   3. [0.93] Machine learning model training\")\n",
    "    print(\"   4. [0.92] Supervised machine learning methods\")\n",
    "    print(\"   5. [0.91] ML classification and regression\")\n",
    "    \n",
    "    print(\"\\n‚ùå Problem: All results are very similar!\")\n",
    "    print(\"   ‚Ä¢ High relevance but low diversity\")\n",
    "    print(\"   ‚Ä¢ User gets repetitive information\")\n",
    "    print(\"   ‚Ä¢ Misses broader context\")\n",
    "    \n",
    "    print(\"\\n‚úÖ With MMR (Œª=0.5):\")\n",
    "    print(\"   1. [0.95] Machine learning algorithms and techniques\")\n",
    "    print(\"   2. [0.85] Neural network architecture patterns\")\n",
    "    print(\"   3. [0.82] Data preprocessing for ML pipelines\")\n",
    "    print(\"   4. [0.79] ML model deployment strategies\")\n",
    "    print(\"   5. [0.76] Performance metrics and evaluation\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Benefits:\")\n",
    "    print(\"   ‚Ä¢ Still relevant to 'machine learning'\")\n",
    "    print(\"   ‚Ä¢ Covers different aspects of ML\")\n",
    "    print(\"   ‚Ä¢ More informative for users\")\n",
    "    print(\"   ‚Ä¢ Better coverage of the domain\")\n",
    "\n",
    "explain_mmr_visually()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Get Baseline Search Results\n",
    "\n",
    "Let's get a large set of candidates to rerank with MMR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_diverse_queries() -> List[Dict]:\n",
    "    \"\"\"Create queries that benefit from MMR in different ways\"\"\"\n",
    "    queries = [\n",
    "        # Ambiguous queries (should benefit from MMR)\n",
    "        {\n",
    "            \"text\": \"optimization\",\n",
    "            \"type\": \"ambiguous\",\n",
    "            \"description\": \"Could mean database, algorithm, or business optimization\"\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"configuration setup\", \n",
    "            \"type\": \"ambiguous\",\n",
    "            \"description\": \"Could apply to various systems and tools\"\n",
    "        },\n",
    "        # Specific queries (might be hurt by high diversity)\n",
    "        {\n",
    "            \"text\": \"password reset procedure\",\n",
    "            \"type\": \"specific\",\n",
    "            \"description\": \"Very specific need, diversity might hurt\"\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"HNSW algorithm implementation\",\n",
    "            \"type\": \"specific\", \n",
    "            \"description\": \"Technical and specific\"\n",
    "        },\n",
    "        # Broad queries (should benefit from MMR)\n",
    "        {\n",
    "            \"text\": \"customer support help\",\n",
    "            \"type\": \"broad\",\n",
    "            \"description\": \"Broad topic with many subtypes\"\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"software development process\",\n",
    "            \"type\": \"broad\",\n",
    "            \"description\": \"Wide domain with many aspects\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return queries\n",
    "\n",
    "# Create test queries\n",
    "test_queries = create_diverse_queries()\n",
    "\n",
    "print(\"üéØ Test Queries for MMR Analysis:\")\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"{i}. [{query['type']:>10}] '{query['text']}'\")\n",
    "    print(f\"   üí≠ {query['description']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Baseline Search Function\n",
    "\n",
    "Get initial candidates for MMR reranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "\n",
    "# Baseline candidate retrieval (independent and robust)\n",
    "def get_baseline_candidates(query_text: str, limit: int = 50) -> Tuple[List, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Get baseline search results and vectors for MMR reranking.\n",
    "    Falls back gracefully if vectors are missing.\n",
    "    \"\"\"\n",
    "    global client, active_collection\n",
    "\n",
    "    # Inspect vector config\n",
    "    collection_info = client.get_collection(active_collection)\n",
    "    vector_config = collection_info.config.params.vectors\n",
    "    is_hybrid = isinstance(vector_config, dict) and \"text_dense\" in vector_config\n",
    "\n",
    "    if is_hybrid:\n",
    "        # Hybrid: use mock dense/sparse for demo\n",
    "        dense_vector = np.random.randn(384)\n",
    "        dense_vector /= np.linalg.norm(dense_vector)\n",
    "        words = query_text.lower().split()\n",
    "        sparse_vector = {i: 1.0 / max(1, len(words)) for i in range(len(words))}\n",
    "\n",
    "        results = search_hybrid_fusion(\n",
    "            client=client,\n",
    "            collection_name=active_collection,\n",
    "            dense_vector=dense_vector,\n",
    "            sparse_vector=sparse_vector,\n",
    "            dense_weight=0.6,\n",
    "            limit=limit * 2,\n",
    "            final_limit=limit,\n",
    "        )\n",
    "\n",
    "        point_ids = [r.id for r in results]\n",
    "        points = client.retrieve(\n",
    "            collection_name=active_collection,\n",
    "            ids=point_ids,\n",
    "            with_vectors=[\"text_dense\"],\n",
    "        )\n",
    "        candidate_vectors = []\n",
    "        for p in points:\n",
    "            if getattr(p, \"vector\", None) and \"text_dense\" in p.vector:\n",
    "                candidate_vectors.append(p.vector[\"text_dense\"])\n",
    "            else:\n",
    "                v = np.random.randn(384)\n",
    "                candidate_vectors.append(v / np.linalg.norm(v))\n",
    "        candidate_vectors = np.array(candidate_vectors)\n",
    "        query_vector = dense_vector\n",
    "    else:\n",
    "        # Single-vector collection\n",
    "        vec_size = int(getattr(vector_config, \"size\", 384) or 384)\n",
    "        query_vector = np.random.randn(vec_size)\n",
    "        query_vector /= np.linalg.norm(query_vector)\n",
    "\n",
    "        results = search_dense(\n",
    "            client=client,\n",
    "            collection_name=active_collection,\n",
    "            query_vector=query_vector,\n",
    "            limit=limit,\n",
    "            with_payload=True,\n",
    "            with_vectors=True,\n",
    "        )\n",
    "\n",
    "        candidate_vectors = []\n",
    "        for r in results:\n",
    "            vec = None\n",
    "            if hasattr(r, \"vector\") and r.vector is not None:\n",
    "                if isinstance(r.vector, dict):\n",
    "                    vec = next(iter(r.vector.values()))\n",
    "                else:\n",
    "                    vec = r.vector\n",
    "            if vec is None:\n",
    "                v = np.random.randn(vec_size)\n",
    "                vec = v / np.linalg.norm(v)\n",
    "            candidate_vectors.append(vec)\n",
    "        candidate_vectors = np.array(candidate_vectors)\n",
    "\n",
    "    print(f\"üìä Retrieved {len(results)} candidates with {candidate_vectors.shape[1]}D vectors\")\n",
    "    return results, candidate_vectors, query_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Implement MMR Reranking\n",
    "\n",
    "Now let's apply MMR to rerank our candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMR implementation is already in utils.py, let's test it\n",
    "\n",
    "def compare_mmr_results(query_text: str, lambda_values: List[float] = [0.2, 0.5, 0.8], k: int = 10):\n",
    "    \"\"\"Compare MMR results with different lambda values\"\"\"\n",
    "    print(f\"\\nüéØ MMR Comparison: '{query_text}'\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get baseline candidates\n",
    "    candidates, candidate_vectors, query_vector = get_baseline_candidates(query_text, limit=50)\n",
    "    \n",
    "    if len(candidates) == 0:\n",
    "        print(\"‚ö†Ô∏è No candidates found\")\n",
    "        return\n",
    "    \n",
    "    results_by_lambda = {}\n",
    "    \n",
    "    # Original ranking (no MMR)\n",
    "    original_top_k = candidates[:k]\n",
    "    results_by_lambda['original'] = original_top_k\n",
    "    \n",
    "    print(f\"\\nüîç ORIGINAL (No MMR):\")\n",
    "    for i, result in enumerate(original_top_k[:5]):\n",
    "        print(f\"  {i+1}. [{result.score:.3f}] {result.payload['text'][:55]}...\")\n",
    "    \n",
    "    # Calculate original redundancy\n",
    "    original_texts = [r.payload['text'] for r in original_top_k]\n",
    "    original_redundancy = calculate_redundancy(original_texts)\n",
    "    print(f\"     üìä Redundancy: {original_redundancy:.3f}\")\n",
    "    \n",
    "    # MMR with different lambda values\n",
    "    for lambda_val in lambda_values:\n",
    "        print(f\"\\nüéöÔ∏è MMR (Œª={lambda_val}):\")\n",
    "        \n",
    "        mmr_results = mmr_rerank(\n",
    "            query_vector=query_vector,\n",
    "            candidate_vectors=candidate_vectors[:len(candidates)],\n",
    "            candidate_results=candidates,\n",
    "            lambda_param=lambda_val,\n",
    "            k=k\n",
    "        )\n",
    "        \n",
    "        results_by_lambda[lambda_val] = mmr_results\n",
    "        \n",
    "        for i, result in enumerate(mmr_results[:5]):\n",
    "            print(f\"  {i+1}. [{result.score:.3f}] {result.payload['text'][:55]}...\")\n",
    "        \n",
    "        # Calculate MMR redundancy\n",
    "        mmr_texts = [r.payload['text'] for r in mmr_results]\n",
    "        mmr_redundancy = calculate_redundancy(mmr_texts)\n",
    "        \n",
    "        # Calculate category diversity\n",
    "        categories = [r.payload.get('category', 'unknown') for r in mmr_results]\n",
    "        unique_categories = len(set(categories))\n",
    "        category_diversity = unique_categories / len(categories) if categories else 0\n",
    "        \n",
    "        print(f\"     üìä Redundancy: {mmr_redundancy:.3f} (vs {original_redundancy:.3f} original)\")\n",
    "        print(f\"     üìÇ Category diversity: {category_diversity:.3f} ({unique_categories}/{len(categories)} unique)\")\n",
    "        \n",
    "        # Show improvement\n",
    "        if mmr_redundancy < original_redundancy:\n",
    "            improvement = (original_redundancy - mmr_redundancy) / original_redundancy * 100\n",
    "            print(f\"     ‚úÖ {improvement:.1f}% redundancy reduction\")\n",
    "    \n",
    "    return results_by_lambda\n",
    "\n",
    "# Test MMR on our first query\n",
    "mmr_comparison = compare_mmr_results(test_queries[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Comprehensive MMR Analysis\n",
    "\n",
    "Let's analyze MMR performance across all our test queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_mmr_performance(queries: List[Dict], lambda_values: List[float] = [0.2, 0.5, 0.8]):\n",
    "    \"\"\"Comprehensive MMR analysis across multiple queries\"\"\"\n",
    "    analysis_results = []\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\nüîç Analyzing: '{query['text']}' ({query['type']})\")\n",
    "        \n",
    "        try:\n",
    "            # Get candidates\n",
    "            candidates, candidate_vectors, query_vector = get_baseline_candidates(\n",
    "                query[\"text\"], limit=30\n",
    "            )\n",
    "            \n",
    "            if len(candidates) < 5:\n",
    "                print(f\"   ‚ö†Ô∏è Insufficient candidates ({len(candidates)})\")\n",
    "                continue\n",
    "            \n",
    "            k = min(10, len(candidates))\n",
    "            original_results = candidates[:k]\n",
    "            \n",
    "            # Calculate baseline metrics\n",
    "            original_texts = [r.payload['text'] for r in original_results]\n",
    "            original_redundancy = calculate_redundancy(original_texts)\n",
    "            \n",
    "            original_categories = [r.payload.get('category', 'unknown') for r in original_results]\n",
    "            original_category_diversity = len(set(original_categories)) / len(original_categories)\n",
    "            original_top_score = original_results[0].score if original_results else 0\n",
    "            \n",
    "            query_analysis = {\n",
    "                'query': query['text'],\n",
    "                'type': query['type'],\n",
    "                'original_redundancy': original_redundancy,\n",
    "                'original_category_diversity': original_category_diversity,\n",
    "                'original_top_score': original_top_score\n",
    "            }\n",
    "            \n",
    "            # Test different lambda values\n",
    "            for lambda_val in lambda_values:\n",
    "                mmr_results = mmr_rerank(\n",
    "                    query_vector=query_vector,\n",
    "                    candidate_vectors=candidate_vectors[:len(candidates)],\n",
    "                    candidate_results=candidates,\n",
    "                    lambda_param=lambda_val,\n",
    "                    k=k\n",
    "                )\n",
    "                \n",
    "                # Calculate MMR metrics\n",
    "                mmr_texts = [r.payload['text'] for r in mmr_results]\n",
    "                mmr_redundancy = calculate_redundancy(mmr_texts)\n",
    "                \n",
    "                mmr_categories = [r.payload.get('category', 'unknown') for r in mmr_results]\n",
    "                mmr_category_diversity = len(set(mmr_categories)) / len(mmr_categories)\n",
    "                mmr_top_score = mmr_results[0].score if mmr_results else 0\n",
    "                \n",
    "                # Calculate overlaps with original\n",
    "                original_ids = {r.id for r in original_results}\n",
    "                mmr_ids = {r.id for r in mmr_results}\n",
    "                overlap_ratio = len(original_ids & mmr_ids) / len(original_ids)\n",
    "                \n",
    "                # Store results\n",
    "                query_analysis[f'mmr_{lambda_val}_redundancy'] = mmr_redundancy\n",
    "                query_analysis[f'mmr_{lambda_val}_category_diversity'] = mmr_category_diversity\n",
    "                query_analysis[f'mmr_{lambda_val}_top_score'] = mmr_top_score\n",
    "                query_analysis[f'mmr_{lambda_val}_overlap'] = overlap_ratio\n",
    "                \n",
    "                redundancy_change = (original_redundancy - mmr_redundancy) / original_redundancy * 100 if original_redundancy > 0 else 0\n",
    "                query_analysis[f'mmr_{lambda_val}_redundancy_improvement'] = redundancy_change\n",
    "                \n",
    "                print(f\"   Œª={lambda_val}: redundancy {redundancy_change:+.1f}%, diversity {mmr_category_diversity:.2f}, overlap {overlap_ratio:.2f}\")\n",
    "            \n",
    "            analysis_results.append(query_analysis)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error analyzing query: {e}\")\n",
    "    \n",
    "    return analysis_results\n",
    "\n",
    "# Run comprehensive analysis\n",
    "print(\"üöÄ Running comprehensive MMR analysis...\")\n",
    "analysis_results = analyze_mmr_performance(test_queries)\n",
    "\n",
    "# Create summary DataFrame\n",
    "if analysis_results:\n",
    "    results_df = pd.DataFrame(analysis_results)\n",
    "    print(f\"\\nüìä Analysis complete: {len(results_df)} queries analyzed\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No analysis results generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà MMR Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'results_df' in locals() and len(results_df) > 0:\n",
    "    # Create comprehensive visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('MMR Reranking Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    lambda_values = [0.2, 0.5, 0.8]\n",
    "    colors = ['#e74c3c', '#f39c12', '#27ae60']  # Red, Orange, Green\n",
    "    \n",
    "    # 1. Redundancy Improvement by Lambda\n",
    "    ax1 = axes[0, 0]\n",
    "    lambda_improvements = []\n",
    "    for lam in lambda_values:\n",
    "        col = f'mmr_{lam}_redundancy_improvement'\n",
    "        if col in results_df.columns:\n",
    "            improvements = results_df[col].values\n",
    "            lambda_improvements.append(improvements)\n",
    "    \n",
    "    if lambda_improvements:\n",
    "        positions = np.arange(len(lambda_values))\n",
    "        bp = ax1.boxplot(lambda_improvements, positions=positions, patch_artist=True)\n",
    "        \n",
    "        for patch, color in zip(bp['boxes'], colors):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.7)\n",
    "        \n",
    "        ax1.set_xticklabels([f'Œª={lam}' for lam in lambda_values])\n",
    "        ax1.set_ylabel('Redundancy Improvement (%)')\n",
    "        ax1.set_title('Redundancy Reduction by Lambda')\n",
    "        ax1.axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # 2. Category Diversity Improvement\n",
    "    ax2 = axes[0, 1]\n",
    "    query_types = results_df['type'].unique()\n",
    "    x_pos = np.arange(len(query_types))\n",
    "    width = 0.25\n",
    "    \n",
    "    original_diversity = []\n",
    "    mmr_diversity_by_lambda = {lam: [] for lam in lambda_values}\n",
    "    \n",
    "    for qtype in query_types:\n",
    "        subset = results_df[results_df['type'] == qtype]\n",
    "        original_diversity.append(subset['original_category_diversity'].mean())\n",
    "        \n",
    "        for lam in lambda_values:\n",
    "            col = f'mmr_{lam}_category_diversity'\n",
    "            if col in subset.columns:\n",
    "                mmr_diversity_by_lambda[lam].append(subset[col].mean())\n",
    "            else:\n",
    "                mmr_diversity_by_lambda[lam].append(0)\n",
    "    \n",
    "    # Plot bars\n",
    "    ax2.bar(x_pos - width, original_diversity, width, label='Original', color='#95a5a6', alpha=0.7)\n",
    "    \n",
    "    for i, lam in enumerate(lambda_values):\n",
    "        ax2.bar(x_pos + (i * width), mmr_diversity_by_lambda[lam], width, \n",
    "               label=f'MMR Œª={lam}', color=colors[i], alpha=0.7)\n",
    "    \n",
    "    ax2.set_ylabel('Category Diversity')\n",
    "    ax2.set_title('Category Diversity by Query Type')\n",
    "    ax2.set_xticks(x_pos)\n",
    "    ax2.set_xticklabels(query_types, rotation=45)\n",
    "    ax2.legend()\n",
    "    \n",
    "    # 3. Top Score Preservation\n",
    "    ax3 = axes[1, 0]\n",
    "    score_ratios_by_lambda = []\n",
    "    \n",
    "    for lam in lambda_values:\n",
    "        score_col = f'mmr_{lam}_top_score'\n",
    "        if score_col in results_df.columns:\n",
    "            # Calculate score retention ratio\n",
    "            score_ratios = results_df[score_col] / results_df['original_top_score']\n",
    "            score_ratios = score_ratios[np.isfinite(score_ratios)]  # Remove inf/nan\n",
    "            score_ratios_by_lambda.append(score_ratios)\n",
    "    \n",
    "    if score_ratios_by_lambda:\n",
    "        bp2 = ax3.boxplot(score_ratios_by_lambda, patch_artist=True)\n",
    "        for patch, color in zip(bp2['boxes'], colors):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.7)\n",
    "        \n",
    "        ax3.set_xticklabels([f'Œª={lam}' for lam in lambda_values])\n",
    "        ax3.set_ylabel('Top Score Retention Ratio')\n",
    "        ax3.set_title('Relevance Preservation')\n",
    "        ax3.axhline(y=1.0, color='black', linestyle='--', alpha=0.3, label='Perfect retention')\n",
    "    \n",
    "    # 4. Result Overlap with Original\n",
    "    ax4 = axes[1, 1]\n",
    "    overlap_data = []\n",
    "    \n",
    "    for lam in lambda_values:\n",
    "        overlap_col = f'mmr_{lam}_overlap'\n",
    "        if overlap_col in results_df.columns:\n",
    "            overlap_data.append(results_df[overlap_col].values)\n",
    "    \n",
    "    if overlap_data:\n",
    "        bp3 = ax4.boxplot(overlap_data, patch_artist=True)\n",
    "        for patch, color in zip(bp3['boxes'], colors):\n",
    "            patch.set_facecolor(color)\n",
    "            patch.set_alpha(0.7)\n",
    "        \n",
    "        ax4.set_xticklabels([f'Œª={lam}' for lam in lambda_values])\n",
    "        ax4.set_ylabel('Overlap with Original (%)')\n",
    "        ax4.set_title('Result Set Stability')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\nüìä MMR Analysis Summary:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for lam in lambda_values:\n",
    "        improvement_col = f'mmr_{lam}_redundancy_improvement'\n",
    "        if improvement_col in results_df.columns:\n",
    "            avg_improvement = results_df[improvement_col].mean()\n",
    "            print(f\"\\nŒª={lam}:\")\n",
    "            print(f\"   üìâ Avg redundancy reduction: {avg_improvement:.1f}%\")\n",
    "            \n",
    "            diversity_col = f'mmr_{lam}_category_diversity'\n",
    "            if diversity_col in results_df.columns:\n",
    "                avg_diversity = results_df[diversity_col].mean()\n",
    "                orig_diversity = results_df['original_category_diversity'].mean()\n",
    "                diversity_improvement = (avg_diversity - orig_diversity) / orig_diversity * 100\n",
    "                print(f\"   üìà Avg diversity improvement: {diversity_improvement:+.1f}%\")\n",
    "            \n",
    "            overlap_col = f'mmr_{lam}_overlap'\n",
    "            if overlap_col in results_df.columns:\n",
    "                avg_overlap = results_df[overlap_col].mean()\n",
    "                print(f\"   üîÑ Avg result overlap: {avg_overlap:.1%}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ When MMR Helps vs When It Hurts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_mmr_effectiveness():\n",
    "    \"\"\"Analyze when MMR helps vs hurts based on query characteristics\"\"\"\n",
    "    \n",
    "    if 'results_df' not in locals() or len(results_df) == 0:\n",
    "        print(\"‚ö†Ô∏è No analysis data available\")\n",
    "        return\n",
    "    \n",
    "    print(\"üéØ When MMR Helps vs Hurts\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Group by query type\n",
    "    for query_type in results_df['type'].unique():\n",
    "        subset = results_df[results_df['type'] == query_type]\n",
    "        print(f\"\\nüìã {query_type.upper()} Queries:\")\n",
    "        \n",
    "        for _, row in subset.iterrows():\n",
    "            query_text = row['query']\n",
    "            print(f\"\\n   Query: '{query_text}'\")\n",
    "            \n",
    "            # Find best lambda for this query\n",
    "            best_lambda = None\n",
    "            best_improvement = -float('inf')\n",
    "            \n",
    "            lambda_performances = []\n",
    "            for lam in [0.2, 0.5, 0.8]:\n",
    "                improvement_col = f'mmr_{lam}_redundancy_improvement'\n",
    "                if improvement_col in row:\n",
    "                    improvement = row[improvement_col]\n",
    "                    lambda_performances.append((lam, improvement))\n",
    "                    \n",
    "                    if improvement > best_improvement:\n",
    "                        best_improvement = improvement\n",
    "                        best_lambda = lam\n",
    "            \n",
    "            # Show performance for each lambda\n",
    "            for lam, improvement in lambda_performances:\n",
    "                status = \"‚úÖ\" if improvement > 5 else \"‚ö†Ô∏è\" if improvement > 0 else \"‚ùå\"\n",
    "                overlap_col = f'mmr_{lam}_overlap'\n",
    "                overlap = row[overlap_col] if overlap_col in row else 0\n",
    "                \n",
    "                print(f\"     Œª={lam}: {status} {improvement:+.1f}% redundancy, {overlap:.1%} overlap\")\n",
    "            \n",
    "            # Recommendation\n",
    "            if best_improvement > 10:\n",
    "                print(f\"     üí° Recommendation: Use Œª={best_lambda} (strong improvement)\")\n",
    "            elif best_improvement > 0:\n",
    "                print(f\"     üí° Recommendation: Use Œª={best_lambda} (modest improvement)\")\n",
    "            else:\n",
    "                print(f\"     üí° Recommendation: Skip MMR (no improvement)\")\n",
    "    \n",
    "    # Overall recommendations\n",
    "    print(f\"\\nüìö General Guidelines:\")\n",
    "    print(f\"\\nüéØ AMBIGUOUS/BROAD Queries:\")\n",
    "    print(f\"   ‚Ä¢ MMR typically helps (Œª=0.3-0.5)\")\n",
    "    print(f\"   ‚Ä¢ Benefits: Reduces repetitive results\")\n",
    "    print(f\"   ‚Ä¢ Benefits: Covers different aspects\")\n",
    "    \n",
    "    print(f\"\\nüéØ SPECIFIC Queries:\")\n",
    "    print(f\"   ‚Ä¢ MMR may hurt (use Œª=0.7-0.8 if needed)\")\n",
    "    print(f\"   ‚Ä¢ Risk: May demote the most relevant results\")\n",
    "    print(f\"   ‚Ä¢ Alternative: Use score threshold instead\")\n",
    "    \n",
    "    print(f\"\\nüéØ RECOMMENDED Œª VALUES:\")\n",
    "    print(f\"   ‚Ä¢ Œª=0.8: Slight diversity, preserve relevance\")\n",
    "    print(f\"   ‚Ä¢ Œª=0.5: Balanced relevance and diversity\")\n",
    "    print(f\"   ‚Ä¢ Œª=0.3: Strong diversity, some relevance loss\")\n",
    "    print(f\"   ‚Ä¢ Œª=0.1: Maximum diversity (rarely recommended)\")\n",
    "\n",
    "analyze_mmr_effectiveness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Server-Side MMR (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_server_side_mmr():\n",
    "    \"\"\"Test server-side MMR if available in Qdrant\"\"\"\n",
    "    print(\"üîÑ Testing Server-Side MMR...\")\n",
    "    \n",
    "    try:\n",
    "        # This is hypothetical - actual server-side MMR API may differ\n",
    "        # from qdrant_client.models import SearchRequest, RerankRequest\n",
    "        \n",
    "        print(\"‚ö†Ô∏è  Server-side MMR not yet available in standard Qdrant client\")\n",
    "        print(\"\\nüìù Expected future API might look like:\")\n",
    "        print(\"\"\"    \n",
    "    # Hypothetical server-side MMR API\n",
    "    results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_vector,\n",
    "        limit=50,  # Get candidates\n",
    "        rerank={\n",
    "            \"method\": \"mmr\",\n",
    "            \"lambda\": 0.5,\n",
    "            \"final_limit\": 10\n",
    "        }\n",
    "    )\n",
    "    \"\"\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Current approach: Client-side MMR (as demonstrated above)\")\n",
    "        print(\"   ‚Ä¢ More flexible parameter tuning\")\n",
    "        print(\"   ‚Ä¢ Works with any vector similarity metric\")\n",
    "        print(\"   ‚Ä¢ Can combine with other reranking methods\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Server-side MMR test failed: {e}\")\n",
    "\n",
    "test_server_side_mmr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéõÔ∏è Parameter Tuning Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mmr_tuning_guide():\n",
    "    \"\"\"Create a comprehensive guide for MMR parameter tuning\"\"\"\n",
    "    \n",
    "    print(\"üéõÔ∏è MMR Parameter Tuning Guide\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Lambda tuning guide\n",
    "    lambda_guide = {\n",
    "        \"0.9-1.0\": {\n",
    "            \"focus\": \"Pure Relevance\", \n",
    "            \"use_case\": \"Specific technical queries\",\n",
    "            \"example\": \"'password reset steps'\",\n",
    "            \"pros\": \"Preserves exact relevance ranking\",\n",
    "            \"cons\": \"No diversity benefit\"\n",
    "        },\n",
    "        \"0.7-0.8\": {\n",
    "            \"focus\": \"Relevance-Heavy\",\n",
    "            \"use_case\": \"Domain-specific searches\", \n",
    "            \"example\": \"'HNSW algorithm optimization'\",\n",
    "            \"pros\": \"Slight diversity, high relevance\",\n",
    "            \"cons\": \"Limited diversity gains\"\n",
    "        },\n",
    "        \"0.4-0.6\": {\n",
    "            \"focus\": \"Balanced\",\n",
    "            \"use_case\": \"General-purpose search\",\n",
    "            \"example\": \"'machine learning techniques'\", \n",
    "            \"pros\": \"Good relevance-diversity trade-off\",\n",
    "            \"cons\": \"May need query-specific tuning\"\n",
    "        },\n",
    "        \"0.2-0.3\": {\n",
    "            \"focus\": \"Diversity-Heavy\",\n",
    "            \"use_case\": \"Exploratory/research queries\",\n",
    "            \"example\": \"'business optimization strategies'\",\n",
    "            \"pros\": \"High diversity, broad coverage\", \n",
    "            \"cons\": \"May sacrifice top relevance\"\n",
    "        },\n",
    "        \"0.0-0.1\": {\n",
    "            \"focus\": \"Pure Diversity\",\n",
    "            \"use_case\": \"Brainstorming/discovery\",\n",
    "            \"example\": \"'innovation approaches'\",\n",
    "            \"pros\": \"Maximum diversity\",\n",
    "            \"cons\": \"Poor relevance, rarely useful\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"\\nüìä Lambda (Œª) Value Guide:\")\n",
    "    for lambda_range, info in lambda_guide.items():\n",
    "        print(f\"\\nüéØ Œª = {lambda_range}: {info['focus']}\")\n",
    "        print(f\"   Use case: {info['use_case']}\")\n",
    "        print(f\"   Example: {info['example']}\")\n",
    "        print(f\"   ‚úÖ Pros: {info['pros']}\")\n",
    "        print(f\"   ‚ö†Ô∏è  Cons: {info['cons']}\")\n",
    "    \n",
    "    print(f\"\\nüîß Tuning Process:\")\n",
    "    print(f\"\\n1Ô∏è‚É£ BASELINE ANALYSIS:\")\n",
    "    print(f\"   ‚Ä¢ Measure redundancy in top-10 results\")\n",
    "    print(f\"   ‚Ä¢ Check category/topic diversity\")\n",
    "    print(f\"   ‚Ä¢ Note query type (specific/broad/ambiguous)\")\n",
    "    \n",
    "    print(f\"\\n2Ô∏è‚É£ LAMBDA SELECTION:\")\n",
    "    print(f\"   ‚Ä¢ Start with Œª=0.5 (balanced)\")\n",
    "    print(f\"   ‚Ä¢ If high redundancy ‚Üí decrease Œª (more diversity)\")\n",
    "    print(f\"   ‚Ä¢ If low relevance ‚Üí increase Œª (more relevance)\")\n",
    "    print(f\"   ‚Ä¢ If specific queries ‚Üí use Œª‚â•0.7\")\n",
    "    \n",
    "    print(f\"\\n3Ô∏è‚É£ EVALUATION METRICS:\")\n",
    "    print(f\"   ‚Ä¢ Redundancy score (target: <0.3 for diverse results)\")\n",
    "    print(f\"   ‚Ä¢ Top-1 relevance retention (target: >90%)\")\n",
    "    print(f\"   ‚Ä¢ Category diversity (target: context-dependent)\")\n",
    "    print(f\"   ‚Ä¢ User satisfaction (A/B testing)\")\n",
    "    \n",
    "    print(f\"\\n4Ô∏è‚É£ PRODUCTION CONSIDERATIONS:\")\n",
    "    print(f\"   ‚Ä¢ Cache MMR results for popular queries\")\n",
    "    print(f\"   ‚Ä¢ Consider query-adaptive Œª values\")\n",
    "    print(f\"   ‚Ä¢ Monitor latency impact (MMR adds computation)\")\n",
    "    print(f\"   ‚Ä¢ A/B test against non-MMR results\")\n",
    "    \n",
    "    # Create decision tree\n",
    "    print(f\"\\nüå≥ Decision Tree:\")\n",
    "    decision_tree = '''\n",
    "    Query Intent?\n",
    "    ‚îú‚îÄ‚îÄ Specific/Technical\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ High precision needed? ‚Üí Œª=0.8-0.9\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ Some diversity OK? ‚Üí Œª=0.6-0.7  \n",
    "    ‚îú‚îÄ‚îÄ General/Broad\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ Balanced results? ‚Üí Œª=0.4-0.6\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ Diverse exploration? ‚Üí Œª=0.2-0.4\n",
    "    ‚îî‚îÄ‚îÄ Ambiguous/Multi-meaning\n",
    "        ‚îú‚îÄ‚îÄ Cover all aspects? ‚Üí Œª=0.2-0.4\n",
    "        ‚îî‚îÄ‚îÄ Balanced approach? ‚Üí Œª=0.4-0.5\n",
    "    '''\n",
    "    print(decision_tree)\n",
    "\n",
    "create_mmr_tuning_guide()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Final Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final collection stats\n",
    "final_info = client.get_collection(active_collection)\n",
    "\n",
    "print(\"üéâ MMR Reranking Summary\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"\\nüìö Collection: {active_collection}\")\n",
    "print(f\"   üìä Total points: {final_info.points_count}\")\n",
    "print(f\"   üéØ Analysis queries: {len(test_queries)}\")\n",
    "\n",
    "print(f\"\\nüîÑ MMR Implementation:\")\n",
    "print(\"   ‚úÖ Client-side MMR reranking\")\n",
    "print(\"   ‚úÖ Configurable Œª parameter (0.0-1.0)\")\n",
    "print(\"   ‚úÖ Works with any vector similarity metric\")\n",
    "print(\"   ‚úÖ Integrates with hybrid search results\")\n",
    "\n",
    "if 'results_df' in locals() and len(results_df) > 0:\n",
    "    # Calculate overall improvements\n",
    "    lambda_05_improvement = results_df['mmr_0.5_redundancy_improvement'].mean()\n",
    "    best_improvements = []\n",
    "    \n",
    "    for lam in [0.2, 0.5, 0.8]:\n",
    "        col = f'mmr_{lam}_redundancy_improvement'\n",
    "        if col in results_df.columns:\n",
    "            best_improvements.append(results_df[col].max())\n",
    "    \n",
    "    print(f\"\\nüìà Performance Results:\")\n",
    "    print(f\"   üìâ Average redundancy reduction (Œª=0.5): {lambda_05_improvement:.1f}%\")\n",
    "    print(f\"   üèÜ Best single-query improvement: {max(best_improvements):.1f}%\")\n",
    "    \n",
    "    # Query type performance\n",
    "    print(f\"\\nüìã By Query Type:\")\n",
    "    for qtype in results_df['type'].unique():\n",
    "        subset = results_df[results_df['type'] == qtype]\n",
    "        avg_improvement = subset['mmr_0.5_redundancy_improvement'].mean()\n",
    "        print(f\"   {qtype:>12}: {avg_improvement:+.1f}% redundancy reduction\")\n",
    "\n",
    "print(f\"\\nüéØ Key Takeaways:\")\n",
    "print(\"   üîπ MMR reduces redundancy while preserving relevance\")\n",
    "print(\"   üîπ Œª parameter controls relevance vs diversity trade-off\")\n",
    "print(\"   üîπ Most effective for broad/ambiguous queries\")\n",
    "print(\"   üîπ Use Œª‚â•0.7 for specific technical queries\")\n",
    "print(\"   üîπ Use Œª‚â§0.4 for exploratory/research queries\")\n",
    "print(\"   üîπ A/B test to find optimal Œª for your domain\")\n",
    "\n",
    "print(f\"\\nüöÄ Ready for Notebook 4: HNSW Index Health!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéÆ Stretch Goals (Optional)\n",
    "\n",
    "Advanced MMR techniques and optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Query-Adaptive Lambda\n",
    "\n",
    "Implement automatic lambda selection based on query characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_lambda_selection(query_text: str, baseline_results: List) -> float:\n",
    "    \"\"\"Automatically select lambda based on query and initial results\"\"\"\n",
    "    \n",
    "    # Start with balanced approach\n",
    "    base_lambda = 0.5\n",
    "    adjustments = []\n",
    "    \n",
    "    # Query length analysis\n",
    "    word_count = len(query_text.split())\n",
    "    if word_count <= 2:\n",
    "        adjustments.append((0.2, \"Short query ‚Üí favor relevance\"))\n",
    "    elif word_count >= 6:\n",
    "        adjustments.append((-0.1, \"Long query ‚Üí more diversity OK\"))\n",
    "    \n",
    "    # Technical term detection\n",
    "    technical_terms = [\n",
    "        'algorithm', 'api', 'database', 'function', 'method', 'class',\n",
    "        'variable', 'parameter', 'configuration', 'installation', 'setup'\n",
    "    ]\n",
    "    technical_score = sum(1 for term in technical_terms if term.lower() in query_text.lower())\n",
    "    if technical_score > 0:\n",
    "        adjustment = min(0.3, technical_score * 0.1)\n",
    "        adjustments.append((adjustment, f\"Technical terms ‚Üí favor relevance\"))\n",
    "    \n",
    "    # Baseline results analysis\n",
    "    if len(baseline_results) >= 5:\n",
    "        # Check score distribution\n",
    "        scores = [r.score for r in baseline_results[:10]]\n",
    "        score_std = np.std(scores)\n",
    "        \n",
    "        if score_std < 0.1:  # Very similar scores\n",
    "            adjustments.append((-0.2, \"Similar scores ‚Üí increase diversity\"))\n",
    "        \n",
    "        # Check category diversity\n",
    "        categories = [r.payload.get('category', 'unknown') for r in baseline_results[:10]]\n",
    "        unique_ratio = len(set(categories)) / len(categories)\n",
    "        \n",
    "        if unique_ratio > 0.8:  # Already diverse\n",
    "            adjustments.append((0.1, \"Already diverse ‚Üí preserve relevance\"))\n",
    "        elif unique_ratio < 0.3:  # Very redundant\n",
    "            adjustments.append((-0.3, \"Low diversity ‚Üí increase MMR diversity\"))\n",
    "    \n",
    "    # Apply adjustments\n",
    "    final_lambda = base_lambda\n",
    "    print(f\"\\nü§ñ Adaptive Lambda for: '{query_text}'\")\n",
    "    print(f\"   Base Œª: {base_lambda}\")\n",
    "    \n",
    "    for adjustment, reason in adjustments:\n",
    "        final_lambda += adjustment\n",
    "        print(f\"   {adjustment:+.2f}: {reason}\")\n",
    "    \n",
    "    # Clamp to valid range\n",
    "    final_lambda = max(0.1, min(0.9, final_lambda))\n",
    "    \n",
    "    print(f\"   Final Œª: {final_lambda:.2f}\")\n",
    "    return final_lambda\n",
    "\n",
    "# Test adaptive lambda on our queries\n",
    "print(\"ü§ñ Adaptive Lambda Selection Examples:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for query in test_queries[:3]:\n",
    "    try:\n",
    "        candidates, vectors, qvec = get_baseline_candidates(query[\"text\"], limit=20)\n",
    "        adaptive_lambda = adaptive_lambda_selection(query[\"text\"], candidates)\n",
    "        \n",
    "        print(f\"\\n   Recommended for '{query['text']}': Œª={adaptive_lambda:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   Error processing '{query['text']}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö° Hybrid MMR: Multiple Vector Types\n",
    "\n",
    "Apply MMR using different vector spaces for relevance vs diversity calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_mmr_rerank(query_dense: np.ndarray, query_sparse: Dict[int, float],\n",
    "                     candidates: List, candidate_dense: np.ndarray, \n",
    "                     candidate_sparse: List[Dict[int, float]], \n",
    "                     lambda_param: float = 0.5, k: int = 10) -> List:\n",
    "    \"\"\"MMR using dense vectors for relevance, sparse for diversity (or vice versa)\"\"\"\n",
    "    \n",
    "    if len(candidates) == 0:\n",
    "        return []\n",
    "    \n",
    "    selected_indices = []\n",
    "    remaining_indices = list(range(len(candidates)))\n",
    "    \n",
    "    # Normalize dense vectors\n",
    "    query_dense_norm = query_dense / np.linalg.norm(query_dense)\n",
    "    candidate_dense_norm = candidate_dense / np.linalg.norm(candidate_dense, axis=1, keepdims=True)\n",
    "    \n",
    "    # Calculate relevance using dense vectors (semantic similarity)\n",
    "    relevance_scores = np.dot(candidate_dense_norm, query_dense_norm)\n",
    "    \n",
    "    # Start with most relevant\n",
    "    first_idx = np.argmax(relevance_scores)\n",
    "    selected_indices.append(first_idx)\n",
    "    remaining_indices.remove(first_idx)\n",
    "    \n",
    "    print(f\"üéØ Hybrid MMR: Using dense for relevance, dense for diversity\")\n",
    "    print(f\"   (In practice, you might use sparse vectors for diversity calculation)\")\n",
    "    \n",
    "    # Iteratively select based on hybrid MMR\n",
    "    for _ in range(min(k - 1, len(remaining_indices))):\n",
    "        mmr_scores = []\n",
    "        \n",
    "        for idx in remaining_indices:\n",
    "            # Relevance from dense vectors\n",
    "            relevance = relevance_scores[idx]\n",
    "            \n",
    "            # Diversity from dense vectors (could use sparse here)\n",
    "            max_sim = 0\n",
    "            for sel_idx in selected_indices:\n",
    "                # Using dense similarity for diversity calculation\n",
    "                sim = np.dot(candidate_dense_norm[idx], candidate_dense_norm[sel_idx])\n",
    "                max_sim = max(max_sim, sim)\n",
    "            \n",
    "            # MMR score\n",
    "            mmr_score = lambda_param * relevance - (1 - lambda_param) * max_sim\n",
    "            mmr_scores.append((idx, mmr_score))\n",
    "        \n",
    "        # Select best MMR score\n",
    "        best_idx = max(mmr_scores, key=lambda x: x[1])[0]\n",
    "        selected_indices.append(best_idx)\n",
    "        remaining_indices.remove(best_idx)\n",
    "    \n",
    "    return [candidates[i] for i in selected_indices]\n",
    "\n",
    "# This is a conceptual implementation - in practice you'd need both vector types\n",
    "print(\"üî¨ Hybrid MMR Concept:\")\n",
    "print(\"=\" * 30)\n",
    "print(\"\\nüí° Advanced MMR Ideas:\")\n",
    "print(\"   ‚Ä¢ Use dense vectors for relevance scoring\")\n",
    "print(\"   ‚Ä¢ Use sparse vectors for diversity calculation\")\n",
    "print(\"   ‚Ä¢ Combine topic modeling with vector similarity\")\n",
    "print(\"   ‚Ä¢ Apply different Œª values for different content types\")\n",
    "print(\"   ‚Ä¢ Use metadata (categories, tags) for diversity constraints\")\n",
    "\n",
    "print(\"\\nüöß Implementation Notes:\")\n",
    "print(\"   ‚Ä¢ Requires access to multiple vector representations\")\n",
    "print(\"   ‚Ä¢ Can significantly improve diversity in specialized domains\")\n",
    "print(\"   ‚Ä¢ Increases computational complexity\")\n",
    "print(\"   ‚Ä¢ Best tested with domain-specific evaluation metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collection cleanup is optional since we're reusing existing collections\n",
    "print(f\"üíæ Preserving collection: {active_collection}\")\n",
    "print(f\"\\n‚ú® Notebook 3 complete! Move on to 04_hnsw_health.ipynb\")\n",
    "print(f\"\\nüéØ Next: Learn how to maintain and optimize HNSW index performance!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
