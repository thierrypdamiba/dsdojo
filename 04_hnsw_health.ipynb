{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4: HNSW Index Health - Churn, Degradation & Healing\n",
    "\n",
    "## 🎯 Objectives\n",
    "\n",
    "In this notebook, you'll learn:\n",
    "- How HNSW (Hierarchical Navigable Small World) graphs work\n",
    "- Why heavy updates/deletes can reduce recall quality\n",
    "- How to measure index health and performance degradation\n",
    "- Techniques for \"healing\" or rebuilding degraded indexes\n",
    "- Monitoring strategies for production vector databases\n",
    "- HNSW parameter tuning for optimal performance\n",
    "\n",
    "## 📋 Prerequisites\n",
    "\n",
    "- Understanding of vector similarity search\n",
    "- Basic knowledge of approximate nearest neighbor (ANN) algorithms\n",
    "- A collection with several thousand points for meaningful analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import (\n",
    "    get_qdrant_client, ensure_collection, create_sample_dataset,\n",
    "    upsert_points_batch, search_dense, exact_topk, \n",
    "    calculate_recall_at_k, measure_latency, print_system_info\n",
    ")\n",
    "\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct, HnswConfigDiff, OptimizersConfigDiff\n",
    "\n",
    "print_system_info()\n",
    "print(\"\\n🔧 HNSW Index Health Workshop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📦 Install Dependencies (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if running in a fresh environment\n",
    "# !pip install qdrant-client numpy pandas matplotlib tqdm scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 HNSW Theory & Background\n",
    "\n",
    "### What is HNSW?\n",
    "\n",
    "HNSW (Hierarchical Navigable Small World) is a graph-based algorithm for approximate nearest neighbor search:\n",
    "\n",
    "- **Graph Structure**: Each vector is a node connected to nearby vectors\n",
    "- **Hierarchical**: Multiple layers with different connection densities\n",
    "- **Navigable**: Search follows graph edges to find nearest neighbors\n",
    "- **Small World**: Short paths between any two nodes\n",
    "\n",
    "### Why Does Index Health Matter?\n",
    "\n",
    "- **Updates/Deletes**: Can create isolated nodes or suboptimal connections\n",
    "- **Recall Degradation**: Poor connections lead to missed relevant results\n",
    "- **Performance Impact**: Longer search paths increase latency\n",
    "- **Production Reliability**: Degraded indexes hurt user experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_hnsw_degradation():\n",
    "    \"\"\"Visual explanation of HNSW degradation\"\"\"\n",
    "    print(\"🏗️ HNSW Index Health Concepts\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    print(\"\\n📍 Healthy HNSW Index:\")\n",
    "    print(\"   • Well-connected graph structure\")\n",
    "    print(\"   • Short paths between similar vectors\")\n",
    "    print(\"   • Balanced layer distribution\")\n",
    "    print(\"   • High recall with efficient search\")\n",
    "    \n",
    "    print(\"\\n⚠️ Common Degradation Causes:\")\n",
    "    print(\"   1. Point Deletions:\")\n",
    "    print(\"      • Remove nodes and their connections\")\n",
    "    print(\"      • Can isolate clusters of vectors\")\n",
    "    print(\"      • May break optimal search paths\")\n",
    "    \n",
    "    print(\"   2. Frequent Updates:\")\n",
    "    print(\"      • Change vector positions in space\")\n",
    "    print(\"      • Old connections become suboptimal\")\n",
    "    print(\"      • Graph topology lags behind data\")\n",
    "    \n",
    "    print(\"   3. Unbalanced Insertions:\")\n",
    "    print(\"      • New vectors in sparse regions\")\n",
    "    print(\"      • Poor initial connections\")\n",
    "    print(\"      • Uneven graph density\")\n",
    "    \n",
    "    print(\"\\n📊 Degradation Symptoms:\")\n",
    "    print(\"   • Reduced recall@k (miss relevant results)\")\n",
    "    print(\"   • Increased search latency (longer paths)\")\n",
    "    print(\"   • Inconsistent performance across queries\")\n",
    "    print(\"   • Higher memory usage per connection\")\n",
    "    \n",
    "    print(\"\\n🔧 Healing Strategies:\")\n",
    "    print(\"   1. Index Optimization:\")\n",
    "    print(\"      • Reconnect isolated nodes\")\n",
    "    print(\"      • Prune poor connections\")\n",
    "    print(\"      • Rebalance layer distribution\")\n",
    "    \n",
    "    print(\"   2. Full Rebuild:\")\n",
    "    print(\"      • Create fresh index from scratch\")\n",
    "    print(\"      • Optimal connections for current data\")\n",
    "    print(\"      • Reset to baseline performance\")\n",
    "\n",
    "explain_hnsw_degradation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚙️ Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "COLLECTION_NAME = \"workshop_health\"\n",
    "VECTOR_SIZE = 384\n",
    "TARGET_POINTS = 1000  # Reduced size to avoid timeout issues\n",
    "\n",
    "# HNSW Parameters for testing\n",
    "HNSW_CONFIG = {\n",
    "    \"m\": 16,          # Number of bi-directional links for each node\n",
    "    \"ef_construct\": 200,  # Size of dynamic candidate list during construction\n",
    "    \"full_scan_threshold\": 10000  # When to use brute force vs HNSW\n",
    "}\n",
    "\n",
    "# Connect to Qdrant\n",
    "client = get_qdrant_client()\n",
    "\n",
    "print(f\"🔗 Connected to Qdrant\")\n",
    "print(f\"📁 Collection: {COLLECTION_NAME}\")\n",
    "print(f\"🎯 Target size: {TARGET_POINTS} points\")\n",
    "print(f\"⚙️ HNSW config: {HNSW_CONFIG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏗️ Build Large-Scale Collection\n",
    "\n",
    "Create a collection with enough points to observe HNSW degradation effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_large_dataset(size: int, seed: int = 42) -> Tuple[pd.DataFrame, np.ndarray]:\n",
    "    \"\"\"Create a large dataset by replicating and adding noise to base data\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Create base dataset\n",
    "    base_df = create_sample_dataset(size=200, seed=seed)\n",
    "    base_size = len(base_df)\n",
    "    \n",
    "    print(f\"📊 Creating large dataset from {base_size} base samples...\")\n",
    "    \n",
    "    # Calculate how many replications we need\n",
    "    replications = max(1, size // base_size)\n",
    "    remainder = size % base_size\n",
    "    \n",
    "    expanded_data = []\n",
    "    all_vectors = []\n",
    "    point_id = 1\n",
    "    \n",
    "    # Generate base vectors\n",
    "    base_vectors = np.random.randn(base_size, VECTOR_SIZE)\n",
    "    base_vectors = base_vectors / np.linalg.norm(base_vectors, axis=1, keepdims=True)\n",
    "    \n",
    "    for rep in range(replications):\n",
    "        for idx, row in base_df.iterrows():\n",
    "            # Add slight noise to create variations\n",
    "            noise_scale = 0.1 + (rep * 0.05)  # Increase noise with each replication\n",
    "            noise = np.random.normal(0, noise_scale, VECTOR_SIZE)\n",
    "            noisy_vector = base_vectors[idx] + noise\n",
    "            noisy_vector = noisy_vector / np.linalg.norm(noisy_vector)  # Renormalize\n",
    "            \n",
    "            # Create varied text\n",
    "            base_text = row['text']\n",
    "            if rep > 0:\n",
    "                variations = [\n",
    "                    f\"{base_text} (variant {rep})\",\n",
    "                    f\"Updated: {base_text}\",\n",
    "                    f\"{base_text} - version {rep + 1}\",\n",
    "                    f\"Modified {base_text}\"\n",
    "                ]\n",
    "                text = np.random.choice(variations)\n",
    "            else:\n",
    "                text = base_text\n",
    "            \n",
    "            expanded_data.append({\n",
    "                'id': point_id,\n",
    "                'text': text,\n",
    "                'category': row['category'],\n",
    "                'lang': row['lang'],\n",
    "                'timestamp': row['timestamp'] + (rep * 86400),  # Spread timestamps\n",
    "                'replica': rep\n",
    "            })\n",
    "            \n",
    "            all_vectors.append(noisy_vector)\n",
    "            point_id += 1\n",
    "            \n",
    "            if len(expanded_data) >= size:\n",
    "                break\n",
    "        \n",
    "        if len(expanded_data) >= size:\n",
    "            break\n",
    "    \n",
    "    # Handle remainder if needed\n",
    "    if len(expanded_data) < size and remainder > 0:\n",
    "        for idx in range(remainder):\n",
    "            row = base_df.iloc[idx]\n",
    "            noise = np.random.normal(0, 0.2, VECTOR_SIZE)\n",
    "            noisy_vector = base_vectors[idx] + noise\n",
    "            noisy_vector = noisy_vector / np.linalg.norm(noisy_vector)\n",
    "            \n",
    "            expanded_data.append({\n",
    "                'id': point_id,\n",
    "                'text': f\"{row['text']} (final variant)\",\n",
    "                'category': row['category'],\n",
    "                'lang': row['lang'],\n",
    "                'timestamp': row['timestamp'] + (replications * 86400),\n",
    "                'replica': replications\n",
    "            })\n",
    "            \n",
    "            all_vectors.append(noisy_vector)\n",
    "            point_id += 1\n",
    "    \n",
    "    final_df = pd.DataFrame(expanded_data)\n",
    "    final_vectors = np.array(all_vectors)\n",
    "    \n",
    "    print(f\"✅ Created dataset: {len(final_df)} points, {final_vectors.shape[1]}D vectors\")\n",
    "    print(f\"📊 Replications: {replications}, Categories: {final_df['category'].nunique()}\")\n",
    "    \n",
    "    return final_df, final_vectors\n",
    "\n",
    "# Create large dataset\n",
    "print(f\"🏗️ Building large-scale dataset...\")\n",
    "large_df, large_vectors = create_large_dataset(TARGET_POINTS)\n",
    "\n",
    "print(f\"\\n📈 Dataset statistics:\")\n",
    "print(f\"   Points: {len(large_df)}\")\n",
    "print(f\"   Vector dim: {large_vectors.shape[1]}\")\n",
    "print(f\"   Categories: {large_df['category'].value_counts().to_dict()}\")\n",
    "print(f\"   Replicas: {large_df['replica'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏗️ Create Health Collection with HNSW Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collection with specific HNSW parameters\n",
    "vector_config = VectorParams(\n",
    "    size=VECTOR_SIZE,\n",
    "    distance=Distance.COSINE,\n",
    "    hnsw_config=HnswConfigDiff(\n",
    "        m=HNSW_CONFIG[\"m\"],\n",
    "        ef_construct=HNSW_CONFIG[\"ef_construct\"],\n",
    "        full_scan_threshold=HNSW_CONFIG[\"full_scan_threshold\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create collection\n",
    "ensure_collection(\n",
    "    client=client,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vector_config=vector_config,\n",
    "    force_recreate=True\n",
    ")\n",
    "\n",
    "# Upload data in batches\n",
    "print(f\"\\n📤 Uploading {len(large_df)} points...\")\n",
    "start_time = time.time()\n",
    "\n",
    "upsert_points_batch(\n",
    "    client=client,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    df=large_df,\n",
    "    vectors=large_vectors,\n",
    "    payload_cols=['text', 'category', 'lang', 'timestamp', 'replica'],\n",
    "    batch_size=100\n",
    ")\n",
    "\n",
    "upload_time = time.time() - start_time\n",
    "print(f\"\\n✅ Upload complete in {upload_time:.2f}s\")\n",
    "\n",
    "# Verify collection\n",
    "info = client.get_collection(COLLECTION_NAME)\n",
    "print(f\"\\n📊 Collection verification:\")\n",
    "print(f\"   Points: {info.points_count}\")\n",
    "print(f\"   Status: {info.status}\")\n",
    "if hasattr(info.config.params, 'vectors'):\n",
    "    hnsw_config = info.config.params.vectors.hnsw_config\n",
    "    if hnsw_config:\n",
    "        print(f\"   HNSW M: {hnsw_config.m}\")\n",
    "        print(f\"   HNSW ef_construct: {hnsw_config.ef_construct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Ground Truth Generation\n",
    "\n",
    "Create exact search results for recall measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ground_truth_queries(vectors: np.ndarray, n_queries: int = 50, seed: int = 42) -> Tuple[np.ndarray, List[int]]:\n",
    "    \"\"\"Create test queries and their ground truth exact results\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Select random vectors as queries\n",
    "    query_indices = np.random.choice(len(vectors), size=n_queries, replace=False)\n",
    "    query_vectors = vectors[query_indices]\n",
    "    \n",
    "    print(f\"🎯 Creating ground truth for {n_queries} queries...\")\n",
    "    \n",
    "    ground_truth = []\n",
    "    \n",
    "    for i, query_vector in tqdm(enumerate(query_vectors), desc=\"Computing exact results\", total=len(query_vectors)):\n",
    "        # Get exact top-k using brute force\n",
    "        exact_results = exact_topk(query_vector, vectors, k=20)\n",
    "        # Extract just the indices (the second element is the score)\n",
    "        exact_ids = [idx for idx, score in exact_results]\n",
    "        ground_truth.append(exact_ids)\n",
    "    \n",
    "    print(f\"✅ Ground truth created: {len(ground_truth)} query result sets\")\n",
    "    return query_vectors, ground_truth\n",
    "\n",
    "# Create ground truth (use subset of vectors for faster computation)\n",
    "subset_size = min(2000, len(large_vectors))  # Limit for faster ground truth computation\n",
    "subset_indices = np.random.choice(len(large_vectors), size=subset_size, replace=False)\n",
    "subset_vectors = large_vectors[subset_indices]\n",
    "\n",
    "print(f\"📊 Using subset of {subset_size} vectors for ground truth computation\")\n",
    "query_vectors, ground_truth = create_ground_truth_queries(subset_vectors, n_queries=30)\n",
    "\n",
    "print(f\"\\n🎯 Ground truth sample:\")\n",
    "print(f\"   Query 0 top-5 exact IDs: {ground_truth[0][:5]}\")\n",
    "print(f\"   Query 1 top-5 exact IDs: {ground_truth[1][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Baseline HNSW Performance\n",
    "\n",
    "Measure recall and latency before any degradation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_search_performance(query_vectors: np.ndarray, ground_truth: List[List[int]], \n",
    "                             ef_search_values: List[int] = [50, 100, 200], k: int = 10) -> Dict:\n",
    "    \"\"\"Measure recall@k and latency for different ef_search values\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for ef_search in ef_search_values:\n",
    "        print(f\"\\n🔍 Testing ef_search = {ef_search}\")\n",
    "        \n",
    "        recalls = []\n",
    "        latencies = []\n",
    "        \n",
    "        for i, query_vector in enumerate(query_vectors[:20]):  # Test subset for speed\n",
    "            # Measure latency\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Search with specific ef_search (this is Qdrant-specific)\n",
    "            try:\n",
    "                search_results = client.search(\n",
    "                    collection_name=COLLECTION_NAME,\n",
    "                    query_vector=query_vector.tolist(),\n",
    "                    limit=k,\n",
    "                    search_params={\"hnsw\": {\"ef\": ef_search}}  # Qdrant-specific parameter\n",
    "                )\n",
    "            except Exception as e:\n",
    "                # Fallback without ef parameter if not supported\n",
    "                search_results = client.search(\n",
    "                    collection_name=COLLECTION_NAME,\n",
    "                    query_vector=query_vector.tolist(),\n",
    "                    limit=k\n",
    "                )\n",
    "            \n",
    "            latency = (time.time() - start_time) * 1000  # Convert to ms\n",
    "            latencies.append(latency)\n",
    "            \n",
    "            # Calculate recall@k\n",
    "            # Note: We need to map back to our subset indices\n",
    "            predicted_ids = []\n",
    "            for result in search_results:\n",
    "                # Find the index in our subset that corresponds to this result\n",
    "                # This is a simplification - in practice you'd maintain proper ID mapping\n",
    "                predicted_ids.append(result.id % len(subset_vectors))  # Map to subset space\n",
    "            \n",
    "            if i < len(ground_truth):\n",
    "                recall = calculate_recall_at_k(predicted_ids, ground_truth[i], k)\n",
    "                recalls.append(recall)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        avg_recall = np.mean(recalls) if recalls else 0\n",
    "        avg_latency = np.mean(latencies)\n",
    "        p95_latency = np.percentile(latencies, 95)\n",
    "        \n",
    "        results[ef_search] = {\n",
    "            'recall@10': avg_recall,\n",
    "            'avg_latency_ms': avg_latency,\n",
    "            'p95_latency_ms': p95_latency,\n",
    "            'queries_tested': len(recalls)\n",
    "        }\n",
    "        \n",
    "        print(f\"   Recall@{k}: {avg_recall:.3f}\")\n",
    "        print(f\"   Avg latency: {avg_latency:.2f}ms\")\n",
    "        print(f\"   P95 latency: {p95_latency:.2f}ms\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Measure baseline performance\n",
    "print(\"📊 Measuring baseline HNSW performance...\")\n",
    "baseline_performance = measure_search_performance(query_vectors, ground_truth)\n",
    "\n",
    "# Display baseline results\n",
    "print(f\"\\n📈 Baseline Performance Summary:\")\n",
    "baseline_df = pd.DataFrame(baseline_performance).T\n",
    "print(baseline_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚡ Simulate Index Churn\n",
    "\n",
    "Create realistic degradation through deletions and updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_index_churn(df: pd.DataFrame, vectors: np.ndarray, \n",
    "                        delete_ratio: float = 0.20, update_ratio: float = 0.15) -> Tuple[List[int], List[PointStruct]]:\n",
    "    \"\"\"Simulate realistic index churn through deletions and updates\"\"\"\n",
    "    \n",
    "    total_points = len(df)\n",
    "    n_deletes = int(total_points * delete_ratio)\n",
    "    n_updates = int(total_points * update_ratio)\n",
    "    \n",
    "    print(f\"🌪️ Simulating index churn:\")\n",
    "    print(f\"   Total points: {total_points}\")\n",
    "    print(f\"   Deletions: {n_deletes} ({delete_ratio:.1%})\")\n",
    "    print(f\"   Updates: {n_updates} ({update_ratio:.1%})\")\n",
    "    \n",
    "    # Select points to delete (avoid first few for query consistency)\n",
    "    safe_start = 100  # Keep some points safe for testing\n",
    "    delete_candidates = list(range(safe_start, len(df)))\n",
    "    delete_ids = random.sample(delete_candidates, n_deletes)\n",
    "    \n",
    "    # Select points to update (different from delete set)\n",
    "    remaining_candidates = [i for i in delete_candidates if i not in delete_ids]\n",
    "    update_indices = random.sample(remaining_candidates, min(n_updates, len(remaining_candidates)))\n",
    "    \n",
    "    print(f\"\\n🗑️ Deleting {len(delete_ids)} points...\")\n",
    "    \n",
    "    # Perform deletions in batches\n",
    "    batch_size = 50\n",
    "    for i in tqdm(range(0, len(delete_ids), batch_size), desc=\"Deleting points\"):\n",
    "        batch_ids = delete_ids[i:i + batch_size]\n",
    "        # Map indices to actual IDs\n",
    "        actual_ids = [int(df.iloc[idx]['id']) for idx in batch_ids if idx < len(df)]\n",
    "        if actual_ids:\n",
    "            client.delete(\n",
    "                collection_name=COLLECTION_NAME,\n",
    "                points_selector=[int(x) for x in actual_ids]\n",
    "            )\n",
    "    \n",
    "    print(f\"\\n🔄 Updating {len(update_indices)} points...\")\n",
    "    \n",
    "    # Create updated points with new vectors\n",
    "    updated_points = []\n",
    "    for idx in update_indices:\n",
    "        if idx < len(df) and idx < len(vectors):\n",
    "            row = df.iloc[idx]\n",
    "            \n",
    "            # Create new vector with significant noise\n",
    "            original_vector = vectors[idx]\n",
    "            noise = np.random.normal(0, 0.3, len(original_vector))  # Significant perturbation\n",
    "            new_vector = original_vector + noise\n",
    "            new_vector = new_vector / np.linalg.norm(new_vector)  # Renormalize\n",
    "            \n",
    "            # Create updated point (cast numpy scalars to native types)\n",
    "            updated_points.append(PointStruct(\n",
    "                id=int(row['id']),\n",
    "                vector=new_vector.tolist(),\n",
    "                payload={\n",
    "                    'text': f\"{str(row['text'])} (updated)\",\n",
    "                    'category': str(row['category']),\n",
    "                    'lang': str(row['lang']),\n",
    "                    'timestamp': int(time.time()),  # Current timestamp\n",
    "                    'replica': int(row['replica'])\n",
    "                }\n",
    "            ))\n",
    "    \n",
    "    # Perform updates in batches\n",
    "    for i in tqdm(range(0, len(updated_points), batch_size), desc=\"Updating points\"):\n",
    "        batch = updated_points[i:i + batch_size]\n",
    "        client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=batch\n",
    "        )\n",
    "    \n",
    "    # Generate some new points to simulate fresh inserts\n",
    "    print(f\"\\n➕ Adding {n_deletes // 2} new points...\")\n",
    "    new_points = []\n",
    "    max_id = int(df['id'].max()) + 1\n",
    "    \n",
    "    for i in range(n_deletes // 2):\n",
    "        # Random new vector\n",
    "        new_vector = np.random.randn(VECTOR_SIZE)\n",
    "        new_vector = new_vector / np.linalg.norm(new_vector)\n",
    "        \n",
    "        new_points.append(PointStruct(\n",
    "            id=int(max_id + i),\n",
    "            vector=new_vector.tolist(),\n",
    "            payload={\n",
    "                'text': f\"New document {i} added during churn\",\n",
    "                'category': random.choice(['product', 'faq', 'technical']),\n",
    "                'lang': 'en',\n",
    "                'timestamp': int(time.time()),\n",
    "                'replica': -1  # Mark as new\n",
    "            }\n",
    "        ))\n",
    "    \n",
    "    # Insert new points\n",
    "    for i in range(0, len(new_points), batch_size):\n",
    "        batch = new_points[i:i + batch_size]\n",
    "        client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=batch\n",
    "        )\n",
    "    \n",
    "    return delete_ids, updated_points + new_points\n",
    "\n",
    "# Simulate churn\n",
    "print(f\"🌪️ Starting index churn simulation...\")\n",
    "deleted_indices, modified_points = simulate_index_churn(large_df, large_vectors)\n",
    "\n",
    "# Verify churn impact\n",
    "info_after_churn = client.get_collection(COLLECTION_NAME)\n",
    "print(f\"\\n📊 Collection after churn:\")\n",
    "print(f\"   Points before: {len(large_df)}\")\n",
    "print(f\"   Points after: {info_after_churn.points_count}\")\n",
    "print(f\"   Points deleted: {len(deleted_indices)}\")\n",
    "print(f\"   Points modified/added: {len(modified_points)}\")\n",
    "\n",
    "churn_ratio = (len(deleted_indices) + len(modified_points)) / len(large_df)\n",
    "print(f\"   Total churn: {churn_ratio:.1%} of original data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📉 Measure Degradation\n",
    "\n",
    "Compare performance after churn to baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure performance after churn\n",
    "print(\"📉 Measuring performance after churn...\")\n",
    "post_churn_performance = measure_search_performance(query_vectors, ground_truth)\n",
    "\n",
    "# Compare performance\n",
    "print(f\"\\n📊 Performance Comparison (Before vs After Churn):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "comparison_data = []\n",
    "for ef_search in [50, 100, 200]:\n",
    "    if ef_search in baseline_performance and ef_search in post_churn_performance:\n",
    "        baseline = baseline_performance[ef_search]\n",
    "        post_churn = post_churn_performance[ef_search]\n",
    "        \n",
    "        recall_change = (post_churn['recall@10'] - baseline['recall@10']) / baseline['recall@10'] * 100\n",
    "        latency_change = (post_churn['avg_latency_ms'] - baseline['avg_latency_ms']) / baseline['avg_latency_ms'] * 100\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'ef_search': ef_search,\n",
    "            'baseline_recall': baseline['recall@10'],\n",
    "            'post_churn_recall': post_churn['recall@10'],\n",
    "            'recall_change_%': recall_change,\n",
    "            'baseline_latency_ms': baseline['avg_latency_ms'],\n",
    "            'post_churn_latency_ms': post_churn['avg_latency_ms'],\n",
    "            'latency_change_%': latency_change\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nef_search = {ef_search}:\")\n",
    "        print(f\"   Recall@10: {baseline['recall@10']:.3f} → {post_churn['recall@10']:.3f} ({recall_change:+.1f}%)\")\n",
    "        print(f\"   Avg Latency: {baseline['avg_latency_ms']:.2f}ms → {post_churn['avg_latency_ms']:.2f}ms ({latency_change:+.1f}%)\")\n",
    "        \n",
    "        if recall_change < -5:\n",
    "            print(f\"   ⚠️  Significant recall degradation detected!\")\n",
    "        if latency_change > 20:\n",
    "            print(f\"   ⚠️  Significant latency increase detected!\")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "if comparison_data:\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(f\"\\n📋 Degradation Summary:\")\n",
    "    print(comparison_df.round(3))\n",
    "    \n",
    "    # Overall degradation assessment\n",
    "    avg_recall_degradation = comparison_df['recall_change_%'].mean()\n",
    "    avg_latency_increase = comparison_df['latency_change_%'].mean()\n",
    "    \n",
    "    print(f\"\\n🎯 Overall Index Health:\")\n",
    "    print(f\"   Average recall degradation: {avg_recall_degradation:.1f}%\")\n",
    "    print(f\"   Average latency increase: {avg_latency_increase:.1f}%\")\n",
    "    \n",
    "    if avg_recall_degradation < -10 or avg_latency_increase > 25:\n",
    "        print(f\"   🚨 Index healing recommended!\")\n",
    "    elif avg_recall_degradation < -5 or avg_latency_increase > 15:\n",
    "        print(f\"   ⚠️  Index performance declining, consider healing\")\n",
    "    else:\n",
    "        print(f\"   ✅ Index health acceptable\")\n",
    "else:\n",
    "    print(\"⚠️ No comparison data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Index Healing/Optimization\n",
    "\n",
    "Apply healing strategies to restore index performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_index_healing(collection_name: str, method: str = \"optimize\"):\n",
    "    \"\"\"Perform index healing using available methods\"\"\"\n",
    "    \n",
    "    print(f\"🔧 Performing index healing: {method}\")\n",
    "    \n",
    "    if method == \"optimize\":\n",
    "        print(\"   Attempting collection optimization...\")\n",
    "        try:\n",
    "            # This is the preferred method if available\n",
    "            # Note: API may vary by Qdrant version\n",
    "            optimization_result = client.update_collection(\n",
    "                collection_name=collection_name,\n",
    "                optimizer_config=OptimizersConfigDiff(\n",
    "                    indexing_threshold=0,  # Force immediate indexing\n",
    "                )\n",
    "            )\n",
    "            print(f\"   ✅ Optimization triggered successfully\")\n",
    "            \n",
    "            # Wait for optimization to complete\n",
    "            print(\"   ⏳ Waiting for optimization to complete...\")\n",
    "            time.sleep(10)  # Give some time for processing\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ Optimization method failed: {e}\")\n",
    "            print(f\"   🔄 Falling back to rebuild method\")\n",
    "            method = \"rebuild\"\n",
    "    \n",
    "    if method == \"rebuild\":\n",
    "        print(\"   Performing index rebuild...\")\n",
    "        try:\n",
    "            # Get current collection info and data\n",
    "            collection_info = client.get_collection(collection_name)\n",
    "            \n",
    "            # Scroll through all points to backup\n",
    "            print(\"   📤 Backing up collection data...\")\n",
    "            all_points = []\n",
    "            offset = None\n",
    "            \n",
    "            while True:\n",
    "                scroll_result = client.scroll(\n",
    "                    collection_name=collection_name,\n",
    "                    limit=1000,\n",
    "                    offset=offset,\n",
    "                    with_vectors=True,\n",
    "                    with_payload=True\n",
    "                )\n",
    "                points, next_offset = scroll_result\n",
    "                \n",
    "                if not points:\n",
    "                    break\n",
    "                \n",
    "                all_points.extend(points)\n",
    "                offset = next_offset\n",
    "                \n",
    "                if next_offset is None:\n",
    "                    break\n",
    "            \n",
    "            print(f\"   📊 Backed up {len(all_points)} points\")\n",
    "            \n",
    "            # Recreate collection\n",
    "            temp_collection = f\"{collection_name}_temp\"\n",
    "            client.recreate_collection(\n",
    "                collection_name=temp_collection,\n",
    "                vectors_config=collection_info.config.params.vectors,\n",
    "                hnsw_config=collection_info.config.params.vectors.hnsw_config\n",
    "            )\n",
    "            \n",
    "            # Restore data in batches\n",
    "            print(\"   📥 Restoring data to fresh index...\")\n",
    "            batch_size = 100\n",
    "            \n",
    "            for i in tqdm(range(0, len(all_points), batch_size), desc=\"Restoring\"):\n",
    "                batch = all_points[i:i + batch_size]\n",
    "                client.upsert(\n",
    "                    collection_name=temp_collection,\n",
    "                    points=batch\n",
    "                )\n",
    "            \n",
    "            # Swap collections (delete old, rename new)\n",
    "            client.delete_collection(collection_name)\n",
    "            \n",
    "            # Rename temp collection (this might not be available in all versions)\n",
    "            try:\n",
    "                # If rename is available\n",
    "                client.update_collection(\n",
    "                    collection_name=temp_collection,\n",
    "                    # Rename operation - this API may not exist\n",
    "                )\n",
    "            except:\n",
    "                # Fallback: recreate original and copy data\n",
    "                client.recreate_collection(\n",
    "                    collection_name=collection_name,\n",
    "                    vectors_config=collection_info.config.params.vectors,\n",
    "                    hnsw_config=collection_info.config.params.vectors.hnsw_config\n",
    "                )\n",
    "                \n",
    "                # Copy all points to original name\n",
    "                for i in tqdm(range(0, len(all_points), batch_size), desc=\"Final restore\"):\n",
    "                    batch = all_points[i:i + batch_size]\n",
    "                    client.upsert(\n",
    "                        collection_name=collection_name,\n",
    "                        points=batch\n",
    "                    )\n",
    "                \n",
    "                # Clean up temp collection\n",
    "                client.delete_collection(temp_collection)\n",
    "            \n",
    "            print(f\"   ✅ Index rebuild completed\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Rebuild failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Perform healing\n",
    "healing_start = time.time()\n",
    "healing_success = perform_index_healing(COLLECTION_NAME, method=\"optimize\")\n",
    "healing_time = time.time() - healing_start\n",
    "\n",
    "print(f\"\\n⏱️ Healing completed in {healing_time:.2f}s\")\n",
    "\n",
    "if healing_success:\n",
    "    # Verify collection after healing\n",
    "    info_after_healing = client.get_collection(COLLECTION_NAME)\n",
    "    print(f\"\\n📊 Collection after healing:\")\n",
    "    print(f\"   Points: {info_after_healing.points_count}\")\n",
    "    print(f\"   Status: {info_after_healing.status}\")\n",
    "else:\n",
    "    print(f\"⚠️ Healing was not successful, proceeding with degraded index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🩺 Post-Healing Performance Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure performance after healing\n",
    "print(\"🩺 Measuring performance after healing...\")\n",
    "post_healing_performance = measure_search_performance(query_vectors, ground_truth)\n",
    "\n",
    "# Compare all three phases\n",
    "print(f\"\\n📊 Complete Performance Analysis:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "phases = {\n",
    "    'Baseline': baseline_performance,\n",
    "    'Post-Churn': post_churn_performance,\n",
    "    'Post-Healing': post_healing_performance\n",
    "}\n",
    "\n",
    "complete_analysis = []\n",
    "\n",
    "for ef_search in [50, 100, 200]:\n",
    "    print(f\"\\n🔍 ef_search = {ef_search}:\")\n",
    "    \n",
    "    phase_data = {'ef_search': ef_search}\n",
    "    \n",
    "    for phase_name, phase_results in phases.items():\n",
    "        if ef_search in phase_results:\n",
    "            recall = phase_results[ef_search]['recall@10']\n",
    "            latency = phase_results[ef_search]['avg_latency_ms']\n",
    "            \n",
    "            phase_data[f'{phase_name.lower()}_recall'] = recall\n",
    "            phase_data[f'{phase_name.lower()}_latency'] = latency\n",
    "            \n",
    "            print(f\"   {phase_name:>12}: Recall {recall:.3f}, Latency {latency:.2f}ms\")\n",
    "    \n",
    "    # Calculate healing effectiveness\n",
    "    if 'baseline_recall' in phase_data and 'post-healing_recall' in phase_data:\n",
    "        healing_recovery = (\n",
    "            (phase_data['post-healing_recall'] - phase_data['post-churn_recall']) /\n",
    "            (phase_data['baseline_recall'] - phase_data['post-churn_recall']) * 100\n",
    "        ) if phase_data['baseline_recall'] != phase_data['post-churn_recall'] else 100\n",
    "        \n",
    "        phase_data['healing_recovery_%'] = healing_recovery\n",
    "        print(f\"   {'Recovery':>12}: {healing_recovery:.1f}% of degradation recovered\")\n",
    "    \n",
    "    complete_analysis.append(phase_data)\n",
    "\n",
    "# Create comprehensive analysis DataFrame\n",
    "if complete_analysis:\n",
    "    analysis_df = pd.DataFrame(complete_analysis)\n",
    "    print(f\"\\n📋 Comprehensive Analysis Table:\")\n",
    "    print(analysis_df.round(3))\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\n🎯 Healing Effectiveness Summary:\")\n",
    "    if 'healing_recovery_%' in analysis_df.columns:\n",
    "        avg_recovery = analysis_df['healing_recovery_%'].mean()\n",
    "        print(f\"   Average recovery: {avg_recovery:.1f}%\")\n",
    "        \n",
    "        if avg_recovery > 80:\n",
    "            print(f\"   ✅ Excellent healing effectiveness\")\n",
    "        elif avg_recovery > 60:\n",
    "            print(f\"   ✅ Good healing effectiveness\")\n",
    "        elif avg_recovery > 40:\n",
    "            print(f\"   ⚠️ Moderate healing effectiveness\")\n",
    "        else:\n",
    "            print(f\"   ❌ Poor healing effectiveness\")\n",
    "    \n",
    "    # Performance comparison\n",
    "    baseline_avg_recall = analysis_df['baseline_recall'].mean()\n",
    "    healed_avg_recall = analysis_df['post-healing_recall'].mean()\n",
    "    final_degradation = (healed_avg_recall - baseline_avg_recall) / baseline_avg_recall * 100\n",
    "    \n",
    "    print(f\"\\n📈 Final Index Health:\")\n",
    "    print(f\"   Baseline avg recall: {baseline_avg_recall:.3f}\")\n",
    "    print(f\"   Post-healing avg recall: {healed_avg_recall:.3f}\")\n",
    "    print(f\"   Final degradation: {final_degradation:+.1f}%\")\n",
    "    \n",
    "    if abs(final_degradation) < 2:\n",
    "        print(f\"   ✅ Index fully restored to baseline performance\")\n",
    "    elif abs(final_degradation) < 5:\n",
    "        print(f\"   ✅ Index mostly restored, minor degradation remains\")\n",
    "    else:\n",
    "        print(f\"   ⚠️ Significant degradation remains after healing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if complete_analysis:\n",
    "    # Create visualization\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('HNSW Index Health Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    df = pd.DataFrame(complete_analysis)\n",
    "    ef_values = df['ef_search'].values\n",
    "    \n",
    "    # Colors for phases\n",
    "    colors = ['#2ecc71', '#e74c3c', '#3498db']  # Green, Red, Blue\n",
    "    phase_names = ['Baseline', 'Post-Churn', 'Post-Healing']\n",
    "    \n",
    "    # 1. Recall comparison\n",
    "    ax1.plot(ef_values, df['baseline_recall'], 'o-', color=colors[0], label='Baseline', linewidth=2, markersize=8)\n",
    "    ax1.plot(ef_values, df['post-churn_recall'], 's-', color=colors[1], label='Post-Churn', linewidth=2, markersize=8)\n",
    "    ax1.plot(ef_values, df['post-healing_recall'], '^-', color=colors[2], label='Post-Healing', linewidth=2, markersize=8)\n",
    "    ax1.set_xlabel('ef_search')\n",
    "    ax1.set_ylabel('Recall@10')\n",
    "    ax1.set_title('Recall Performance Across Index States')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    \n",
    "    # 2. Latency comparison\n",
    "    ax2.plot(ef_values, df['baseline_latency'], 'o-', color=colors[0], label='Baseline', linewidth=2, markersize=8)\n",
    "    ax2.plot(ef_values, df['post-churn_latency'], 's-', color=colors[1], label='Post-Churn', linewidth=2, markersize=8)\n",
    "    ax2.plot(ef_values, df['post-healing_latency'], '^-', color=colors[2], label='Post-Healing', linewidth=2, markersize=8)\n",
    "    ax2.set_xlabel('ef_search')\n",
    "    ax2.set_ylabel('Average Latency (ms)')\n",
    "    ax2.set_title('Latency Performance Across Index States')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Recall degradation and recovery\n",
    "    x_pos = np.arange(len(ef_values))\n",
    "    width = 0.35\n",
    "    \n",
    "    # Calculate degradation percentages\n",
    "    churn_degradation = ((df['post-churn_recall'] - df['baseline_recall']) / df['baseline_recall'] * 100).values\n",
    "    healing_improvement = ((df['post-healing_recall'] - df['post-churn_recall']) / df['baseline_recall'] * 100).values\n",
    "    \n",
    "    ax3.bar(x_pos - width/2, churn_degradation, width, label='Churn Impact', color=colors[1], alpha=0.7)\n",
    "    ax3.bar(x_pos + width/2, healing_improvement, width, label='Healing Recovery', color=colors[2], alpha=0.7)\n",
    "    ax3.set_xlabel('ef_search')\n",
    "    ax3.set_ylabel('Recall Change (%)')\n",
    "    ax3.set_title('Index Degradation and Recovery')\n",
    "    ax3.set_xticks(x_pos)\n",
    "    ax3.set_xticklabels(ef_values)\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # 4. Recall vs Latency trade-off\n",
    "    # Show the trade-off for each phase\n",
    "    ax4.scatter(df['baseline_latency'], df['baseline_recall'], c=colors[0], s=100, alpha=0.7, label='Baseline')\n",
    "    ax4.scatter(df['post-churn_latency'], df['post-churn_recall'], c=colors[1], s=100, alpha=0.7, label='Post-Churn')\n",
    "    ax4.scatter(df['post-healing_latency'], df['post-healing_recall'], c=colors[2], s=100, alpha=0.7, label='Post-Healing')\n",
    "    \n",
    "    # Connect points for same ef_search values\n",
    "    for i in range(len(df)):\n",
    "        ax4.plot([df['baseline_latency'].iloc[i], df['post-churn_latency'].iloc[i]], \n",
    "                [df['baseline_recall'].iloc[i], df['post-churn_recall'].iloc[i]], \n",
    "                'r--', alpha=0.5, linewidth=1)\n",
    "        ax4.plot([df['post-churn_latency'].iloc[i], df['post-healing_latency'].iloc[i]], \n",
    "                [df['post-churn_recall'].iloc[i], df['post-healing_recall'].iloc[i]], \n",
    "                'b--', alpha=0.5, linewidth=1)\n",
    "    \n",
    "    ax4.set_xlabel('Average Latency (ms)')\n",
    "    ax4.set_ylabel('Recall@10')\n",
    "    ax4.set_title('Recall vs Latency Trade-off')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n📊 Chart Interpretation:\")\n",
    "    print(\"   • Top-left: Recall degradation and recovery across ef_search values\")\n",
    "    print(\"   • Top-right: Latency changes during churn and healing\")\n",
    "    print(\"   • Bottom-left: Quantifies degradation (red) vs recovery (blue)\")\n",
    "    print(\"   • Bottom-right: Shows recall-latency trade-offs for each phase\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ No complete analysis data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 HNSW Parameter Tuning Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hnsw_tuning_guide():\n",
    "    \"\"\"Comprehensive guide for HNSW parameter tuning\"\"\"\n",
    "    \n",
    "    print(\"🔧 HNSW Parameter Tuning Guide\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"\\n📊 Key HNSW Parameters:\")\n",
    "    \n",
    "    params = {\n",
    "        \"M (max connections)\": {\n",
    "            \"description\": \"Maximum bi-directional links per node\",\n",
    "            \"range\": \"4-64 (typical: 16-32)\",\n",
    "            \"impact\": \"Higher = better recall, more memory\",\n",
    "            \"tuning\": \"Start with 16, increase if recall insufficient\"\n",
    "        },\n",
    "        \"ef_construct\": {\n",
    "            \"description\": \"Dynamic candidate list size during construction\", \n",
    "            \"range\": \"100-800 (typical: 200-400)\",\n",
    "            \"impact\": \"Higher = better quality, slower indexing\",\n",
    "            \"tuning\": \"2-4x your target ef_search value\"\n",
    "        },\n",
    "        \"ef_search\": {\n",
    "            \"description\": \"Dynamic candidate list size during search\",\n",
    "            \"range\": \"10-500+ (typical: 50-200)\", \n",
    "            \"impact\": \"Higher = better recall, slower search\",\n",
    "            \"tuning\": \"Tune based on recall/latency requirements\"\n",
    "        },\n",
    "        \"full_scan_threshold\": {\n",
    "            \"description\": \"Switch to brute force below this size\",\n",
    "            \"range\": \"1000-50000\",\n",
    "            \"impact\": \"Affects small collection performance\",\n",
    "            \"tuning\": \"Set based on expected collection size\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for param, info in params.items():\n",
    "        print(f\"\\n🎛️ {param}:\")\n",
    "        print(f\"   📖 {info['description']}\")\n",
    "        print(f\"   📏 Range: {info['range']}\")\n",
    "        print(f\"   ⚡ Impact: {info['impact']}\")\n",
    "        print(f\"   🎯 Tuning: {info['tuning']}\")\n",
    "    \n",
    "    print(f\"\\n🎯 Tuning Strategy:\")\n",
    "    print(f\"\\n1️⃣ BASELINE ESTABLISHMENT:\")\n",
    "    print(f\"   • Start with M=16, ef_construct=200\")\n",
    "    print(f\"   • Test ef_search values: 50, 100, 200\")\n",
    "    print(f\"   • Measure baseline recall and latency\")\n",
    "    \n",
    "    print(f\"\\n2️⃣ RECALL OPTIMIZATION:\")\n",
    "    print(f\"   • If recall < target → increase M or ef_search\")\n",
    "    print(f\"   • If building from scratch → increase ef_construct\")\n",
    "    print(f\"   • Consider data distribution and dimensionality\")\n",
    "    \n",
    "    print(f\"\\n3️⃣ LATENCY OPTIMIZATION:\")\n",
    "    print(f\"   • If latency too high → decrease ef_search\")\n",
    "    print(f\"   • If memory usage high → decrease M\")\n",
    "    print(f\"   • Balance recall requirements with performance\")\n",
    "    \n",
    "    print(f\"\\n4️⃣ PRODUCTION CONSIDERATIONS:\")\n",
    "    print(f\"   • Monitor recall degradation over time\")\n",
    "    print(f\"   • Set up automated health checks\")\n",
    "    print(f\"   • Plan healing/rebuild schedules\")\n",
    "    print(f\"   • Consider different parameters per use case\")\n",
    "    \n",
    "    print(f\"\\n📈 Performance Guidelines:\")\n",
    "    \n",
    "    scenarios = {\n",
    "        \"High Precision Search\": {\n",
    "            \"params\": \"M=32, ef_construct=400, ef_search=200\",\n",
    "            \"trade_off\": \"Best recall, higher latency/memory\"\n",
    "        },\n",
    "        \"Balanced Performance\": {\n",
    "            \"params\": \"M=16, ef_construct=200, ef_search=100\",\n",
    "            \"trade_off\": \"Good recall-latency balance\"\n",
    "        },\n",
    "        \"Low Latency Search\": {\n",
    "            \"params\": \"M=8, ef_construct=100, ef_search=50\",\n",
    "            \"trade_off\": \"Fast search, moderate recall\"\n",
    "        },\n",
    "        \"Memory Constrained\": {\n",
    "            \"params\": \"M=8, ef_construct=150, ef_search=75\",\n",
    "            \"trade_off\": \"Lower memory usage, some recall loss\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for scenario, config in scenarios.items():\n",
    "        print(f\"\\n🎪 {scenario}:\")\n",
    "        print(f\"   ⚙️ Params: {config['params']}\")\n",
    "        print(f\"   ⚖️ Trade-off: {config['trade_off']}\")\n",
    "\n",
    "hnsw_tuning_guide()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Production Monitoring Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_monitoring_strategy():\n",
    "    \"\"\"Production monitoring strategy for HNSW index health\"\"\"\n",
    "    \n",
    "    print(\"📊 Production HNSW Monitoring Strategy\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"\\n🎯 Key Health Metrics:\")\n",
    "    \n",
    "    metrics = {\n",
    "        \"Recall Proxy\": {\n",
    "            \"method\": \"Compare ef_search=50 vs ef_search=400 overlap\",\n",
    "            \"frequency\": \"Every hour\",\n",
    "            \"alert_threshold\": \"< 80% overlap\",\n",
    "            \"description\": \"Detects recall degradation without ground truth\"\n",
    "        },\n",
    "        \"Search Latency\": {\n",
    "            \"method\": \"P50, P95, P99 response times\",\n",
    "            \"frequency\": \"Real-time\",\n",
    "            \"alert_threshold\": \"P95 > 2x baseline\",\n",
    "            \"description\": \"Indicates graph connectivity issues\"\n",
    "        },\n",
    "        \"Result Consistency\": {\n",
    "            \"method\": \"Track repeated query result stability\",\n",
    "            \"frequency\": \"Every 15 minutes\",\n",
    "            \"alert_threshold\": \"< 95% consistency\",\n",
    "            \"description\": \"Detects index instability from updates\"\n",
    "        },\n",
    "        \"Churn Rate\": {\n",
    "            \"method\": \"Monitor insert/update/delete rates\",\n",
    "            \"frequency\": \"Every 5 minutes\", \n",
    "            \"alert_threshold\": \"> 10% collection size/hour\",\n",
    "            \"description\": \"High churn predicts future degradation\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for metric, config in metrics.items():\n",
    "        print(f\"\\n📈 {metric}:\")\n",
    "        print(f\"   🔍 Method: {config['method']}\")\n",
    "        print(f\"   ⏰ Frequency: {config['frequency']}\")\n",
    "        print(f\"   🚨 Alert: {config['alert_threshold']}\")\n",
    "        print(f\"   💭 Purpose: {config['description']}\")\n",
    "    \n",
    "    print(f\"\\n🚨 Alert Levels:\")\n",
    "    \n",
    "    alert_levels = {\n",
    "        \"🟢 Healthy\": {\n",
    "            \"conditions\": \"All metrics within normal ranges\",\n",
    "            \"action\": \"Continue normal monitoring\"\n",
    "        },\n",
    "        \"🟡 Warning\": {\n",
    "            \"conditions\": \"1-2 metrics showing degradation\", \n",
    "            \"action\": \"Increase monitoring frequency, plan healing\"\n",
    "        },\n",
    "        \"🟠 Degraded\": {\n",
    "            \"conditions\": \"Multiple metrics affected, user impact likely\",\n",
    "            \"action\": \"Schedule immediate index healing\"\n",
    "        },\n",
    "        \"🔴 Critical\": {\n",
    "            \"conditions\": \"Severe degradation, significant user impact\",\n",
    "            \"action\": \"Emergency index rebuild, investigate cause\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for level, config in alert_levels.items():\n",
    "        print(f\"\\n{level}:\")\n",
    "        print(f\"   📊 Conditions: {config['conditions']}\")\n",
    "        print(f\"   🔧 Action: {config['action']}\")\n",
    "    \n",
    "    print(f\"\\n🛠️ Automated Responses:\")\n",
    "    \n",
    "    responses = {\n",
    "        \"Proactive Healing\": \"Trigger optimization when churn > threshold\",\n",
    "        \"Dynamic ef_search\": \"Increase ef_search temporarily during degradation\",\n",
    "        \"Load Balancing\": \"Route traffic to healthier replicas\",\n",
    "        \"Graceful Degradation\": \"Fall back to exact search for critical queries\"\n",
    "    }\n",
    "    \n",
    "    for response, description in responses.items():\n",
    "        print(f\"   • {response}: {description}\")\n",
    "    \n",
    "    print(f\"\\n📅 Maintenance Schedule:\")\n",
    "    \n",
    "    schedule = {\n",
    "        \"Daily\": \"Review health metrics, check alert logs\",\n",
    "        \"Weekly\": \"Analyze performance trends, tune parameters\",\n",
    "        \"Monthly\": \"Full index health assessment, capacity planning\",\n",
    "        \"Quarterly\": \"Benchmark against new HNSW versions/settings\"\n",
    "    }\n",
    "    \n",
    "    for frequency, task in schedule.items():\n",
    "        print(f\"   🗓️ {frequency}: {task}\")\n",
    "\n",
    "create_monitoring_strategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 Final Summary & Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final collection status\n",
    "final_info = client.get_collection(COLLECTION_NAME)\n",
    "\n",
    "print(\"🎉 HNSW Index Health Workshop Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\n📚 Collection: {COLLECTION_NAME}\")\n",
    "print(f\"   📊 Final point count: {final_info.points_count}\")\n",
    "print(f\"   ⚙️ HNSW configuration: M={HNSW_CONFIG['m']}, ef_construct={HNSW_CONFIG['ef_construct']}\")\n",
    "print(f\"   🌪️ Churn simulation: {len(deleted_indices)} deleted, {len(modified_points)} modified\")\n",
    "print(f\"   ⏱️ Healing time: {healing_time:.2f}s\")\n",
    "\n",
    "print(f\"\\n🔍 Analysis Performed:\")\n",
    "print(\"   ✅ Baseline performance measurement\")\n",
    "print(\"   ✅ Index churn simulation (deletions + updates)\")\n",
    "print(\"   ✅ Degradation quantification\")\n",
    "print(\"   ✅ Healing/optimization process\")\n",
    "print(\"   ✅ Recovery effectiveness analysis\")\n",
    "\n",
    "if 'complete_analysis' in locals() and complete_analysis:\n",
    "    df = pd.DataFrame(complete_analysis)\n",
    "    avg_baseline_recall = df['baseline_recall'].mean()\n",
    "    avg_degraded_recall = df['post-churn_recall'].mean()\n",
    "    avg_healed_recall = df['post-healing_recall'].mean()\n",
    "    \n",
    "    print(f\"\\n📊 Performance Summary:\")\n",
    "    print(f\"   Baseline recall: {avg_baseline_recall:.3f}\")\n",
    "    print(f\"   Post-churn recall: {avg_degraded_recall:.3f} ({(avg_degraded_recall-avg_baseline_recall)/avg_baseline_recall*100:+.1f}%)\")\n",
    "    print(f\"   Post-healing recall: {avg_healed_recall:.3f} ({(avg_healed_recall-avg_baseline_recall)/avg_baseline_recall*100:+.1f}%)\")\n",
    "    \n",
    "    if 'healing_recovery_%' in df.columns:\n",
    "        avg_recovery = df['healing_recovery_%'].mean()\n",
    "        print(f\"   Healing effectiveness: {avg_recovery:.1f}% recovery\")\n",
    "\n",
    "print(f\"\\n🎯 Key Learnings:\")\n",
    "print(\"   🔹 HNSW indexes degrade with heavy churn (updates/deletes)\")\n",
    "print(\"   🔹 Degradation manifests as reduced recall and increased latency\")\n",
    "print(\"   🔹 Regular optimization/healing can restore performance\")\n",
    "print(\"   🔹 Proactive monitoring prevents severe degradation\")\n",
    "print(\"   🔹 Parameter tuning balances recall, latency, and memory\")\n",
    "\n",
    "print(f\"\\n🛠️ Production Best Practices:\")\n",
    "print(\"   📊 Monitor recall proxy metrics (ef_search overlap)\")\n",
    "print(\"   ⏰ Track latency percentiles (P95, P99)\")\n",
    "print(\"   🔄 Schedule regular index optimization\")\n",
    "print(\"   🚨 Set up automated degradation alerts\")\n",
    "print(\"   📈 Tune HNSW parameters for your workload\")\n",
    "print(\"   💾 Plan for index rebuilds during major degradation\")\n",
    "\n",
    "print(f\"\\n🚀 Next Steps:\")\n",
    "print(\"   • Implement monitoring dashboards\")\n",
    "print(\"   • Set up automated healing triggers\")\n",
    "print(\"   • Benchmark different HNSW parameters\")\n",
    "print(\"   • Test healing strategies in staging\")\n",
    "print(\"   • Document runbooks for production issues\")\n",
    "\n",
    "print(f\"\\n✨ Workshop complete! You now understand HNSW index health management.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧹 Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to clean up the large collection\n",
    "# PRESERVE_COLLECTIONS = True  # Set to False to delete\n",
    "\n",
    "# if not PRESERVE_COLLECTIONS:\n",
    "#     try:\n",
    "#         client.delete_collection(COLLECTION_NAME)\n",
    "#         print(f\"🗑️ Deleted collection: {COLLECTION_NAME}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Note: Could not delete collection: {e}\")\n",
    "# else:\n",
    "#     print(f\"💾 Collection preserved: {COLLECTION_NAME}\")\n",
    "\n",
    "print(f\"\\n🎉 HNSW Index Health workshop complete!\")\n",
    "print(f\"\\n🚀 Ready for Notebook 5: Agentic RAG!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
