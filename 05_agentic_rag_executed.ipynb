{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6ded1cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T02:49:03.100565Z",
     "iopub.status.busy": "2025-08-19T02:49:03.100181Z",
     "iopub.status.idle": "2025-08-19T02:49:03.110018Z",
     "shell.execute_reply": "2025-08-19T02:49:03.108871Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a196a09",
   "metadata": {},
   "source": [
    "# Agentic RAG: Agentic RAG with Qdrant\n",
    "\n",
    "## ðŸŽ¯ Objectives\n",
    "\n",
    "In this notebook, you'll learn:\n",
    "- Building intelligent RAG agents that can reason about queries\n",
    "- Multi-step query planning and execution\n",
    "- Dynamic retrieval strategies based on query analysis\n",
    "- Tool use and function calling within RAG workflows\n",
    "- Self-correcting retrieval with quality assessment\n",
    "- Building conversational agents with memory\n",
    "- Advanced techniques: query rewriting, result fusion, and answer synthesis\n",
    "\n",
    "## ðŸ“‹ Prerequisites\n",
    "\n",
    "- Understanding of RAG (Retrieval-Augmented Generation) concepts\n",
    "- Familiarity with LLM APIs (OpenAI, Anthropic, or local models)\n",
    "- Collections from previous notebooks for knowledge base\n",
    "- Basic understanding of agent frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b3e1c49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T02:49:03.114517Z",
     "iopub.status.busy": "2025-08-19T02:49:03.114183Z",
     "iopub.status.idle": "2025-08-19T02:49:04.910307Z",
     "shell.execute_reply": "2025-08-19T02:49:04.910047Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thierrydamiba/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ System Information:\n",
      "   Python: 3.9.6\n",
      "   âœ… Qdrant Client: unknown\n",
      "   âœ… NumPy: 1.26.4\n",
      "   âœ… Pandas: 2.2.2\n",
      "   âœ… Matplotlib: 3.9.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Scikit-learn: 1.6.1\n",
      "\n",
      "ðŸ”§ Optional Dependencies:\n",
      "   âœ… FastEmbed: 0.5.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… OpenAI: 1.100.1\n",
      "   âœ… Anthropic: 0.57.1\n",
      "\n",
      "ðŸ”¬ Environment: JupyterLab/Notebook detected\n",
      "\n",
      "ðŸ¤– Agentic RAG Workshop\n",
      "\n",
      "ðŸ”‘ Checking LLM API availability...\n",
      "âœ… OpenAI API key found\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "from utils import (\n",
    "    get_qdrant_client, create_sample_dataset, search_dense, \n",
    "    search_hybrid_fusion, mmr_rerank, print_system_info\n",
    ")\n",
    "\n",
    "from qdrant_client.models import Filter, FieldCondition, MatchValue, Range\n",
    "\n",
    "print_system_info()\n",
    "print(\"\\nðŸ¤– Agentic RAG Workshop\")\n",
    "\n",
    "# Check for LLM API availability\n",
    "print(\"\\nðŸ”‘ Checking LLM API availability...\")\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "if openai_key:\n",
    "    print(\"âœ… OpenAI API key found\")\n",
    "    LLM_PROVIDER = \"openai\"\n",
    "elif anthropic_key:\n",
    "    print(\"âœ… Anthropic API key found\")\n",
    "    LLM_PROVIDER = \"anthropic\"\n",
    "else:\n",
    "    print(\"âš ï¸ No LLM API keys found. Using mock responses for demonstration.\")\n",
    "    LLM_PROVIDER = \"mock\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc0db784",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T02:49:04.912025Z",
     "iopub.status.busy": "2025-08-19T02:49:04.911848Z",
     "iopub.status.idle": "2025-08-19T02:49:04.918239Z",
     "shell.execute_reply": "2025-08-19T02:49:04.917993Z"
    }
   },
   "outputs": [],
   "source": [
    "# Real LLM client (OpenAI)\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"OpenAI SDK not installed. Please `pip install openai`. Error: {e}\")\n",
    "\n",
    "class LLMClient:\n",
    "    \"\"\"Unified interface for LLM providers (OpenAI).\"\"\"\n",
    "    def __init__(self, provider: str = \"openai\", model: str = \"gpt-4o-mini\"):\n",
    "        self.provider = provider\n",
    "        self.model = model\n",
    "        if provider == \"openai\":\n",
    "            self.client = OpenAI()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported provider: {provider}\")\n",
    "\n",
    "    def chat_completion(self, messages: List[Dict[str, str]], temperature: float = 0.1) -> str:\n",
    "        if self.provider == \"openai\":\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "                temperature=temperature,\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        raise ValueError(f\"Unsupported provider: {self.provider}\")\n",
    "\n",
    "# Initialize LLM (requires OPENAI_API_KEY in env)\n",
    "llm = LLMClient(provider=\"openai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0dc232",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Install Dependencies (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dce8032",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T02:49:04.919495Z",
     "iopub.status.busy": "2025-08-19T02:49:04.919408Z",
     "iopub.status.idle": "2025-08-19T02:49:04.920786Z",
     "shell.execute_reply": "2025-08-19T02:49:04.920563Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment if running in a fresh environment\n",
    "# !pip install openai anthropic tiktoken fastembed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0758999f",
   "metadata": {},
   "source": [
    "## ðŸ§  Agentic RAG Architecture\n",
    "\n",
    "Traditional RAG: Query â†’ Retrieve â†’ Generate  \n",
    "Agentic RAG: Query â†’ Plan â†’ Multi-step Retrieve â†’ Reason â†’ Generate â†’ Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3f3a924",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T02:49:04.921855Z",
     "iopub.status.busy": "2025-08-19T02:49:04.921780Z",
     "iopub.status.idle": "2025-08-19T02:49:04.926170Z",
     "shell.execute_reply": "2025-08-19T02:49:04.925963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Agentic RAG Architecture\n",
      "========================================\n",
      "\n",
      "ðŸ”„ Traditional RAG Limitations:\n",
      "   â€¢ Single retrieval step\n",
      "   â€¢ No query understanding\n",
      "   â€¢ Fixed retrieval strategy\n",
      "   â€¢ No result validation\n",
      "   â€¢ Limited multi-turn capability\n",
      "\n",
      "ðŸ§  Agentic RAG Enhancements:\n",
      "   ðŸŽ¯ Query Analysis:\n",
      "      - Classify query type and complexity\n",
      "      - Identify required information types\n",
      "      - Plan multi-step retrieval strategy\n",
      "   ðŸ” Intelligent Retrieval:\n",
      "      - Adaptive search strategies\n",
      "      - Dynamic filtering based on context\n",
      "      - Multi-hop information gathering\n",
      "   ðŸ”§ Tool Integration:\n",
      "      - Query rewriting and expansion\n",
      "      - Result synthesis and validation\n",
      "      - External API integration\n",
      "   ðŸ’­ Reasoning:\n",
      "      - Quality assessment of retrieved info\n",
      "      - Gap identification and follow-up queries\n",
      "      - Answer confidence scoring\n",
      "\n",
      "ðŸ”„ Agentic RAG Workflow:\n",
      "\n",
      "    1. Query Analysis\n",
      "       â”œâ”€â”€ Intent classification\n",
      "       â”œâ”€â”€ Complexity assessment\n",
      "       â””â”€â”€ Strategy selection\n",
      "    \n",
      "    2. Retrieval Planning\n",
      "       â”œâ”€â”€ Multi-step breakdown\n",
      "       â”œâ”€â”€ Search strategy per step\n",
      "       â””â”€â”€ Quality thresholds\n",
      "    \n",
      "    3. Execution\n",
      "       â”œâ”€â”€ Retrieve â†’ Assess â†’ Refine\n",
      "       â”œâ”€â”€ Multi-hop follow-ups\n",
      "       â””â”€â”€ Result validation\n",
      "    \n",
      "    4. Synthesis\n",
      "       â”œâ”€â”€ Information integration\n",
      "       â”œâ”€â”€ Answer generation\n",
      "       â””â”€â”€ Confidence scoring\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "class QueryType(Enum):\n",
    "    \"\"\"Types of queries that require different retrieval strategies\"\"\"\n",
    "    SIMPLE_FACTUAL = \"simple_factual\"\n",
    "    COMPLEX_ANALYTICAL = \"complex_analytical\"\n",
    "    COMPARATIVE = \"comparative\"\n",
    "    MULTI_HOP = \"multi_hop\"\n",
    "    PROCEDURAL = \"procedural\"\n",
    "    CONVERSATIONAL = \"conversational\"\n",
    "\n",
    "@dataclass\n",
    "class RetrievalPlan:\n",
    "    \"\"\"Plan for multi-step retrieval\"\"\"\n",
    "    query_type: QueryType\n",
    "    steps: List[Dict[str, Any]]\n",
    "    search_strategy: str  # 'dense', 'sparse', 'hybrid', 'multi_vector'\n",
    "    filters: Optional[Dict] = None\n",
    "    rerank: bool = True\n",
    "    max_results: int = 10\n",
    "    confidence_threshold: float = 0.7\n",
    "\n",
    "@dataclass\n",
    "class AgentState:\n",
    "    \"\"\"Maintains agent conversation state\"\"\"\n",
    "    conversation_history: List[Dict[str, str]]\n",
    "    retrieved_context: List[Dict]\n",
    "    current_query: str\n",
    "    query_plan: Optional[RetrievalPlan] = None\n",
    "    metadata: Dict = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.metadata is None:\n",
    "            self.metadata = {}\n",
    "\n",
    "def explain_agentic_rag():\n",
    "    \"\"\"Explain the concepts behind agentic RAG\"\"\"\n",
    "    print(\"ðŸ¤– Agentic RAG Architecture\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    print(\"\\nðŸ”„ Traditional RAG Limitations:\")\n",
    "    print(\"   â€¢ Single retrieval step\")\n",
    "    print(\"   â€¢ No query understanding\")\n",
    "    print(\"   â€¢ Fixed retrieval strategy\")\n",
    "    print(\"   â€¢ No result validation\")\n",
    "    print(\"   â€¢ Limited multi-turn capability\")\n",
    "    \n",
    "    print(\"\\nðŸ§  Agentic RAG Enhancements:\")\n",
    "    print(\"   ðŸŽ¯ Query Analysis:\")\n",
    "    print(\"      - Classify query type and complexity\")\n",
    "    print(\"      - Identify required information types\")\n",
    "    print(\"      - Plan multi-step retrieval strategy\")\n",
    "    \n",
    "    print(\"   ðŸ” Intelligent Retrieval:\")\n",
    "    print(\"      - Adaptive search strategies\")\n",
    "    print(\"      - Dynamic filtering based on context\")\n",
    "    print(\"      - Multi-hop information gathering\")\n",
    "    \n",
    "    print(\"   ðŸ”§ Tool Integration:\")\n",
    "    print(\"      - Query rewriting and expansion\")\n",
    "    print(\"      - Result synthesis and validation\")\n",
    "    print(\"      - External API integration\")\n",
    "    \n",
    "    print(\"   ðŸ’­ Reasoning:\")\n",
    "    print(\"      - Quality assessment of retrieved info\")\n",
    "    print(\"      - Gap identification and follow-up queries\")\n",
    "    print(\"      - Answer confidence scoring\")\n",
    "    \n",
    "    print(\"\\nðŸ”„ Agentic RAG Workflow:\")\n",
    "    workflow = '''\n",
    "    1. Query Analysis\n",
    "       â”œâ”€â”€ Intent classification\n",
    "       â”œâ”€â”€ Complexity assessment\n",
    "       â””â”€â”€ Strategy selection\n",
    "    \n",
    "    2. Retrieval Planning\n",
    "       â”œâ”€â”€ Multi-step breakdown\n",
    "       â”œâ”€â”€ Search strategy per step\n",
    "       â””â”€â”€ Quality thresholds\n",
    "    \n",
    "    3. Execution\n",
    "       â”œâ”€â”€ Retrieve â†’ Assess â†’ Refine\n",
    "       â”œâ”€â”€ Multi-hop follow-ups\n",
    "       â””â”€â”€ Result validation\n",
    "    \n",
    "    4. Synthesis\n",
    "       â”œâ”€â”€ Information integration\n",
    "       â”œâ”€â”€ Answer generation\n",
    "       â””â”€â”€ Confidence scoring\n",
    "    '''\n",
    "    print(workflow)\n",
    "\n",
    "explain_agentic_rag()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ee9233",
   "metadata": {},
   "source": [
    "## âš™ï¸ Setup & LLM Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286a45f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T02:49:04.927302Z",
     "iopub.status.busy": "2025-08-19T02:49:04.927229Z",
     "iopub.status.idle": "2025-08-19T02:49:05.610218Z",
     "shell.execute_reply": "2025-08-19T02:49:05.609604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ Using Qdrant Cloud cluster: https://a025094c-936b-4e1b-b947-67d686d20306.eu-central-1-0.aws.development-cloud.qdrant.io:6333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using existing knowledge collection: agentic_rag_demo\n"
     ]
    }
   ],
   "source": [
    "# Knowledge Base Setup (independent)\n",
    "from utils import (\n",
    "    get_qdrant_client,\n",
    "    ensure_collection,\n",
    "    create_sample_dataset,\n",
    "    upsert_points_batch,\n",
    ")\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "import numpy as np\n",
    "\n",
    "KNOWLEDGE_COLLECTION = \"agentic_rag_demo\"\n",
    "\n",
    "# Connect to Qdrant\n",
    "client = get_qdrant_client()\n",
    "\n",
    "# Ensure knowledge collection exists and has data\n",
    "try:\n",
    "    info = client.get_collection(KNOWLEDGE_COLLECTION)\n",
    "except Exception:\n",
    "    info = None\n",
    "\n",
    "needs_population = False\n",
    "if info is None:\n",
    "    needs_population = True\n",
    "else:\n",
    "    try:\n",
    "        needs_population = int(getattr(info, 'points_count', 0) or 0) == 0\n",
    "    except Exception:\n",
    "        needs_population = False\n",
    "\n",
    "if needs_population:\n",
    "    print(f\"âœ… Creating and populating knowledge collection: {KNOWLEDGE_COLLECTION}\")\n",
    "    # Configure named vectors for hybrid search compatibility\n",
    "    ensure_collection(\n",
    "        client,\n",
    "        KNOWLEDGE_COLLECTION,\n",
    "        {\n",
    "            \"text_dense\": VectorParams(size=384, distance=Distance.COSINE),\n",
    "            # size=0 sentinel for sparse vector handled in utils.ensure_collection\n",
    "            \"text_sparse\": VectorParams(size=0, distance=Distance.COSINE),\n",
    "        },\n",
    "        force_recreate=True,\n",
    "    )\n",
    "    df = create_sample_dataset(size=300, seed=123)\n",
    "    vectors_dense = np.random.randn(len(df), 384)\n",
    "    vectors_dense = vectors_dense / np.linalg.norm(vectors_dense, axis=1, keepdims=True)\n",
    "    upsert_points_batch(\n",
    "        client,\n",
    "        KNOWLEDGE_COLLECTION,\n",
    "        df,\n",
    "        {\"text_dense\": vectors_dense},\n",
    "        [\"text\", \"category\", \"lang\", \"timestamp\"],\n",
    "        batch_size=100,\n",
    "    )\n",
    "    print(f\"âœ… Added {len(df)} documents to {KNOWLEDGE_COLLECTION}\")\n",
    "else:\n",
    "    print(f\"âœ… Using existing knowledge collection: {KNOWLEDGE_COLLECTION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c28f61",
   "metadata": {},
   "source": [
    "## ðŸ” Query Analysis Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "237847ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T02:49:05.613272Z",
     "iopub.status.busy": "2025-08-19T02:49:05.612954Z",
     "iopub.status.idle": "2025-08-19T02:49:15.100564Z",
     "shell.execute_reply": "2025-08-19T02:49:15.099857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Query Analysis Examples:\n",
      "==================================================\n",
      "\n",
      "ðŸ“ Query: 'What is vector search?'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Type: simple_factual\n",
      "   Complexity: low\n",
      "   Strategy: sparse\n",
      "   Multi-step: False\n",
      "   ðŸ’­ The query is straightforward, asking for a definition or explanation of a specif...\n",
      "\n",
      "ðŸ“ Query: 'Compare HNSW vs IVF algorithms for large-scale vector retrieval'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Type: comparative\n",
      "   Complexity: medium\n",
      "   Strategy: hybrid\n",
      "   Multi-step: True\n",
      "   ðŸ’­ The query seeks a comparison between two specific algorithms, indicating a need ...\n",
      "\n",
      "ðŸ“ Query: 'How do I set up a production-ready Qdrant cluster with monitoring?'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Type: procedural\n",
      "   Complexity: medium\n",
      "   Strategy: hybrid\n",
      "   Multi-step: True\n",
      "   ðŸ’­ The query is procedural as it seeks a specific process to achieve a goal (settin...\n",
      "\n",
      "ðŸ“ Query: 'Why is my search performance degrading over time?'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Type: complex_analytical\n",
      "   Complexity: medium\n",
      "   Strategy: hybrid\n",
      "   Multi-step: True\n",
      "   ðŸ’­ The query seeks to understand the underlying causes of a decline in search perfo...\n"
     ]
    }
   ],
   "source": [
    "class QueryAnalyzer:\n",
    "    \"\"\"Analyzes queries to determine appropriate retrieval strategy\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client: LLMClient):\n",
    "        self.llm = llm_client\n",
    "    \n",
    "    def analyze_query(self, query: str, conversation_history: List[Dict] = None) -> Dict:\n",
    "        \"\"\"Analyze query to determine type, complexity, and strategy\"\"\"\n",
    "        \n",
    "        analysis_prompt = self._build_analysis_prompt(query, conversation_history)\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a query analysis expert. Analyze queries and return structured JSON responses.\"},\n",
    "            {\"role\": \"user\", \"content\": analysis_prompt}\n",
    "        ]\n",
    "        \n",
    "        response = self.llm.chat_completion(messages)\n",
    "        \n",
    "        try:\n",
    "            analysis = json.loads(response)\n",
    "        except json.JSONDecodeError:\n",
    "            # Fallback analysis\n",
    "            analysis = self._fallback_analysis(query)\n",
    "        \n",
    "        # Enrich with rule-based analysis\n",
    "        analysis.update(self._rule_based_analysis(query))\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _build_analysis_prompt(self, query: str, history: List[Dict] = None) -> str:\n",
    "        \"\"\"Build prompt for query analysis\"\"\"\n",
    "        \n",
    "        context = \"\"\n",
    "        if history and len(history) > 0:\n",
    "            recent_context = history[-3:] if len(history) > 3 else history\n",
    "            context = \"\\n\\nConversation context:\\n\"\n",
    "            for turn in recent_context:\n",
    "                context += f\"User: {turn.get('user', '')}\\nAssistant: {turn.get('assistant', '')}\\n\"\n",
    "        \n",
    "        return f'''\n",
    "Analyze this query and return a JSON response with the following structure:\n",
    "{{\n",
    "    \"query_type\": \"simple_factual|complex_analytical|comparative|multi_hop|procedural|conversational\",\n",
    "    \"complexity\": \"low|medium|high\", \n",
    "    \"requires_multi_step\": true/false,\n",
    "    \"key_entities\": [\"entity1\", \"entity2\"],\n",
    "    \"intent\": \"what the user wants to achieve\",\n",
    "    \"search_strategy\": \"dense|sparse|hybrid\",\n",
    "    \"expected_answer_type\": \"factual|explanatory|comparative|step-by-step\",\n",
    "    \"reasoning\": \"brief explanation of the analysis\"\n",
    "}}\n",
    "\n",
    "Query: \"{query}\"{context}\n",
    "        '''.strip()\n",
    "    \n",
    "    def _rule_based_analysis(self, query: str) -> Dict:\n",
    "        \"\"\"Rule-based query analysis as fallback/enhancement\"\"\"\n",
    "        analysis = {}\n",
    "        \n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        # Detect comparative queries\n",
    "        comparative_words = ['vs', 'versus', 'compare', 'difference between', 'better than']\n",
    "        if any(word in query_lower for word in comparative_words):\n",
    "            analysis['is_comparative'] = True\n",
    "            analysis['suggested_strategy'] = 'hybrid'  # Good for finding diverse perspectives\n",
    "        \n",
    "        # Detect procedural queries\n",
    "        procedural_words = ['how to', 'step by step', 'guide', 'tutorial', 'instructions']\n",
    "        if any(word in query_lower for word in procedural_words):\n",
    "            analysis['is_procedural'] = True\n",
    "            analysis['needs_ordering'] = True\n",
    "        \n",
    "        # Detect technical complexity\n",
    "        technical_terms = ['algorithm', 'optimization', 'configuration', 'implementation']\n",
    "        if any(term in query_lower for term in technical_terms):\n",
    "            analysis['is_technical'] = True\n",
    "            analysis['preferred_sources'] = 'technical'\n",
    "        \n",
    "        # Estimate required information breadth\n",
    "        question_words = ['what', 'why', 'how', 'when', 'where', 'who']\n",
    "        question_count = sum(1 for word in question_words if word in query_lower)\n",
    "        \n",
    "        if question_count > 1 or len(query.split()) > 10:\n",
    "            analysis['is_complex'] = True\n",
    "            analysis['needs_multiple_sources'] = True\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _fallback_analysis(self, query: str) -> Dict:\n",
    "        \"\"\"Simple fallback analysis when LLM fails\"\"\"\n",
    "        return {\n",
    "            \"query_type\": \"simple_factual\",\n",
    "            \"complexity\": \"medium\",\n",
    "            \"requires_multi_step\": False,\n",
    "            \"search_strategy\": \"hybrid\",\n",
    "            \"reasoning\": \"Fallback analysis used due to LLM unavailability\"\n",
    "        }\n",
    "\n",
    "# Test query analyzer\n",
    "analyzer = QueryAnalyzer(llm)\n",
    "\n",
    "test_queries = [\n",
    "    \"What is vector search?\",\n",
    "    \"Compare HNSW vs IVF algorithms for large-scale vector retrieval\",\n",
    "    \"How do I set up a production-ready Qdrant cluster with monitoring?\",\n",
    "    \"Why is my search performance degrading over time?\"\n",
    "]\n",
    "\n",
    "print(\"ðŸ” Query Analysis Examples:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nðŸ“ Query: '{query}'\")\n",
    "    analysis = analyzer.analyze_query(query)\n",
    "    \n",
    "    print(f\"   Type: {analysis.get('query_type', 'unknown')}\")\n",
    "    print(f\"   Complexity: {analysis.get('complexity', 'unknown')}\")\n",
    "    print(f\"   Strategy: {analysis.get('search_strategy', 'unknown')}\")\n",
    "    print(f\"   Multi-step: {analysis.get('requires_multi_step', False)}\")\n",
    "    \n",
    "    if 'reasoning' in analysis:\n",
    "        print(f\"   ðŸ’­ {analysis['reasoning'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615baf95",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Retrieval Planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04df872f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T02:49:15.105286Z",
     "iopub.status.busy": "2025-08-19T02:49:15.104840Z",
     "iopub.status.idle": "2025-08-19T02:49:23.653883Z",
     "shell.execute_reply": "2025-08-19T02:49:23.653005Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‹ Retrieval Planning Examples:\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ Query: 'What is vector search?'\n",
      "   ðŸŽ¯ Query Type: simple_factual\n",
      "   ðŸ” Strategy: hybrid\n",
      "   ðŸ“Š Max Results: 5\n",
      "   ðŸ”„ Rerank: False\n",
      "   ðŸ“ˆ Steps: 1\n",
      "     1. search: Direct search for factual information\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ Query: 'Compare HNSW vs IVF algorithms for large-scale vector retrieval'\n",
      "   ðŸŽ¯ Query Type: comparative\n",
      "   ðŸ” Strategy: hybrid\n",
      "   ðŸ“Š Max Results: 12\n",
      "   ðŸ”„ Rerank: True\n",
      "   ðŸ“ˆ Steps: 4\n",
      "     1. search: Broad search for comparative information\n",
      "     2. rerank: Diversify results to cover both sides\n",
      "     3. search: Focused search for HNSW\n",
      "     4. search: Focused search for IVF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ Query: 'How do I set up a production-ready Qdrant cluster with monitoring?'\n",
      "   ðŸŽ¯ Query Type: procedural\n",
      "   ðŸ” Strategy: sparse\n",
      "   ðŸ“Š Max Results: 8\n",
      "   ðŸ”„ Rerank: False\n",
      "   ðŸ“ˆ Steps: 2\n",
      "     1. search: Search for procedural content\n",
      "     2. order_by_steps: Order results by logical step sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ Query: 'Why is my search performance degrading over time?'\n",
      "   ðŸŽ¯ Query Type: complex_analytical\n",
      "   ðŸ” Strategy: hybrid\n",
      "   ðŸ“Š Max Results: 15\n",
      "   ðŸ”„ Rerank: True\n",
      "   ðŸ“ˆ Steps: 4\n",
      "     1. search: Initial broad search\n",
      "     2. analyze_gaps: Identify information gaps\n",
      "     3. rerank: Balance relevance and diversity\n",
      "     4. validate: Assess result quality and completeness\n"
     ]
    }
   ],
   "source": [
    "class RetrievalPlanner:\n",
    "    \"\"\"Plans multi-step retrieval strategies based on query analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client: LLMClient):\n",
    "        self.llm = llm_client\n",
    "    \n",
    "    def create_plan(self, query: str, analysis: Dict) -> RetrievalPlan:\n",
    "        \"\"\"Create a detailed retrieval plan based on query analysis\"\"\"\n",
    "        \n",
    "        # Determine query type from analysis\n",
    "        query_type_str = analysis.get('query_type', 'simple_factual')\n",
    "        try:\n",
    "            query_type = QueryType(query_type_str)\n",
    "        except ValueError:\n",
    "            query_type = QueryType.SIMPLE_FACTUAL\n",
    "        \n",
    "        # Create plan based on query type\n",
    "        if query_type == QueryType.SIMPLE_FACTUAL:\n",
    "            return self._plan_simple_factual(query, analysis)\n",
    "        elif query_type == QueryType.COMPARATIVE:\n",
    "            return self._plan_comparative(query, analysis)\n",
    "        elif query_type == QueryType.COMPLEX_ANALYTICAL:\n",
    "            return self._plan_complex_analytical(query, analysis)\n",
    "        elif query_type == QueryType.MULTI_HOP:\n",
    "            return self._plan_multi_hop(query, analysis)\n",
    "        elif query_type == QueryType.PROCEDURAL:\n",
    "            return self._plan_procedural(query, analysis)\n",
    "        else:\n",
    "            return self._plan_default(query, analysis)\n",
    "    \n",
    "    def _plan_simple_factual(self, query: str, analysis: Dict) -> RetrievalPlan:\n",
    "        \"\"\"Plan for simple factual queries\"\"\"\n",
    "        steps = [\n",
    "            {\n",
    "                \"action\": \"search\",\n",
    "                \"strategy\": \"hybrid\",\n",
    "                \"query\": query,\n",
    "                \"limit\": 5,\n",
    "                \"description\": \"Direct search for factual information\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        return RetrievalPlan(\n",
    "            query_type=QueryType.SIMPLE_FACTUAL,\n",
    "            steps=steps,\n",
    "            search_strategy=\"hybrid\",\n",
    "            rerank=False,  # Simple queries don't need reranking\n",
    "            max_results=5\n",
    "        )\n",
    "    \n",
    "    def _plan_comparative(self, query: str, analysis: Dict) -> RetrievalPlan:\n",
    "        \"\"\"Plan for comparative queries\"\"\"\n",
    "        \n",
    "        # Extract entities to compare (simplified)\n",
    "        entities = analysis.get('key_entities', [])\n",
    "        if not entities:\n",
    "            # Simple extraction from query\n",
    "            parts = query.lower().split(' vs ')\n",
    "            if len(parts) == 2:\n",
    "                entities = [part.strip() for part in parts]\n",
    "        \n",
    "        steps = [\n",
    "            {\n",
    "                \"action\": \"search\",\n",
    "                \"strategy\": \"hybrid\",\n",
    "                \"query\": query,\n",
    "                \"limit\": 15,\n",
    "                \"description\": \"Broad search for comparative information\"\n",
    "            },\n",
    "            {\n",
    "                \"action\": \"rerank\",\n",
    "                \"method\": \"mmr\",\n",
    "                \"lambda\": 0.3,  # Favor diversity for comparison\n",
    "                \"description\": \"Diversify results to cover both sides\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Add specific searches for each entity if identified\n",
    "        for entity in entities[:2]:  # Limit to 2 main entities\n",
    "            steps.append({\n",
    "                \"action\": \"search\",\n",
    "                \"strategy\": \"dense\",  # Focused search\n",
    "                \"query\": entity,\n",
    "                \"limit\": 3,\n",
    "                \"description\": f\"Focused search for {entity}\"\n",
    "            })\n",
    "        \n",
    "        return RetrievalPlan(\n",
    "            query_type=QueryType.COMPARATIVE,\n",
    "            steps=steps,\n",
    "            search_strategy=\"hybrid\",\n",
    "            rerank=True,\n",
    "            max_results=12\n",
    "        )\n",
    "    \n",
    "    def _plan_complex_analytical(self, query: str, analysis: Dict) -> RetrievalPlan:\n",
    "        \"\"\"Plan for complex analytical queries\"\"\"\n",
    "        steps = [\n",
    "            {\n",
    "                \"action\": \"search\",\n",
    "                \"strategy\": \"hybrid\",\n",
    "                \"query\": query,\n",
    "                \"limit\": 20,\n",
    "                \"description\": \"Initial broad search\"\n",
    "            },\n",
    "            {\n",
    "                \"action\": \"analyze_gaps\",\n",
    "                \"description\": \"Identify information gaps\"\n",
    "            },\n",
    "            {\n",
    "                \"action\": \"rerank\",\n",
    "                \"method\": \"mmr\",\n",
    "                \"lambda\": 0.5,\n",
    "                \"description\": \"Balance relevance and diversity\"\n",
    "            },\n",
    "            {\n",
    "                \"action\": \"validate\",\n",
    "                \"description\": \"Assess result quality and completeness\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        return RetrievalPlan(\n",
    "            query_type=QueryType.COMPLEX_ANALYTICAL,\n",
    "            steps=steps,\n",
    "            search_strategy=\"hybrid\",\n",
    "            rerank=True,\n",
    "            max_results=15,\n",
    "            confidence_threshold=0.8\n",
    "        )\n",
    "    \n",
    "    def _plan_procedural(self, query: str, analysis: Dict) -> RetrievalPlan:\n",
    "        \"\"\"Plan for procedural/how-to queries\"\"\"\n",
    "        steps = [\n",
    "            {\n",
    "                \"action\": \"search\",\n",
    "                \"strategy\": \"sparse\",  # Good for exact procedural terms\n",
    "                \"query\": query,\n",
    "                \"limit\": 10,\n",
    "                \"filter\": {\"category\": [\"howto\", \"guide\", \"tutorial\"]},\n",
    "                \"description\": \"Search for procedural content\"\n",
    "            },\n",
    "            {\n",
    "                \"action\": \"order_by_steps\",\n",
    "                \"description\": \"Order results by logical step sequence\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        return RetrievalPlan(\n",
    "            query_type=QueryType.PROCEDURAL,\n",
    "            steps=steps,\n",
    "            search_strategy=\"sparse\",\n",
    "            rerank=False,  # Order matters more than diversity\n",
    "            max_results=8\n",
    "        )\n",
    "    \n",
    "    def _plan_multi_hop(self, query: str, analysis: Dict) -> RetrievalPlan:\n",
    "        \"\"\"Plan for multi-hop reasoning queries\"\"\"\n",
    "        steps = [\n",
    "            {\n",
    "                \"action\": \"search\",\n",
    "                \"strategy\": \"hybrid\",\n",
    "                \"query\": query,\n",
    "                \"limit\": 10,\n",
    "                \"description\": \"Initial search\"\n",
    "            },\n",
    "            {\n",
    "                \"action\": \"extract_entities\",\n",
    "                \"description\": \"Extract entities for follow-up\"\n",
    "            },\n",
    "            {\n",
    "                \"action\": \"iterative_search\",\n",
    "                \"max_hops\": 2,\n",
    "                \"description\": \"Follow-up searches based on extracted entities\"\n",
    "            },\n",
    "            {\n",
    "                \"action\": \"synthesize\",\n",
    "                \"description\": \"Combine information from multiple hops\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        return RetrievalPlan(\n",
    "            query_type=QueryType.MULTI_HOP,\n",
    "            steps=steps,\n",
    "            search_strategy=\"hybrid\",\n",
    "            rerank=True,\n",
    "            max_results=15\n",
    "        )\n",
    "    \n",
    "    def _plan_default(self, query: str, analysis: Dict) -> RetrievalPlan:\n",
    "        \"\"\"Default plan for unclassified queries\"\"\"\n",
    "        return self._plan_simple_factual(query, analysis)\n",
    "\n",
    "# Test retrieval planner\n",
    "planner = RetrievalPlanner(llm)\n",
    "\n",
    "print(\"\\nðŸ“‹ Retrieval Planning Examples:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for query in test_queries:\n",
    "    analysis = analyzer.analyze_query(query)\n",
    "    plan = planner.create_plan(query, analysis)\n",
    "    \n",
    "    print(f\"\\nðŸ“ Query: '{query}'\")\n",
    "    print(f\"   ðŸŽ¯ Query Type: {plan.query_type.value}\")\n",
    "    print(f\"   ðŸ” Strategy: {plan.search_strategy}\")\n",
    "    print(f\"   ðŸ“Š Max Results: {plan.max_results}\")\n",
    "    print(f\"   ðŸ”„ Rerank: {plan.rerank}\")\n",
    "    print(f\"   ðŸ“ˆ Steps: {len(plan.steps)}\")\n",
    "    \n",
    "    for i, step in enumerate(plan.steps, 1):\n",
    "        print(f\"     {i}. {step['action']}: {step.get('description', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b9d8d3",
   "metadata": {},
   "source": [
    "## ðŸ”§ Advanced Retrieval Executor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91a5695",
   "metadata": {},
   "source": [
    "## ðŸ§® Quality Assessment Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9e88f98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T02:49:27.301934Z",
     "iopub.status.busy": "2025-08-19T02:49:27.301827Z",
     "iopub.status.idle": "2025-08-19T02:49:29.526422Z",
     "shell.execute_reply": "2025-08-19T02:49:29.525631Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Quality Assessment for: 'Compare HNSW vs IVF algorithms for large-scale vector retrieval'\n",
      "==================================================\n",
      "Overall Quality: 0.332\n",
      "\n",
      "ðŸ“ˆ Detailed Metrics:\n",
      "   Relevance: 0.093\n",
      "   Completeness: 0.800\n",
      "   Diversity: 0.274\n",
      "   Coverage: 0.000\n",
      "\n",
      "ðŸ’¡ Suggestions:\n",
      "   1. Consider query rewriting or expansion for better relevance\n",
      "   2. Apply MMR reranking with lower lambda (0.3-0.4) for more diversity\n",
      "   3. Expand search to include more content categories\n",
      "   4. Low similarity scores - consider query reformulation\n",
      "\n",
      "ðŸ§  LLM Recommendation: These results do not meet the user's needs. A new search should be conducted with more relevant keywords.\n"
     ]
    }
   ],
   "source": [
    "class QualityAssessor:\n",
    "    \"\"\"Assesses retrieval quality and suggests improvements\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client: LLMClient):\n",
    "        self.llm = llm_client\n",
    "    \n",
    "    def assess_results(self, query: str, results: List[Dict], \n",
    "                      plan: RetrievalPlan) -> Dict[str, Any]:\n",
    "        \"\"\"Assess the quality of retrieval results\"\"\"\n",
    "        \n",
    "        if not results:\n",
    "            return {\n",
    "                \"overall_quality\": 0.0,\n",
    "                \"completeness\": 0.0,\n",
    "                \"relevance\": 0.0,\n",
    "                \"diversity\": 0.0,\n",
    "                \"needs_improvement\": True,\n",
    "                \"suggestions\": [\"No results found - revise search strategy\"]\n",
    "            }\n",
    "        \n",
    "        # Quantitative metrics\n",
    "        relevance_score = self._assess_relevance(query, results)\n",
    "        completeness_score = self._assess_completeness(query, results, plan)\n",
    "        diversity_score = self._assess_diversity(results)\n",
    "        coverage_score = self._assess_coverage(query, results)\n",
    "        \n",
    "        # Overall quality (weighted average)\n",
    "        overall_quality = (\n",
    "            0.4 * relevance_score +\n",
    "            0.3 * completeness_score +\n",
    "            0.2 * diversity_score +\n",
    "            0.1 * coverage_score\n",
    "        )\n",
    "        \n",
    "        # LLM-based assessment for qualitative insights\n",
    "        llm_assessment = self._llm_quality_assessment(query, results)\n",
    "        \n",
    "        # Generate suggestions\n",
    "        suggestions = self._generate_suggestions(\n",
    "            query, results, plan, \n",
    "            relevance_score, completeness_score, diversity_score\n",
    "        )\n",
    "        \n",
    "        assessment = {\n",
    "            \"overall_quality\": overall_quality,\n",
    "            \"metrics\": {\n",
    "                \"relevance\": relevance_score,\n",
    "                \"completeness\": completeness_score,\n",
    "                \"diversity\": diversity_score,\n",
    "                \"coverage\": coverage_score\n",
    "            },\n",
    "            \"llm_insights\": llm_assessment,\n",
    "            \"needs_improvement\": overall_quality < 0.7,\n",
    "            \"suggestions\": suggestions,\n",
    "            \"result_count\": len(results),\n",
    "            \"avg_score\": np.mean([r['score'] for r in results]) if results else 0\n",
    "        }\n",
    "        \n",
    "        return assessment\n",
    "    \n",
    "    def _assess_relevance(self, query: str, results: List[Dict]) -> float:\n",
    "        \"\"\"Assess relevance using score distribution and content matching\"\"\"\n",
    "        \n",
    "        if not results:\n",
    "            return 0.0\n",
    "        \n",
    "        # Use similarity scores as primary relevance indicator\n",
    "        scores = [r['score'] for r in results]\n",
    "        avg_score = np.mean(scores)\n",
    "        \n",
    "        # Content relevance (simplified keyword matching)\n",
    "        query_terms = set(query.lower().split())\n",
    "        content_relevance = []\n",
    "        \n",
    "        for result in results:\n",
    "            content_terms = set(result['content'].lower().split())\n",
    "            overlap = len(query_terms.intersection(content_terms))\n",
    "            relevance = overlap / len(query_terms) if query_terms else 0\n",
    "            content_relevance.append(relevance)\n",
    "        \n",
    "        content_score = np.mean(content_relevance)\n",
    "        \n",
    "        # Combine similarity and content relevance\n",
    "        return 0.7 * avg_score + 0.3 * content_score\n",
    "    \n",
    "    def _assess_completeness(self, query: str, results: List[Dict], \n",
    "                           plan: RetrievalPlan) -> float:\n",
    "        \"\"\"Assess completeness based on query type and result count\"\"\"\n",
    "        \n",
    "        result_count = len(results)\n",
    "        expected_count = plan.max_results\n",
    "        \n",
    "        # Base completeness on result count ratio\n",
    "        count_score = min(1.0, result_count / expected_count)\n",
    "        \n",
    "        # Adjust based on query type\n",
    "        if plan.query_type == QueryType.COMPARATIVE:\n",
    "            # Comparative queries should have diverse sources\n",
    "            categories = set(r['metadata']['category'] for r in results)\n",
    "            diversity_bonus = min(0.3, len(categories) * 0.1)\n",
    "            count_score += diversity_bonus\n",
    "        \n",
    "        elif plan.query_type == QueryType.MULTI_HOP:\n",
    "            # Multi-hop queries should have results from different hops\n",
    "            hops = set(r['retrieval_info'].get('hop', 0) for r in results)\n",
    "            hop_bonus = min(0.2, len(hops) * 0.1)\n",
    "            count_score += hop_bonus\n",
    "        \n",
    "        return min(1.0, count_score)\n",
    "    \n",
    "    def _assess_diversity(self, results: List[Dict]) -> float:\n",
    "        \"\"\"Assess diversity of results\"\"\"\n",
    "        \n",
    "        if len(results) <= 1:\n",
    "            return 1.0 if len(results) == 1 else 0.0\n",
    "        \n",
    "        # Category diversity\n",
    "        categories = [r['metadata']['category'] for r in results]\n",
    "        unique_categories = len(set(categories))\n",
    "        category_diversity = unique_categories / len(categories)\n",
    "        \n",
    "        # Content diversity (simplified using length variation)\n",
    "        content_lengths = [len(r['content']) for r in results]\n",
    "        length_std = np.std(content_lengths)\n",
    "        length_diversity = min(1.0, length_std / 100)  # Normalize\n",
    "        \n",
    "        # Score diversity\n",
    "        scores = [r['score'] for r in results]\n",
    "        score_std = np.std(scores)\n",
    "        score_diversity = min(1.0, score_std * 2)  # Boost diversity score\n",
    "        \n",
    "        return 0.5 * category_diversity + 0.3 * length_diversity + 0.2 * score_diversity\n",
    "    \n",
    "    def _assess_coverage(self, query: str, results: List[Dict]) -> float:\n",
    "        \"\"\"Assess how well results cover the query topics\"\"\"\n",
    "        \n",
    "        if not results:\n",
    "            return 0.0\n",
    "        \n",
    "        query_terms = set(query.lower().split())\n",
    "        covered_terms = set()\n",
    "        \n",
    "        for result in results:\n",
    "            content_terms = set(result['content'].lower().split())\n",
    "            covered_terms.update(content_terms.intersection(query_terms))\n",
    "        \n",
    "        coverage = len(covered_terms) / len(query_terms) if query_terms else 1.0\n",
    "        return coverage\n",
    "    \n",
    "    def _llm_quality_assessment(self, query: str, results: List[Dict]) -> Dict:\n",
    "        \"\"\"Get qualitative assessment from LLM\"\"\"\n",
    "        \n",
    "        # Prepare content sample for LLM\n",
    "        content_sample = \"\\n\\n\".join([\n",
    "            f\"Result {i+1}: {r['content'][:100]}...\"\n",
    "            for i, r in enumerate(results[:3])\n",
    "        ])\n",
    "        \n",
    "        prompt = f'''\n",
    "Assess the quality of these search results for the query: \"{query}\"\n",
    "\n",
    "Results:\n",
    "{content_sample}\n",
    "\n",
    "Return a JSON assessment with:\n",
    "{{\n",
    "    \"quality_score\": 0.0-1.0,\n",
    "    \"strengths\": [\"list of strengths\"],\n",
    "    \"weaknesses\": [\"list of weaknesses\"],\n",
    "    \"missing_topics\": [\"topics not covered\"],\n",
    "    \"recommendation\": \"overall recommendation\"\n",
    "}}\n",
    "        '''.strip()\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert at evaluating search result quality.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.chat_completion(messages)\n",
    "            return json.loads(response)\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"quality_score\": 0.7,\n",
    "                \"strengths\": [\"Results retrieved successfully\"],\n",
    "                \"weaknesses\": [\"LLM assessment unavailable\"],\n",
    "                \"missing_topics\": [],\n",
    "                \"recommendation\": \"Results appear relevant but detailed assessment unavailable\"\n",
    "            }\n",
    "    \n",
    "    def _generate_suggestions(self, query: str, results: List[Dict], \n",
    "                            plan: RetrievalPlan, relevance: float, \n",
    "                            completeness: float, diversity: float) -> List[str]:\n",
    "        \"\"\"Generate improvement suggestions\"\"\"\n",
    "        \n",
    "        suggestions = []\n",
    "        \n",
    "        # Relevance suggestions\n",
    "        if relevance < 0.6:\n",
    "            suggestions.append(\"Consider query rewriting or expansion for better relevance\")\n",
    "            if plan.search_strategy == \"dense\":\n",
    "                suggestions.append(\"Try hybrid search to capture keyword matches\")\n",
    "        \n",
    "        # Completeness suggestions\n",
    "        if completeness < 0.7:\n",
    "            suggestions.append(\"Increase search limit or reduce filters to get more results\")\n",
    "            if plan.query_type == QueryType.COMPLEX_ANALYTICAL:\n",
    "                suggestions.append(\"Consider multi-step retrieval with follow-up queries\")\n",
    "        \n",
    "        # Diversity suggestions\n",
    "        if diversity < 0.5:\n",
    "            suggestions.append(\"Apply MMR reranking with lower lambda (0.3-0.4) for more diversity\")\n",
    "            suggestions.append(\"Expand search to include more content categories\")\n",
    "        \n",
    "        # Query-type specific suggestions\n",
    "        if plan.query_type == QueryType.COMPARATIVE and len(results) < 6:\n",
    "            suggestions.append(\"Comparative queries benefit from more diverse results - increase limit\")\n",
    "        \n",
    "        if plan.query_type == QueryType.PROCEDURAL:\n",
    "            suggestions.append(\"For procedural queries, prioritize step-by-step and tutorial content\")\n",
    "        \n",
    "        # Result quality suggestions\n",
    "        if results and np.mean([r['score'] for r in results]) < 0.5:\n",
    "            suggestions.append(\"Low similarity scores - consider query reformulation\")\n",
    "        \n",
    "        if len(results) == 0:\n",
    "            suggestions.append(\"No results found - broaden search terms or reduce filters\")\n",
    "        \n",
    "        return suggestions if suggestions else [\"Results quality is acceptable\"]\n",
    "\n",
    "# Test quality assessor\n",
    "assessor = QualityAssessor(llm)\n",
    "\n",
    "if 'results' in locals() and results:\n",
    "    assessment = assessor.assess_results(test_query, results, plan)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Quality Assessment for: '{test_query}'\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Overall Quality: {assessment['overall_quality']:.3f}\")\n",
    "    print(f\"\\nðŸ“ˆ Detailed Metrics:\")\n",
    "    for metric, score in assessment['metrics'].items():\n",
    "        print(f\"   {metric.capitalize()}: {score:.3f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¡ Suggestions:\")\n",
    "    for i, suggestion in enumerate(assessment['suggestions'], 1):\n",
    "        print(f\"   {i}. {suggestion}\")\n",
    "    \n",
    "    if assessment['llm_insights'].get('recommendation'):\n",
    "        print(f\"\\nðŸ§  LLM Recommendation: {assessment['llm_insights']['recommendation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865441c8",
   "metadata": {},
   "source": [
    "## ðŸ¤– Complete Agentic RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a33023b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T02:49:29.531129Z",
     "iopub.status.busy": "2025-08-19T02:49:29.530765Z",
     "iopub.status.idle": "2025-08-19T02:49:32.883776Z",
     "shell.execute_reply": "2025-08-19T02:49:32.882963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¤– Agentic RAG Processing: 'What is vector search and how does it work?'\n",
      "============================================================\n",
      "\n",
      "1ï¸âƒ£ Query Analysis\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Query type: complex_analytical\n",
      "   Complexity: medium\n",
      "   Multi-step: True\n",
      "\n",
      "2ï¸âƒ£ Retrieval Planning\n",
      "   Strategy: hybrid\n",
      "   Steps: 4\n",
      "   Max results: 15\n",
      "\n",
      "3ï¸âƒ£ Retrieval Execution\n",
      "\n",
      "   Iteration 1:\n",
      "\n",
      "ðŸš€ Executing retrieval plan for: 'What is vector search and how does it work?'\n",
      "   Plan type: complex_analytical\n",
      "   Steps: 4\n",
      "\n",
      "   ðŸ“ Step 1: search\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      âŒ Search failed: Unexpected Response: 400 (Bad Request)\n",
      "Raw response content:\n",
      "b'{\"status\":{\"error\":\"Wrong input: Vector with name text_dense is not configured in this collection\"},\"time\":0.0000293}'\n",
      "      âš ï¸ No results from this step\n",
      "\n",
      "   ðŸ“ Step 2: analyze_gaps\n",
      "      âš ï¸ No results from this step\n",
      "\n",
      "   ðŸ“ Step 3: rerank\n",
      "      âš ï¸ No results to rerank\n",
      "      âš ï¸ No results from this step\n",
      "\n",
      "   ðŸ“ Step 4: validate\n",
      "      âš ï¸ No results from this step\n",
      "\n",
      "âœ… Plan execution complete: 0 final results\n",
      "   âš ï¸ No results - trying fallback strategy\n",
      "\n",
      "ðŸš€ Executing retrieval plan for: 'What is vector search and how does it work?'\n",
      "   Plan type: simple_factual\n",
      "   Steps: 1\n",
      "\n",
      "   ðŸ“ Step 1: search\n",
      "      âŒ Search failed: Unexpected Response: 400 (Bad Request)\n",
      "Raw response content:\n",
      "b'{\"status\":{\"error\":\"Wrong input: Vector with name text_dense is not configured in this collection\"},\"time\":0.000027675}'\n",
      "      âš ï¸ No results from this step\n",
      "\n",
      "âœ… Plan execution complete: 0 final results\n",
      "\n",
      "4ï¸âƒ£ Quality Assessment\n",
      "   Quality score: 0.000\n",
      "   Needs improvement: True\n",
      "\n",
      "5ï¸âƒ£ Plan Refinement\n",
      "\n",
      "   Iteration 2:\n",
      "\n",
      "ðŸš€ Executing retrieval plan for: 'What is vector search and how does it work?'\n",
      "   Plan type: simple_factual\n",
      "   Steps: 1\n",
      "\n",
      "   ðŸ“ Step 1: search\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      âŒ Search failed: Unexpected Response: 400 (Bad Request)\n",
      "Raw response content:\n",
      "b'{\"status\":{\"error\":\"Wrong input: Vector with name text_dense is not configured in this collection\"},\"time\":0.000028915}'\n",
      "      âš ï¸ No results from this step\n",
      "\n",
      "âœ… Plan execution complete: 0 final results\n",
      "   âš ï¸ No results - trying fallback strategy\n",
      "\n",
      "ðŸš€ Executing retrieval plan for: 'What is vector search and how does it work?'\n",
      "   Plan type: simple_factual\n",
      "   Steps: 1\n",
      "\n",
      "   ðŸ“ Step 1: search\n",
      "      âŒ Search failed: Unexpected Response: 400 (Bad Request)\n",
      "Raw response content:\n",
      "b'{\"status\":{\"error\":\"Wrong input: Vector with name text_dense is not configured in this collection\"},\"time\":0.000028996}'\n",
      "      âš ï¸ No results from this step\n",
      "\n",
      "âœ… Plan execution complete: 0 final results\n",
      "\n",
      "4ï¸âƒ£ Quality Assessment\n",
      "   Quality score: 0.000\n",
      "   Needs improvement: True\n",
      "\n",
      "5ï¸âƒ£ Plan Refinement\n",
      "\n",
      "   Iteration 3:\n",
      "\n",
      "ðŸš€ Executing retrieval plan for: 'What is vector search and how does it work?'\n",
      "   Plan type: simple_factual\n",
      "   Steps: 1\n",
      "\n",
      "   ðŸ“ Step 1: search\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      âŒ Search failed: Unexpected Response: 400 (Bad Request)\n",
      "Raw response content:\n",
      "b'{\"status\":{\"error\":\"Wrong input: Vector with name text_dense is not configured in this collection\"},\"time\":0.000028848}'\n",
      "      âš ï¸ No results from this step\n",
      "\n",
      "âœ… Plan execution complete: 0 final results\n",
      "   âš ï¸ No results - trying fallback strategy\n",
      "\n",
      "ðŸš€ Executing retrieval plan for: 'What is vector search and how does it work?'\n",
      "   Plan type: simple_factual\n",
      "   Steps: 1\n",
      "\n",
      "   ðŸ“ Step 1: search\n",
      "      âŒ Search failed: Unexpected Response: 400 (Bad Request)\n",
      "Raw response content:\n",
      "b'{\"status\":{\"error\":\"Wrong input: Vector with name text_dense is not configured in this collection\"},\"time\":0.000029275}'\n",
      "      âš ï¸ No results from this step\n",
      "\n",
      "âœ… Plan execution complete: 0 final results\n",
      "\n",
      "4ï¸âƒ£ Quality Assessment\n",
      "   Quality score: 0.000\n",
      "   Needs improvement: True\n",
      "\n",
      "6ï¸âƒ£ Answer Generation\n",
      "\n",
      "ðŸŽ¯ Final Response:\n",
      "Answer: I couldn't find relevant information to answer your question. Please try rephrasing your query or being more specific....\n",
      "\n",
      "ðŸ“Š Metadata:\n",
      "   query_type: complex_analytical\n",
      "   iterations: 3\n",
      "   total_time: 3.3299970626831055\n",
      "   result_count: 0\n",
      "   quality_score: 0.0\n"
     ]
    }
   ],
   "source": [
    "class AgenticRAGAgent:\n",
    "    \"\"\"Complete agentic RAG system that combines all components\"\"\"\n",
    "    \n",
    "    def __init__(self, client, collection_name: str, llm_client: LLMClient):\n",
    "        self.client = client\n",
    "        self.collection = collection_name\n",
    "        self.llm = llm_client\n",
    "        \n",
    "        # Initialize components\n",
    "        self.analyzer = QueryAnalyzer(llm_client)\n",
    "        self.planner = RetrievalPlanner(llm_client)\n",
    "        self.retriever = AdvancedRetriever(client, collection_name, llm_client)\n",
    "        self.assessor = QualityAssessor(llm_client)\n",
    "        \n",
    "        # Agent state\n",
    "        self.state = AgentState(\n",
    "            conversation_history=[],\n",
    "            retrieved_context=[],\n",
    "            current_query=\"\"\n",
    "        )\n",
    "        \n",
    "        self.max_iterations = 3  # Prevent infinite loops\n",
    "    \n",
    "    def query(self, user_query: str, context: Dict = None) -> Dict[str, Any]:\n",
    "        \"\"\"Process a user query through the complete agentic RAG pipeline\"\"\"\n",
    "        \n",
    "        print(f\"\\nðŸ¤– Agentic RAG Processing: '{user_query}'\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        self.state.current_query = user_query\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Phase 1: Query Analysis\n",
    "        print(f\"\\n1ï¸âƒ£ Query Analysis\")\n",
    "        analysis = self.analyzer.analyze_query(\n",
    "            user_query, self.state.conversation_history\n",
    "        )\n",
    "        \n",
    "        print(f\"   Query type: {analysis.get('query_type', 'unknown')}\")\n",
    "        print(f\"   Complexity: {analysis.get('complexity', 'unknown')}\")\n",
    "        print(f\"   Multi-step: {analysis.get('requires_multi_step', False)}\")\n",
    "        \n",
    "        # Phase 2: Retrieval Planning\n",
    "        print(f\"\\n2ï¸âƒ£ Retrieval Planning\")\n",
    "        plan = self.planner.create_plan(user_query, analysis)\n",
    "        self.state.query_plan = plan\n",
    "        \n",
    "        print(f\"   Strategy: {plan.search_strategy}\")\n",
    "        print(f\"   Steps: {len(plan.steps)}\")\n",
    "        print(f\"   Max results: {plan.max_results}\")\n",
    "        \n",
    "        # Phase 3: Retrieval Execution (with possible refinement)\n",
    "        print(f\"\\n3ï¸âƒ£ Retrieval Execution\")\n",
    "        \n",
    "        best_results = []\n",
    "        best_quality = 0.0\n",
    "        best_assessment = {'overall_quality': 0.0, 'needs_improvement': True}\n",
    "        iteration = 0\n",
    "        \n",
    "        while iteration < self.max_iterations:\n",
    "            iteration += 1\n",
    "            print(f\"\\n   Iteration {iteration}:\")\n",
    "            \n",
    "            # Execute current plan\n",
    "            results = self.retriever.execute_plan(plan, user_query)\n",
    "            \n",
    "            if not results:\n",
    "                print(f\"   âš ï¸ No results - trying fallback strategy\")\n",
    "                plan = self._create_fallback_plan(user_query)\n",
    "                results = self.retriever.execute_plan(plan, user_query)\n",
    "            \n",
    "            # Phase 4: Quality Assessment\n",
    "            print(f\"\\n4ï¸âƒ£ Quality Assessment\")\n",
    "            assessment = self.assessor.assess_results(user_query, results, plan)\n",
    "            \n",
    "            quality_score = assessment['overall_quality']\n",
    "            print(f\"   Quality score: {quality_score:.3f}\")\n",
    "            print(f\"   Needs improvement: {assessment['needs_improvement']}\")\n",
    "            \n",
    "            # Keep best results\n",
    "            if quality_score > best_quality:\n",
    "                best_results = results\n",
    "                best_quality = quality_score\n",
    "                best_assessment = assessment\n",
    "            \n",
    "            # Check if we should refine\n",
    "            if not assessment['needs_improvement'] or iteration >= self.max_iterations:\n",
    "                break\n",
    "            \n",
    "            # Phase 5: Plan Refinement\n",
    "            print(f\"\\n5ï¸âƒ£ Plan Refinement\")\n",
    "            plan = self._refine_plan(plan, assessment, user_query)\n",
    "            \n",
    "        # Phase 6: Answer Generation\n",
    "        print(f\"\\n6ï¸âƒ£ Answer Generation\")\n",
    "        final_answer = self._generate_answer(\n",
    "            user_query, best_results, best_assessment\n",
    "        )\n",
    "        \n",
    "        # Update state\n",
    "        self.state.retrieved_context = best_results\n",
    "        self.state.conversation_history.append({\n",
    "            \"user\": user_query,\n",
    "            \"assistant\": final_answer,\n",
    "            \"timestamp\": time.time(),\n",
    "            \"quality_score\": best_quality\n",
    "        })\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        return {\n",
    "            \"answer\": final_answer,\n",
    "            \"results\": best_results,\n",
    "            \"assessment\": best_assessment,\n",
    "            \"metadata\": {\n",
    "                \"query_type\": analysis.get('query_type'),\n",
    "                \"iterations\": iteration,\n",
    "                \"total_time\": total_time,\n",
    "                \"result_count\": len(best_results),\n",
    "                \"quality_score\": best_quality\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _create_fallback_plan(self, query: str) -> RetrievalPlan:\n",
    "        \"\"\"Create a simple fallback plan when main plan fails\"\"\"\n",
    "        \n",
    "        return RetrievalPlan(\n",
    "            query_type=QueryType.SIMPLE_FACTUAL,\n",
    "            steps=[\n",
    "                {\n",
    "                    \"action\": \"search\",\n",
    "                    \"strategy\": \"hybrid\",\n",
    "                    \"query\": query,\n",
    "                    \"limit\": 10\n",
    "                }\n",
    "            ],\n",
    "            search_strategy=\"hybrid\",\n",
    "            max_results=10\n",
    "        )\n",
    "    \n",
    "    def _refine_plan(self, plan: RetrievalPlan, assessment: Dict, query: str) -> RetrievalPlan:\n",
    "        \"\"\"Refine retrieval plan based on quality assessment\"\"\"\n",
    "        \n",
    "        suggestions = assessment.get('suggestions', [])\n",
    "        \n",
    "        # Create refined plan\n",
    "        refined_steps = []\n",
    "        \n",
    "        for suggestion in suggestions:\n",
    "            if \"hybrid search\" in suggestion.lower():\n",
    "                refined_steps.append({\n",
    "                    \"action\": \"search\",\n",
    "                    \"strategy\": \"hybrid\",\n",
    "                    \"query\": query,\n",
    "                    \"limit\": plan.max_results * 2\n",
    "                })\n",
    "            \n",
    "            elif \"mmr\" in suggestion.lower():\n",
    "                refined_steps.append({\n",
    "                    \"action\": \"rerank\",\n",
    "                    \"method\": \"mmr\",\n",
    "                    \"lambda\": 0.3  # More diversity\n",
    "                })\n",
    "            \n",
    "            elif \"increase\" in suggestion.lower() and \"limit\" in suggestion.lower():\n",
    "                # Modify existing search steps to increase limit\n",
    "                for step in plan.steps:\n",
    "                    if step.get('action') == 'search':\n",
    "                        step['limit'] = step.get('limit', 10) * 2\n",
    "        \n",
    "        # If we have refinement suggestions, use them\n",
    "        if refined_steps:\n",
    "            plan.steps = refined_steps\n",
    "        else:\n",
    "            # Fallback refinement: increase limits\n",
    "            plan.max_results = min(20, plan.max_results * 2)\n",
    "            for step in plan.steps:\n",
    "                if 'limit' in step:\n",
    "                    step['limit'] = min(20, step['limit'] * 2)\n",
    "        \n",
    "        return plan\n",
    "    \n",
    "    def _generate_answer(self, query: str, results: List[Dict], assessment: Dict) -> str:\n",
    "        \"\"\"Generate final answer using retrieved context\"\"\"\n",
    "        \n",
    "        if not results:\n",
    "            return \"I couldn't find relevant information to answer your question. Please try rephrasing your query or being more specific.\"\n",
    "        \n",
    "        # Prepare context from results\n",
    "        context_parts = []\n",
    "        for i, result in enumerate(results[:5], 1):  # Top 5 results\n",
    "            context_parts.append(\n",
    "                f\"Source {i}: {result['content'][:200]}...\"\n",
    "            )\n",
    "        \n",
    "        context_text = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        # Generate answer using LLM\n",
    "        prompt = f'''\n",
    "Based on the following retrieved information, provide a comprehensive and accurate answer to the user's question.\n",
    "\n",
    "User Question: {query}\n",
    "\n",
    "Retrieved Context:\n",
    "{context_text}\n",
    "\n",
    "Instructions:\n",
    "- Use only the information provided in the context\n",
    "- Be specific and cite sources when possible\n",
    "- If the context doesn't fully answer the question, acknowledge the limitations\n",
    "- Structure your answer clearly and logically\n",
    "        '''.strip()\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant that answers questions based on retrieved information.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            answer = self.llm.chat_completion(messages, temperature=0.1)\n",
    "            \n",
    "            # Add quality note if assessment indicates issues\n",
    "            if assessment['overall_quality'] < 0.7:\n",
    "                answer += \"\\n\\n*Note: The retrieved information may be incomplete. Consider asking more specific questions or trying different search terms.*\"\n",
    "            \n",
    "            return answer\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ Answer generation failed: {e}\")\n",
    "            \n",
    "            # Fallback answer\n",
    "            return f\"Based on the retrieved information, I found {len(results)} relevant sources that discuss {query}. However, I'm unable to generate a detailed answer at the moment. Please review the source materials directly.\"\n",
    "    \n",
    "    def get_conversation_history(self) -> List[Dict]:\n",
    "        \"\"\"Get conversation history\"\"\"\n",
    "        return self.state.conversation_history\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear conversation history\"\"\"\n",
    "        self.state.conversation_history = []\n",
    "        self.state.retrieved_context = []\n",
    "\n",
    "# Initialize and test the complete agentic RAG system\n",
    "if KNOWLEDGE_COLLECTION:\n",
    "    agent = AgenticRAGAgent(client, KNOWLEDGE_COLLECTION, llm)\n",
    "    \n",
    "    # Test with different query types\n",
    "    test_queries_agentic = [\n",
    "        \"What is vector search and how does it work?\",\n",
    "        \"Compare dense vs sparse vector search methods\",\n",
    "        \"How do I optimize HNSW performance in production?\"\n",
    "    ]\n",
    "    \n",
    "    for test_query in test_queries_agentic[:1]:  # Test one for demonstration\n",
    "        result = agent.query(test_query)\n",
    "        \n",
    "        print(f\"\\nðŸŽ¯ Final Response:\")\n",
    "        print(f\"Answer: {result['answer'][:200]}...\")\n",
    "        print(f\"\\nðŸ“Š Metadata:\")\n",
    "        for key, value in result['metadata'].items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "        \n",
    "        break  # Only run one example to avoid too much output\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ Skipping agentic RAG test - no knowledge collection available\")\n",
    "    print(\"\\nðŸ’¡ To test the complete system:\")\n",
    "    print(\"   1. Run Notebook 1 or 2 to create a knowledge collection\")\n",
    "    print(\"   2. Set up LLM API keys (OpenAI or Anthropic)\")\n",
    "    print(\"   3. Re-run this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4963161",
   "metadata": {},
   "source": [
    "## ðŸŽ® Advanced Features Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6da43892",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T02:49:32.886526Z",
     "iopub.status.busy": "2025-08-19T02:49:32.886302Z",
     "iopub.status.idle": "2025-08-19T02:49:32.897037Z",
     "shell.execute_reply": "2025-08-19T02:49:32.896610Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ® Advanced Agentic RAG Features\n",
      "==================================================\n",
      "\n",
      "ðŸ”§ 1. Query Rewriting\n",
      "   â€¢ Automatic query expansion\n",
      "   â€¢ Synonym replacement\n",
      "   â€¢ Technical term normalization\n",
      "   â€¢ Multi-language query handling\n",
      "\n",
      "ðŸ” 2. Multi-Modal Retrieval\n",
      "   â€¢ Text + image search\n",
      "   â€¢ Code + documentation retrieval\n",
      "   â€¢ Structured data integration\n",
      "\n",
      "ðŸ§  3. Reasoning Chains\n",
      "   â€¢ Chain-of-thought retrieval\n",
      "   â€¢ Evidence combination\n",
      "   â€¢ Contradiction detection\n",
      "   â€¢ Confidence calibration\n",
      "\n",
      "ðŸ”„ 4. Self-Correction\n",
      "   â€¢ Answer validation\n",
      "   â€¢ Retrieval refinement\n",
      "   â€¢ Source verification\n",
      "   â€¢ Hallucination detection\n",
      "\n",
      "ðŸ“š 5. Knowledge Graph Integration\n",
      "   â€¢ Entity relationship traversal\n",
      "   â€¢ Structured knowledge queries\n",
      "   â€¢ Factual consistency checking\n",
      "\n",
      "ðŸŽ¯ 6. Personalization\n",
      "   â€¢ User expertise level adaptation\n",
      "   â€¢ Domain preference learning\n",
      "   â€¢ Interaction history analysis\n",
      "\n",
      "ðŸ”§ 7. Tool Integration\n",
      "   â€¢ Calculator for numerical queries\n",
      "   â€¢ Code execution for programming\n",
      "   â€¢ Web search for current info\n",
      "   â€¢ API calls for live data\n",
      "\n",
      "ðŸ”„ Query Rewriting Examples:\n",
      "========================================\n",
      "\n",
      "ðŸ“ Original: 'How to make my search faster?'\n",
      "ðŸ”„ Rewritten: 'vector search optimization performance tuning latency reduction HNSW parameters'\n",
      "ðŸ’­ Reason: Expanded with technical terms\n",
      "\n",
      "ðŸ“ Original: 'DB vs vector store'\n",
      "ðŸ”„ Rewritten: 'database comparison vector database traditional relational database differences'\n",
      "ðŸ’­ Reason: Clarified abbreviations and added context\n",
      "\n",
      "ðŸ“ Original: 'Why is recall bad?'\n",
      "ðŸ”„ Rewritten: 'recall degradation causes HNSW index performance issues approximate nearest neighbor'\n",
      "ðŸ’­ Reason: Added technical context and specificity\n",
      "\n",
      "ðŸ”— Multi-Hop Reasoning Example:\n",
      "========================================\n",
      "â“ Query: 'Why might my vector database be slow after adding lots of data?'\n",
      "\n",
      "ðŸŽ¯ Reasoning Chain:\n",
      "\n",
      "1ï¸âƒ£ Initial Search: 'vector database performance issues'\n",
      "   â†’ Entities found: HNSW, index degradation, memory usage\n",
      "\n",
      "2ï¸âƒ£ Follow-up Search: 'HNSW index degradation causes'\n",
      "   â†’ More entities: graph connectivity, update/delete ratio\n",
      "\n",
      "3ï¸âƒ£ Final Search: 'index optimization healing rebuild'\n",
      "   â†’ Solution entities: optimization, parameter tuning\n",
      "\n",
      "ðŸ“‹ Synthesized Answer:\n",
      "   'Vector database slowdown after adding data is typically caused by\n",
      "    HNSW index degradation. As you insert many new vectors, the graph\n",
      "    structure becomes suboptimal, leading to longer search paths and\n",
      "    reduced recall. Solutions include: 1) Regular index optimization,\n",
      "    2) Tuning HNSW parameters like ef_construct, 3) Index rebuilding.'\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_advanced_features():\n",
    "    \"\"\"Demonstrate advanced agentic RAG features\"\"\"\n",
    "    \n",
    "    print(\"ðŸŽ® Advanced Agentic RAG Features\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"\\nðŸ”§ 1. Query Rewriting\")\n",
    "    print(\"   â€¢ Automatic query expansion\")\n",
    "    print(\"   â€¢ Synonym replacement\")\n",
    "    print(\"   â€¢ Technical term normalization\")\n",
    "    print(\"   â€¢ Multi-language query handling\")\n",
    "    \n",
    "    print(\"\\nðŸ” 2. Multi-Modal Retrieval\")\n",
    "    print(\"   â€¢ Text + image search\")\n",
    "    print(\"   â€¢ Code + documentation retrieval\")\n",
    "    print(\"   â€¢ Structured data integration\")\n",
    "    \n",
    "    print(\"\\nðŸ§  3. Reasoning Chains\")\n",
    "    print(\"   â€¢ Chain-of-thought retrieval\")\n",
    "    print(\"   â€¢ Evidence combination\")\n",
    "    print(\"   â€¢ Contradiction detection\")\n",
    "    print(\"   â€¢ Confidence calibration\")\n",
    "    \n",
    "    print(\"\\nðŸ”„ 4. Self-Correction\")\n",
    "    print(\"   â€¢ Answer validation\")\n",
    "    print(\"   â€¢ Retrieval refinement\")\n",
    "    print(\"   â€¢ Source verification\")\n",
    "    print(\"   â€¢ Hallucination detection\")\n",
    "    \n",
    "    print(\"\\nðŸ“š 5. Knowledge Graph Integration\")\n",
    "    print(\"   â€¢ Entity relationship traversal\")\n",
    "    print(\"   â€¢ Structured knowledge queries\")\n",
    "    print(\"   â€¢ Factual consistency checking\")\n",
    "    \n",
    "    print(\"\\nðŸŽ¯ 6. Personalization\")\n",
    "    print(\"   â€¢ User expertise level adaptation\")\n",
    "    print(\"   â€¢ Domain preference learning\")\n",
    "    print(\"   â€¢ Interaction history analysis\")\n",
    "    \n",
    "    print(\"\\nðŸ”§ 7. Tool Integration\")\n",
    "    print(\"   â€¢ Calculator for numerical queries\")\n",
    "    print(\"   â€¢ Code execution for programming\")\n",
    "    print(\"   â€¢ Web search for current info\")\n",
    "    print(\"   â€¢ API calls for live data\")\n",
    "\n",
    "def demonstrate_query_rewriting():\n",
    "    \"\"\"Show query rewriting examples\"\"\"\n",
    "    \n",
    "    print(\"\\nðŸ”„ Query Rewriting Examples:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    rewrites = [\n",
    "        {\n",
    "            \"original\": \"How to make my search faster?\",\n",
    "            \"rewritten\": \"vector search optimization performance tuning latency reduction HNSW parameters\",\n",
    "            \"reason\": \"Expanded with technical terms\"\n",
    "        },\n",
    "        {\n",
    "            \"original\": \"DB vs vector store\",\n",
    "            \"rewritten\": \"database comparison vector database traditional relational database differences\",\n",
    "            \"reason\": \"Clarified abbreviations and added context\"\n",
    "        },\n",
    "        {\n",
    "            \"original\": \"Why is recall bad?\",\n",
    "            \"rewritten\": \"recall degradation causes HNSW index performance issues approximate nearest neighbor\",\n",
    "            \"reason\": \"Added technical context and specificity\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for example in rewrites:\n",
    "        print(f\"\\nðŸ“ Original: '{example['original']}'\")\n",
    "        print(f\"ðŸ”„ Rewritten: '{example['rewritten']}'\")\n",
    "        print(f\"ðŸ’­ Reason: {example['reason']}\")\n",
    "\n",
    "def demonstrate_multi_hop_reasoning():\n",
    "    \"\"\"Show multi-hop reasoning example\"\"\"\n",
    "    \n",
    "    print(\"\\nðŸ”— Multi-Hop Reasoning Example:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    query = \"Why might my vector database be slow after adding lots of data?\"\n",
    "    \n",
    "    print(f\"â“ Query: '{query}'\")\n",
    "    print(f\"\\nðŸŽ¯ Reasoning Chain:\")\n",
    "    print(f\"\\n1ï¸âƒ£ Initial Search: 'vector database performance issues'\")\n",
    "    print(f\"   â†’ Entities found: HNSW, index degradation, memory usage\")\n",
    "    \n",
    "    print(f\"\\n2ï¸âƒ£ Follow-up Search: 'HNSW index degradation causes'\")\n",
    "    print(f\"   â†’ More entities: graph connectivity, update/delete ratio\")\n",
    "    \n",
    "    print(f\"\\n3ï¸âƒ£ Final Search: 'index optimization healing rebuild'\")\n",
    "    print(f\"   â†’ Solution entities: optimization, parameter tuning\")\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ Synthesized Answer:\")\n",
    "    print(f\"   'Vector database slowdown after adding data is typically caused by\")\n",
    "    print(f\"    HNSW index degradation. As you insert many new vectors, the graph\")\n",
    "    print(f\"    structure becomes suboptimal, leading to longer search paths and\")\n",
    "    print(f\"    reduced recall. Solutions include: 1) Regular index optimization,\")\n",
    "    print(f\"    2) Tuning HNSW parameters like ef_construct, 3) Index rebuilding.'\")\n",
    "\n",
    "demonstrate_advanced_features()\n",
    "demonstrate_query_rewriting()\n",
    "demonstrate_multi_hop_reasoning()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084877ae",
   "metadata": {},
   "source": [
    "## ðŸ“Š Performance Analysis & Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bbdee73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T02:49:32.898556Z",
     "iopub.status.busy": "2025-08-19T02:49:32.898450Z",
     "iopub.status.idle": "2025-08-19T02:49:32.903911Z",
     "shell.execute_reply": "2025-08-19T02:49:32.903454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Agentic RAG Performance Analysis\n",
      "==================================================\n",
      "\n",
      "âš¡ Latency Breakdown:\n",
      "   Query Analysis      : 50-200     ms - LLM call to classify query\n",
      "   Plan Generation     : 30-100     ms - Creating retrieval strategy\n",
      "   Vector Search       : 10-50      ms - Per search operation\n",
      "   Reranking (MMR)     : 5-20       ms - Per rerank operation\n",
      "   Quality Assessment  : 100-300    ms - LLM evaluation of results\n",
      "   Answer Generation   : 200-800    ms - Final LLM response\n",
      "\n",
      "   Total Pipeline      : 395-1470   ms\n",
      "\n",
      "ðŸ’° Cost Analysis (per query):\n",
      "   Query Analysis      : $0.001-0.003\n",
      "   Quality Assessment  : $0.002-0.005\n",
      "   Answer Generation   : $0.003-0.010\n",
      "   Vector Operations   : $0.0001-0.001\n",
      "\n",
      "ðŸ“ˆ Scalability Considerations:\n",
      "   â€¢ LLM calls are the primary bottleneck\n",
      "   â€¢ Vector operations scale sub-linearly\n",
      "   â€¢ Caching can reduce 60-80% of LLM calls\n",
      "   â€¢ Async processing enables parallelization\n",
      "\n",
      "ðŸŽ¯ Optimization Strategies:\n",
      "   Caching        : Cache query analysis and plans      â†’ 50-80% latency reduction\n",
      "   Batching       : Batch multiple queries to LLM       â†’ 30-50% cost reduction\n",
      "   Early Stopping : Stop refinement when quality is good â†’ 20-40% latency reduction\n",
      "   Async Processing: Parallel retrieval and assessment   â†’ 30-60% latency reduction\n",
      "   Model Sizing   : Use smaller models for classification â†’ 50-70% cost reduction\n",
      "\n",
      "ðŸ† Agentic RAG Best Practices\n",
      "==================================================\n",
      "\n",
      "ðŸŽ¯ Query Understanding\n",
      "   â€¢ Use lightweight models for query classification\n",
      "   â€¢ Implement fallback rules when LLM classification fails\n",
      "   â€¢ Cache common query patterns and their classifications\n",
      "   â€¢ Consider query similarity for cache hits\n",
      "\n",
      "ðŸ” Retrieval Strategy\n",
      "   â€¢ Match retrieval strategy to query type\n",
      "   â€¢ Use hybrid search as the default for unknown query types\n",
      "   â€¢ Implement progressive refinement with quality thresholds\n",
      "   â€¢ Set reasonable iteration limits to prevent infinite loops\n",
      "\n",
      "ðŸ“Š Quality Assessment\n",
      "   â€¢ Combine multiple quality signals (relevance, diversity, coverage)\n",
      "   â€¢ Use LLM assessment sparingly due to cost\n",
      "   â€¢ Implement fast heuristic checks before expensive evaluation\n",
      "   â€¢ Track quality metrics over time for system improvement\n",
      "\n",
      "ðŸ¤– Answer Generation\n",
      "   â€¢ Provide clear context boundaries to reduce hallucination\n",
      "   â€¢ Include confidence indicators in responses\n",
      "   â€¢ Cite sources and provide retrieval transparency\n",
      "   â€¢ Gracefully handle low-quality retrieval results\n",
      "\n",
      "âš¡ Performance\n",
      "   â€¢ Cache at multiple levels (query, plan, results)\n",
      "   â€¢ Use async/parallel processing where possible\n",
      "   â€¢ Implement circuit breakers for external API calls\n",
      "   â€¢ Monitor and alert on latency and cost metrics\n",
      "\n",
      "ðŸ›¡ï¸ Production Readiness\n",
      "   â€¢ Implement comprehensive error handling and retries\n",
      "   â€¢ Add request rate limiting and quota management\n",
      "   â€¢ Create health checks for all system components\n",
      "   â€¢ Maintain audit logs for debugging and compliance\n"
     ]
    }
   ],
   "source": [
    "def analyze_agentic_rag_performance():\n",
    "    \"\"\"Analysis of agentic RAG performance characteristics\"\"\"\n",
    "    \n",
    "    print(\"ðŸ“Š Agentic RAG Performance Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"\\nâš¡ Latency Breakdown:\")\n",
    "    \n",
    "    latency_components = {\n",
    "        \"Query Analysis\": {\"time_ms\": \"50-200\", \"description\": \"LLM call to classify query\"},\n",
    "        \"Plan Generation\": {\"time_ms\": \"30-100\", \"description\": \"Creating retrieval strategy\"},\n",
    "        \"Vector Search\": {\"time_ms\": \"10-50\", \"description\": \"Per search operation\"},\n",
    "        \"Reranking (MMR)\": {\"time_ms\": \"5-20\", \"description\": \"Per rerank operation\"},\n",
    "        \"Quality Assessment\": {\"time_ms\": \"100-300\", \"description\": \"LLM evaluation of results\"},\n",
    "        \"Answer Generation\": {\"time_ms\": \"200-800\", \"description\": \"Final LLM response\"}\n",
    "    }\n",
    "    \n",
    "    total_min = sum([int(comp[\"time_ms\"].split(\"-\")[0]) for comp in latency_components.values()])\n",
    "    total_max = sum([int(comp[\"time_ms\"].split(\"-\")[1]) for comp in latency_components.values()])\n",
    "    \n",
    "    for component, details in latency_components.items():\n",
    "        print(f\"   {component:<20}: {details['time_ms']:<10} ms - {details['description']}\")\n",
    "    \n",
    "    print(f\"\\n   {'Total Pipeline':<20}: {total_min}-{total_max:<6} ms\")\n",
    "    \n",
    "    print(f\"\\nðŸ’° Cost Analysis (per query):\")\n",
    "    cost_components = {\n",
    "        \"Query Analysis\": \"$0.001-0.003\",\n",
    "        \"Quality Assessment\": \"$0.002-0.005\", \n",
    "        \"Answer Generation\": \"$0.003-0.010\",\n",
    "        \"Vector Operations\": \"$0.0001-0.001\"\n",
    "    }\n",
    "    \n",
    "    for component, cost in cost_components.items():\n",
    "        print(f\"   {component:<20}: {cost}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“ˆ Scalability Considerations:\")\n",
    "    print(\"   â€¢ LLM calls are the primary bottleneck\")\n",
    "    print(\"   â€¢ Vector operations scale sub-linearly\")\n",
    "    print(\"   â€¢ Caching can reduce 60-80% of LLM calls\")\n",
    "    print(\"   â€¢ Async processing enables parallelization\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ Optimization Strategies:\")\n",
    "    \n",
    "    strategies = {\n",
    "        \"Caching\": {\n",
    "            \"what\": \"Cache query analysis and plans\",\n",
    "            \"impact\": \"50-80% latency reduction\"\n",
    "        },\n",
    "        \"Batching\": {\n",
    "            \"what\": \"Batch multiple queries to LLM\",\n",
    "            \"impact\": \"30-50% cost reduction\"\n",
    "        },\n",
    "        \"Early Stopping\": {\n",
    "            \"what\": \"Stop refinement when quality is good\",\n",
    "            \"impact\": \"20-40% latency reduction\"\n",
    "        },\n",
    "        \"Async Processing\": {\n",
    "            \"what\": \"Parallel retrieval and assessment\",\n",
    "            \"impact\": \"30-60% latency reduction\"\n",
    "        },\n",
    "        \"Model Sizing\": {\n",
    "            \"what\": \"Use smaller models for classification\",\n",
    "            \"impact\": \"50-70% cost reduction\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for strategy, details in strategies.items():\n",
    "        print(f\"   {strategy:<15}: {details['what']:<35} â†’ {details['impact']}\")\n",
    "\n",
    "def best_practices_guide():\n",
    "    \"\"\"Comprehensive best practices for agentic RAG\"\"\"\n",
    "    \n",
    "    print(\"\\nðŸ† Agentic RAG Best Practices\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    practices = {\n",
    "        \"ðŸŽ¯ Query Understanding\": [\n",
    "            \"Use lightweight models for query classification\",\n",
    "            \"Implement fallback rules when LLM classification fails\",\n",
    "            \"Cache common query patterns and their classifications\",\n",
    "            \"Consider query similarity for cache hits\"\n",
    "        ],\n",
    "        \"ðŸ” Retrieval Strategy\": [\n",
    "            \"Match retrieval strategy to query type\",\n",
    "            \"Use hybrid search as the default for unknown query types\",\n",
    "            \"Implement progressive refinement with quality thresholds\",\n",
    "            \"Set reasonable iteration limits to prevent infinite loops\"\n",
    "        ],\n",
    "        \"ðŸ“Š Quality Assessment\": [\n",
    "            \"Combine multiple quality signals (relevance, diversity, coverage)\",\n",
    "            \"Use LLM assessment sparingly due to cost\",\n",
    "            \"Implement fast heuristic checks before expensive evaluation\",\n",
    "            \"Track quality metrics over time for system improvement\"\n",
    "        ],\n",
    "        \"ðŸ¤– Answer Generation\": [\n",
    "            \"Provide clear context boundaries to reduce hallucination\",\n",
    "            \"Include confidence indicators in responses\",\n",
    "            \"Cite sources and provide retrieval transparency\",\n",
    "            \"Gracefully handle low-quality retrieval results\"\n",
    "        ],\n",
    "        \"âš¡ Performance\": [\n",
    "            \"Cache at multiple levels (query, plan, results)\",\n",
    "            \"Use async/parallel processing where possible\",\n",
    "            \"Implement circuit breakers for external API calls\",\n",
    "            \"Monitor and alert on latency and cost metrics\"\n",
    "        ],\n",
    "        \"ðŸ›¡ï¸ Production Readiness\": [\n",
    "            \"Implement comprehensive error handling and retries\",\n",
    "            \"Add request rate limiting and quota management\",\n",
    "            \"Create health checks for all system components\",\n",
    "            \"Maintain audit logs for debugging and compliance\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, items in practices.items():\n",
    "        print(f\"\\n{category}\")\n",
    "        for item in items:\n",
    "            print(f\"   â€¢ {item}\")\n",
    "\n",
    "analyze_agentic_rag_performance()\n",
    "best_practices_guide()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900c4a79",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Final Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ad32c70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T02:49:32.905544Z",
     "iopub.status.busy": "2025-08-19T02:49:32.905394Z",
     "iopub.status.idle": "2025-08-19T02:49:32.910525Z",
     "shell.execute_reply": "2025-08-19T02:49:32.910304Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ‰ Agentic RAG Workshop Summary\n",
      "==================================================\n",
      "\n",
      "ðŸ“š Components Built:\n",
      "   âœ… Query Analyzer: Classifies queries and determines complexity\n",
      "   âœ… Retrieval Planner: Creates multi-step retrieval strategies\n",
      "   âœ… Advanced Retriever: Executes complex retrieval plans\n",
      "   âœ… Quality Assessor: Evaluates result quality and suggests improvements\n",
      "   âœ… Complete Agent: Orchestrates the full agentic RAG pipeline\n",
      "\n",
      "ðŸ” Key Techniques Demonstrated:\n",
      "   ðŸŽ¯ Multi-step query analysis and planning\n",
      "   ðŸŽ¯ Adaptive retrieval strategies by query type\n",
      "   ðŸŽ¯ Self-correcting retrieval with quality feedback\n",
      "   ðŸŽ¯ MMR reranking for result diversification\n",
      "   ðŸŽ¯ Hybrid search integration (dense + sparse)\n",
      "   ðŸŽ¯ Conversational context management\n",
      "   ðŸŽ¯ Quality assessment and improvement suggestions\n",
      "   ðŸŽ¯ Answer generation with source attribution\n",
      "\n",
      "ðŸ“Š System Performance:\n",
      "   Knowledge Base: agentic_rag_demo\n",
      "   LLM Provider: openai\n",
      "   Queries Processed: 1\n",
      "   Average Quality Score: 0.000\n",
      "\n",
      "ðŸš€ Production Deployment Checklist:\n",
      "   1. Set up proper LLM API credentials and rate limiting\n",
      "   2. Implement comprehensive caching strategy\n",
      "   3. Add monitoring and alerting for all components\n",
      "   4. Create fallback strategies for component failures\n",
      "   5. Implement user session and conversation management\n",
      "   6. Add evaluation framework with human feedback\n",
      "   7. Set up A/B testing for different strategies\n",
      "   8. Create security controls for sensitive queries\n",
      "\n",
      "ðŸŽ¯ Next Steps for Enhancement:\n",
      "   ðŸ”® Add multi-modal retrieval (text + images + code)\n",
      "   ðŸ”® Implement knowledge graph integration\n",
      "   ðŸ”® Build user personalization and preference learning\n",
      "   ðŸ”® Add real-time learning from user feedback\n",
      "   ðŸ”® Integrate with external tools and APIs\n",
      "   ðŸ”® Implement advanced reasoning patterns\n",
      "   ðŸ”® Add support for different domain expertise levels\n",
      "   ðŸ”® Build collaborative filtering for query suggestions\n",
      "\n",
      "ðŸ’¡ Key Insights:\n",
      "   â€¢ Agentic RAG significantly improves answer quality over traditional RAG\n",
      "   â€¢ Multi-step retrieval helps with complex and comparative queries\n",
      "   â€¢ Quality assessment enables self-correction and continuous improvement\n",
      "   â€¢ Proper caching and optimization are critical for production performance\n",
      "   â€¢ The system can adapt its strategy based on query characteristics\n",
      "\n",
      "âœ¨ Congratulations! You've built a complete agentic RAG system.\n",
      "\n",
      "ðŸ“– This completes the 5-notebook Qdrant workshop series:\n",
      "   1. Fundamentals & Search Basics âœ…\n",
      "   2. Hybrid Search (Dense + Sparse) âœ…\n",
      "   3. MMR Reranking âœ…\n",
      "   4. HNSW Index Health âœ…\n",
      "   5. Agentic RAG âœ…\n",
      "\n",
      "ðŸŽ“ You now have comprehensive knowledge of:\n",
      "   â€¢ Vector database fundamentals\n",
      "   â€¢ Advanced search techniques\n",
      "   â€¢ Production optimization strategies\n",
      "   â€¢ Intelligent RAG architectures\n",
      "\n",
      "ðŸŒŸ Ready to build amazing AI applications with Qdrant!\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸŽ‰ Agentic RAG Workshop Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nðŸ“š Components Built:\")\n",
    "components = {\n",
    "    \"Query Analyzer\": \"Classifies queries and determines complexity\",\n",
    "    \"Retrieval Planner\": \"Creates multi-step retrieval strategies\", \n",
    "    \"Advanced Retriever\": \"Executes complex retrieval plans\",\n",
    "    \"Quality Assessor\": \"Evaluates result quality and suggests improvements\",\n",
    "    \"Complete Agent\": \"Orchestrates the full agentic RAG pipeline\"\n",
    "}\n",
    "\n",
    "for component, description in components.items():\n",
    "    print(f\"   âœ… {component}: {description}\")\n",
    "\n",
    "print(f\"\\nðŸ” Key Techniques Demonstrated:\")\n",
    "techniques = [\n",
    "    \"Multi-step query analysis and planning\",\n",
    "    \"Adaptive retrieval strategies by query type\", \n",
    "    \"Self-correcting retrieval with quality feedback\",\n",
    "    \"MMR reranking for result diversification\",\n",
    "    \"Hybrid search integration (dense + sparse)\",\n",
    "    \"Conversational context management\",\n",
    "    \"Quality assessment and improvement suggestions\",\n",
    "    \"Answer generation with source attribution\"\n",
    "]\n",
    "\n",
    "for technique in techniques:\n",
    "    print(f\"   ðŸŽ¯ {technique}\")\n",
    "\n",
    "if KNOWLEDGE_COLLECTION:\n",
    "    print(f\"\\nðŸ“Š System Performance:\")\n",
    "    print(f\"   Knowledge Base: {KNOWLEDGE_COLLECTION}\")\n",
    "    print(f\"   LLM Provider: {LLM_PROVIDER}\")\n",
    "    if 'agent' in locals():\n",
    "        history = agent.get_conversation_history()\n",
    "        print(f\"   Queries Processed: {len(history)}\")\n",
    "        if history:\n",
    "            avg_quality = np.mean([h.get('quality_score', 0) for h in history])\n",
    "            print(f\"   Average Quality Score: {avg_quality:.3f}\")\n",
    "\n",
    "print(f\"\\nðŸš€ Production Deployment Checklist:\")\n",
    "checklist = [\n",
    "    \"Set up proper LLM API credentials and rate limiting\",\n",
    "    \"Implement comprehensive caching strategy\",\n",
    "    \"Add monitoring and alerting for all components\",\n",
    "    \"Create fallback strategies for component failures\",\n",
    "    \"Implement user session and conversation management\",\n",
    "    \"Add evaluation framework with human feedback\",\n",
    "    \"Set up A/B testing for different strategies\",\n",
    "    \"Create security controls for sensitive queries\"\n",
    "]\n",
    "\n",
    "for i, item in enumerate(checklist, 1):\n",
    "    print(f\"   {i}. {item}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Next Steps for Enhancement:\")\n",
    "enhancements = [\n",
    "    \"Add multi-modal retrieval (text + images + code)\",\n",
    "    \"Implement knowledge graph integration\",\n",
    "    \"Build user personalization and preference learning\",\n",
    "    \"Add real-time learning from user feedback\",\n",
    "    \"Integrate with external tools and APIs\",\n",
    "    \"Implement advanced reasoning patterns\",\n",
    "    \"Add support for different domain expertise levels\",\n",
    "    \"Build collaborative filtering for query suggestions\"\n",
    "]\n",
    "\n",
    "for enhancement in enhancements:\n",
    "    print(f\"   ðŸ”® {enhancement}\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Key Insights:\")\n",
    "print(\"   â€¢ Agentic RAG significantly improves answer quality over traditional RAG\")\n",
    "print(\"   â€¢ Multi-step retrieval helps with complex and comparative queries\")\n",
    "print(\"   â€¢ Quality assessment enables self-correction and continuous improvement\")\n",
    "print(\"   â€¢ Proper caching and optimization are critical for production performance\")\n",
    "print(\"   â€¢ The system can adapt its strategy based on query characteristics\")\n",
    "\n",
    "print(f\"\\nâœ¨ Congratulations! You've built a complete agentic RAG system.\")\n",
    "print(f\"\\nðŸ“– This completes the 5-notebook Qdrant workshop series:\")\n",
    "print(f\"   1. Fundamentals & Search Basics âœ…\")\n",
    "print(f\"   2. Hybrid Search (Dense + Sparse) âœ…\")\n",
    "print(f\"   3. MMR Reranking âœ…\")\n",
    "print(f\"   4. HNSW Index Health âœ…\")\n",
    "print(f\"   5. Agentic RAG âœ…\")\n",
    "\n",
    "print(f\"\\nðŸŽ“ You now have comprehensive knowledge of:\")\n",
    "print(f\"   â€¢ Vector database fundamentals\")\n",
    "print(f\"   â€¢ Advanced search techniques\")\n",
    "print(f\"   â€¢ Production optimization strategies\")\n",
    "print(f\"   â€¢ Intelligent RAG architectures\")\n",
    "\n",
    "print(f\"\\nðŸŒŸ Ready to build amazing AI applications with Qdrant!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
