{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6ded1cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T03:43:54.945273Z",
     "iopub.status.busy": "2025-08-19T03:43:54.944783Z",
     "iopub.status.idle": "2025-08-19T03:43:54.953440Z",
     "shell.execute_reply": "2025-08-19T03:43:54.952669Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a196a09",
   "metadata": {},
   "source": [
    "# Notebook 5: Agentic RAG with Qdrant\n",
    "\n",
    "## üéØ Objectives\n",
    "\n",
    "In this notebook, you'll learn:\n",
    "- Building intelligent RAG agents that can reason about queries\n",
    "- Multi-step query planning and execution\n",
    "- Dynamic retrieval strategies based on query analysis\n",
    "- Tool use and function calling within RAG workflows\n",
    "- Self-correcting retrieval with quality assessment\n",
    "- Building conversational agents with memory\n",
    "- Advanced techniques: query rewriting, result fusion, and answer synthesis\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "- Understanding of RAG (Retrieval-Augmented Generation) concepts\n",
    "- Familiarity with LLM APIs (OpenAI, Anthropic, or local models)\n",
    "- Collections from previous notebooks for knowledge base\n",
    "- Basic understanding of agent frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7236717",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T03:43:54.958444Z",
     "iopub.status.busy": "2025-08-19T03:43:54.958031Z",
     "iopub.status.idle": "2025-08-19T03:43:54.968161Z",
     "shell.execute_reply": "2025-08-19T03:43:54.967684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîê Loaded environment from .env\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env for Jupyter\n",
    "try:\n",
    "    from dotenv import load_dotenv, find_dotenv\n",
    "    load_dotenv(find_dotenv(), override=False)\n",
    "    print(\"üîê Loaded environment from .env\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not load .env via python-dotenv: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b3e1c49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T03:43:54.970186Z",
     "iopub.status.busy": "2025-08-19T03:43:54.970036Z",
     "iopub.status.idle": "2025-08-19T03:43:56.361905Z",
     "shell.execute_reply": "2025-08-19T03:43:56.361636Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thierrydamiba/dsdojo/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß System Information:\n",
      "   Python: 3.9.6\n",
      "   ‚úÖ Qdrant Client: unknown\n",
      "   ‚úÖ NumPy: 2.0.2\n",
      "   ‚úÖ Pandas: 2.3.1\n",
      "   ‚úÖ Matplotlib: 3.9.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Scikit-learn: 1.6.1\n",
      "\n",
      "üîß Optional Dependencies:\n",
      "   ‚úÖ FastEmbed: 0.7.1\n",
      "   ‚úÖ OpenAI: 1.100.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Anthropic: 0.64.0\n",
      "\n",
      "üî¨ Environment: JupyterLab/Notebook detected\n",
      "\n",
      "ü§ñ Agentic RAG Workshop\n",
      "\n",
      "üîë Checking LLM API availability...\n",
      "‚úÖ OpenAI API key found\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "from utils import (\n",
    "    get_qdrant_client, create_sample_dataset, search_dense, \n",
    "    search_hybrid_fusion, mmr_rerank, print_system_info\n",
    ")\n",
    "\n",
    "from qdrant_client.models import Filter, FieldCondition, MatchValue, Range\n",
    "\n",
    "print_system_info()\n",
    "print(\"\\nü§ñ Agentic RAG Workshop\")\n",
    "\n",
    "# Check for LLM API availability\n",
    "print(\"\\nüîë Checking LLM API availability...\")\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "if openai_key:\n",
    "    print(\"‚úÖ OpenAI API key found\")\n",
    "    LLM_PROVIDER = \"openai\"\n",
    "elif anthropic_key:\n",
    "    print(\"‚úÖ Anthropic API key found\")\n",
    "    LLM_PROVIDER = \"anthropic\"\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No LLM API keys found. Using mock responses for demonstration.\")\n",
    "    LLM_PROVIDER = \"mock\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc0db784",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T03:43:56.363169Z",
     "iopub.status.busy": "2025-08-19T03:43:56.363007Z",
     "iopub.status.idle": "2025-08-19T03:43:56.392680Z",
     "shell.execute_reply": "2025-08-19T03:43:56.392384Z"
    }
   },
   "outputs": [],
   "source": [
    "# LLM clients\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Optional SDKs\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "except Exception:\n",
    "    OpenAI = None\n",
    "\n",
    "class LLMClient:\n",
    "    \"\"\"Unified interface for LLM providers (OpenAI).\"\"\"\n",
    "    def __init__(self, provider: str = \"openai\", model: str = \"gpt-4o-mini\"):\n",
    "        self.provider = provider\n",
    "        self.model = model\n",
    "        if provider == \"openai\":\n",
    "            if OpenAI is None:\n",
    "                raise RuntimeError(\"OpenAI SDK not installed. Please `pip install openai`.\")\n",
    "            self.client = OpenAI()\n",
    "        elif provider == \"mock\":\n",
    "            self.client = None\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported provider: {provider}\")\n",
    "\n",
    "    def chat_completion(self, messages: List[Dict[str, str]], temperature: float = 0.1) -> str:\n",
    "        if self.provider == \"openai\":\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "                temperature=temperature,\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        elif self.provider == \"mock\":\n",
    "            # Return a minimal valid JSON for analysis steps\n",
    "            return (\n",
    "                '{\"query_type\":\"simple_factual\",\"complexity\":\"medium\",\"requires_multi_step\":false,'\n",
    "                '\"key_entities\":[],\"intent\":\"answer the question\",\"search_strategy\":\"hybrid\",'\n",
    "                '\"expected_answer_type\":\"factual\",\"reasoning\":\"mock analysis\"}'\n",
    "            )\n",
    "        raise ValueError(f\"Unsupported provider: {self.provider}\")\n",
    "\n",
    "# Initialize LLM based on available keys\n",
    "if 'LLM_PROVIDER' in globals() and LLM_PROVIDER == 'openai':\n",
    "    llm = LLMClient(provider=\"openai\")\n",
    "else:\n",
    "    # Fallback to mock so the notebook can run without real API keys\n",
    "    llm = LLMClient(provider=\"mock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0dc232",
   "metadata": {},
   "source": [
    "## üì¶ Install Dependencies (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dce8032",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T03:43:56.394104Z",
     "iopub.status.busy": "2025-08-19T03:43:56.394023Z",
     "iopub.status.idle": "2025-08-19T03:43:56.395467Z",
     "shell.execute_reply": "2025-08-19T03:43:56.395265Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment if running in a fresh environment\n",
    "# !pip install openai anthropic tiktoken fastembed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0758999f",
   "metadata": {},
   "source": [
    "## üß† Agentic RAG Architecture\n",
    "\n",
    "Traditional RAG: Query ‚Üí Retrieve ‚Üí Generate  \n",
    "Agentic RAG: Query ‚Üí Plan ‚Üí Multi-step Retrieve ‚Üí Reason ‚Üí Generate ‚Üí Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3f3a924",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T03:43:56.396584Z",
     "iopub.status.busy": "2025-08-19T03:43:56.396506Z",
     "iopub.status.idle": "2025-08-19T03:43:56.401243Z",
     "shell.execute_reply": "2025-08-19T03:43:56.401012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Agentic RAG Architecture\n",
      "========================================\n",
      "\n",
      "üîÑ Traditional RAG Limitations:\n",
      "   ‚Ä¢ Single retrieval step\n",
      "   ‚Ä¢ No query understanding\n",
      "   ‚Ä¢ Fixed retrieval strategy\n",
      "   ‚Ä¢ No result validation\n",
      "   ‚Ä¢ Limited multi-turn capability\n",
      "\n",
      "üß† Agentic RAG Enhancements:\n",
      "   üéØ Query Analysis:\n",
      "      - Classify query type and complexity\n",
      "      - Identify required information types\n",
      "      - Plan multi-step retrieval strategy\n",
      "   üîç Intelligent Retrieval:\n",
      "      - Adaptive search strategies\n",
      "      - Dynamic filtering based on context\n",
      "      - Multi-hop information gathering\n",
      "   üîß Tool Integration:\n",
      "      - Query rewriting and expansion\n",
      "      - Result synthesis and validation\n",
      "      - External API integration\n",
      "   üí≠ Reasoning:\n",
      "      - Quality assessment of retrieved info\n",
      "      - Gap identification and follow-up queries\n",
      "      - Answer confidence scoring\n",
      "\n",
      "üîÑ Agentic RAG Workflow:\n",
      "\n",
      "    1. Query Analysis\n",
      "       ‚îú‚îÄ‚îÄ Intent classification\n",
      "       ‚îú‚îÄ‚îÄ Complexity assessment\n",
      "       ‚îî‚îÄ‚îÄ Strategy selection\n",
      "    \n",
      "    2. Retrieval Planning\n",
      "       ‚îú‚îÄ‚îÄ Multi-step breakdown\n",
      "       ‚îú‚îÄ‚îÄ Search strategy per step\n",
      "       ‚îî‚îÄ‚îÄ Quality thresholds\n",
      "    \n",
      "    3. Execution\n",
      "       ‚îú‚îÄ‚îÄ Retrieve ‚Üí Assess ‚Üí Refine\n",
      "       ‚îú‚îÄ‚îÄ Multi-hop follow-ups\n",
      "       ‚îî‚îÄ‚îÄ Result validation\n",
      "    \n",
      "    4. Synthesis\n",
      "       ‚îú‚îÄ‚îÄ Information integration\n",
      "       ‚îú‚îÄ‚îÄ Answer generation\n",
      "       ‚îî‚îÄ‚îÄ Confidence scoring\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "class QueryType(Enum):\n",
    "    \"\"\"Types of queries that require different retrieval strategies\"\"\"\n",
    "    SIMPLE_FACTUAL = \"simple_factual\"\n",
    "    COMPLEX_ANALYTICAL = \"complex_analytical\"\n",
    "    COMPARATIVE = \"comparative\"\n",
    "    MULTI_HOP = \"multi_hop\"\n",
    "    PROCEDURAL = \"procedural\"\n",
    "    CONVERSATIONAL = \"conversational\"\n",
    "\n",
    "@dataclass\n",
    "class RetrievalPlan:\n",
    "    \"\"\"Plan for multi-step retrieval\"\"\"\n",
    "    query_type: QueryType\n",
    "    steps: List[Dict[str, Any]]\n",
    "    search_strategy: str  # 'dense', 'sparse', 'hybrid', 'multi_vector'\n",
    "    filters: Optional[Dict] = None\n",
    "    rerank: bool = True\n",
    "    max_results: int = 10\n",
    "    confidence_threshold: float = 0.7\n",
    "\n",
    "@dataclass\n",
    "class AgentState:\n",
    "    \"\"\"Maintains agent conversation state\"\"\"\n",
    "    conversation_history: List[Dict[str, str]]\n",
    "    retrieved_context: List[Dict]\n",
    "    current_query: str\n",
    "    query_plan: Optional[RetrievalPlan] = None\n",
    "    metadata: Dict = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.metadata is None:\n",
    "            self.metadata = {}\n",
    "\n",
    "def explain_agentic_rag():\n",
    "    \"\"\"Explain the concepts behind agentic RAG\"\"\"\n",
    "    print(\"ü§ñ Agentic RAG Architecture\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    print(\"\\nüîÑ Traditional RAG Limitations:\")\n",
    "    print(\"   ‚Ä¢ Single retrieval step\")\n",
    "    print(\"   ‚Ä¢ No query understanding\")\n",
    "    print(\"   ‚Ä¢ Fixed retrieval strategy\")\n",
    "    print(\"   ‚Ä¢ No result validation\")\n",
    "    print(\"   ‚Ä¢ Limited multi-turn capability\")\n",
    "    \n",
    "    print(\"\\nüß† Agentic RAG Enhancements:\")\n",
    "    print(\"   üéØ Query Analysis:\")\n",
    "    print(\"      - Classify query type and complexity\")\n",
    "    print(\"      - Identify required information types\")\n",
    "    print(\"      - Plan multi-step retrieval strategy\")\n",
    "    \n",
    "    print(\"   üîç Intelligent Retrieval:\")\n",
    "    print(\"      - Adaptive search strategies\")\n",
    "    print(\"      - Dynamic filtering based on context\")\n",
    "    print(\"      - Multi-hop information gathering\")\n",
    "    \n",
    "    print(\"   üîß Tool Integration:\")\n",
    "    print(\"      - Query rewriting and expansion\")\n",
    "    print(\"      - Result synthesis and validation\")\n",
    "    print(\"      - External API integration\")\n",
    "    \n",
    "    print(\"   üí≠ Reasoning:\")\n",
    "    print(\"      - Quality assessment of retrieved info\")\n",
    "    print(\"      - Gap identification and follow-up queries\")\n",
    "    print(\"      - Answer confidence scoring\")\n",
    "    \n",
    "    print(\"\\nüîÑ Agentic RAG Workflow:\")\n",
    "    workflow = '''\n",
    "    1. Query Analysis\n",
    "       ‚îú‚îÄ‚îÄ Intent classification\n",
    "       ‚îú‚îÄ‚îÄ Complexity assessment\n",
    "       ‚îî‚îÄ‚îÄ Strategy selection\n",
    "    \n",
    "    2. Retrieval Planning\n",
    "       ‚îú‚îÄ‚îÄ Multi-step breakdown\n",
    "       ‚îú‚îÄ‚îÄ Search strategy per step\n",
    "       ‚îî‚îÄ‚îÄ Quality thresholds\n",
    "    \n",
    "    3. Execution\n",
    "       ‚îú‚îÄ‚îÄ Retrieve ‚Üí Assess ‚Üí Refine\n",
    "       ‚îú‚îÄ‚îÄ Multi-hop follow-ups\n",
    "       ‚îî‚îÄ‚îÄ Result validation\n",
    "    \n",
    "    4. Synthesis\n",
    "       ‚îú‚îÄ‚îÄ Information integration\n",
    "       ‚îú‚îÄ‚îÄ Answer generation\n",
    "       ‚îî‚îÄ‚îÄ Confidence scoring\n",
    "    '''\n",
    "    print(workflow)\n",
    "\n",
    "explain_agentic_rag()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ee9233",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Setup & LLM Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "286a45f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T03:43:56.402435Z",
     "iopub.status.busy": "2025-08-19T03:43:56.402353Z",
     "iopub.status.idle": "2025-08-19T03:43:57.912768Z",
     "shell.execute_reply": "2025-08-19T03:43:57.912001Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Using Qdrant Cloud cluster: https://a025094c-936b-4e1b-b947-67d686d20306.eu-central-1-0.aws.development-cloud.qdrant.io:6333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Using knowledge base: workshop_hybrid\n"
     ]
    }
   ],
   "source": [
    "# Knowledge Base Setup (independent)\n",
    "from utils import (\n",
    "    get_qdrant_client,\n",
    "    ensure_collection,\n",
    "    create_sample_dataset,\n",
    "    upsert_points_batch,\n",
    ")\n",
    "from qdrant_client.models import VectorParams, Distance\n",
    "import numpy as np\n",
    "\n",
    "DEFAULT_KNOWLEDGE_COLLECTION = \"agentic_rag_demo\"\n",
    "\n",
    "# Connect to Qdrant\n",
    "client = get_qdrant_client()\n",
    "\n",
    "# Discover existing collections\n",
    "try:\n",
    "    collections_response = client.get_collections()\n",
    "    existing_collections = [c.name for c in getattr(collections_response, \"collections\", [])]\n",
    "except Exception:\n",
    "    existing_collections = []\n",
    "\n",
    "# Prefer richer workshop collections if available\n",
    "preferred_order = [\"workshop_hybrid\", \"workshop_fundamentals\"]\n",
    "KNOWLEDGE_COLLECTION = next((name for name in preferred_order if name in existing_collections), None)\n",
    "\n",
    "# If no preferred collection exists, use default demo collection\n",
    "if KNOWLEDGE_COLLECTION is None:\n",
    "    KNOWLEDGE_COLLECTION = DEFAULT_KNOWLEDGE_COLLECTION\n",
    "\n",
    "# Determine if we need to populate\n",
    "needs_population = False\n",
    "if KNOWLEDGE_COLLECTION in existing_collections:\n",
    "    try:\n",
    "        info = client.get_collection(KNOWLEDGE_COLLECTION)\n",
    "        needs_population = int(getattr(info, \"points_count\", 0) or 0) == 0\n",
    "    except Exception:\n",
    "        needs_population = True\n",
    "else:\n",
    "    needs_population = True\n",
    "\n",
    "if needs_population:\n",
    "    print(f\"‚úÖ Creating and populating knowledge collection: {KNOWLEDGE_COLLECTION}\")\n",
    "    ensure_collection(\n",
    "        client=client,\n",
    "        collection_name=KNOWLEDGE_COLLECTION,\n",
    "        vector_config=VectorParams(size=384, distance=Distance.COSINE),\n",
    "        force_recreate=True,\n",
    "    )\n",
    "    df = create_sample_dataset(size=300, seed=123)\n",
    "    vectors = np.random.randn(len(df), 384)\n",
    "    vectors = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "    upsert_points_batch(\n",
    "        client=client,\n",
    "        collection_name=KNOWLEDGE_COLLECTION,\n",
    "        df=df,\n",
    "        vectors=vectors,\n",
    "        payload_cols=[\"text\", \"category\", \"lang\", \"timestamp\"],\n",
    "        batch_size=100,\n",
    "    )\n",
    "    print(f\"‚úÖ Added {len(df)} documents to {KNOWLEDGE_COLLECTION}\")\n",
    "else:\n",
    "    print(f\"üìö Using knowledge base: {KNOWLEDGE_COLLECTION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c28f61",
   "metadata": {},
   "source": [
    "## üîç Query Analysis Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "237847ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T03:43:57.917313Z",
     "iopub.status.busy": "2025-08-19T03:43:57.916831Z",
     "iopub.status.idle": "2025-08-19T03:44:08.364138Z",
     "shell.execute_reply": "2025-08-19T03:44:08.363217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Query Analysis Examples:\n",
      "==================================================\n",
      "\n",
      "üìù Query: 'What is vector search?'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Type: simple_factual\n",
      "   Complexity: low\n",
      "   Strategy: sparse\n",
      "   Multi-step: False\n",
      "   üí≠ The query is straightforward and seeks a definition or explanation of a specific...\n",
      "\n",
      "üìù Query: 'Compare HNSW vs IVF algorithms for large-scale vector retrieval'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Type: comparative\n",
      "   Complexity: medium\n",
      "   Strategy: sparse\n",
      "   Multi-step: False\n",
      "   üí≠ The query seeks a comparison between two specific algorithms, indicating a need ...\n",
      "\n",
      "üìù Query: 'How do I set up a production-ready Qdrant cluster with monitoring?'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Type: procedural\n",
      "   Complexity: medium\n",
      "   Strategy: hybrid\n",
      "   Multi-step: True\n",
      "   üí≠ The query seeks a procedural guide on setting up a specific technology (Qdrant) ...\n",
      "\n",
      "üìù Query: 'Why is my search performance degrading over time?'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Type: complex_analytical\n",
      "   Complexity: medium\n",
      "   Strategy: hybrid\n",
      "   Multi-step: True\n",
      "   üí≠ The query seeks to analyze a problem (degrading search performance) and requires...\n"
     ]
    }
   ],
   "source": [
    "class QueryAnalyzer:\n",
    "    \"\"\"Analyzes queries to determine appropriate retrieval strategy\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client: LLMClient):\n",
    "        self.llm = llm_client\n",
    "    \n",
    "    def analyze_query(self, query: str, conversation_history: List[Dict] = None) -> Dict:\n",
    "        \"\"\"Analyze query to determine type, complexity, and strategy\"\"\"\n",
    "        \n",
    "        analysis_prompt = self._build_analysis_prompt(query, conversation_history)\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a query analysis expert. Analyze queries and return structured JSON responses.\"},\n",
    "            {\"role\": \"user\", \"content\": analysis_prompt}\n",
    "        ]\n",
    "        \n",
    "        response = self.llm.chat_completion(messages)\n",
    "        \n",
    "        try:\n",
    "            analysis = json.loads(response)\n",
    "        except json.JSONDecodeError:\n",
    "            # Fallback analysis\n",
    "            analysis = self._fallback_analysis(query)\n",
    "        \n",
    "        # Enrich with rule-based analysis\n",
    "        analysis.update(self._rule_based_analysis(query))\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _build_analysis_prompt(self, query: str, history: List[Dict] = None) -> str:\n",
    "        \"\"\"Build prompt for query analysis\"\"\"\n",
    "        \n",
    "        context = \"\"\n",
    "        if history and len(history) > 0:\n",
    "            recent_context = history[-3:] if len(history) > 3 else history\n",
    "            context = \"\\n\\nConversation context:\\n\"\n",
    "            for turn in recent_context:\n",
    "                context += f\"User: {turn.get('user', '')}\\nAssistant: {turn.get('assistant', '')}\\n\"\n",
    "        \n",
    "        return f'''\n",
    "Analyze this query and return a JSON response with the following structure:\n",
    "{{\n",
    "    \"query_type\": \"simple_factual|complex_analytical|comparative|multi_hop|procedural|conversational\",\n",
    "    \"complexity\": \"low|medium|high\", \n",
    "    \"requires_multi_step\": true/false,\n",
    "    \"key_entities\": [\"entity1\", \"entity2\"],\n",
    "    \"intent\": \"what the user wants to achieve\",\n",
    "    \"search_strategy\": \"dense|sparse|hybrid\",\n",
    "    \"expected_answer_type\": \"factual|explanatory|comparative|step-by-step\",\n",
    "    \"reasoning\": \"brief explanation of the analysis\"\n",
    "}}\n",
    "\n",
    "Query: \"{query}\"{context}\n",
    "        '''.strip()\n",
    "    \n",
    "    def _rule_based_analysis(self, query: str) -> Dict:\n",
    "        \"\"\"Rule-based query analysis as fallback/enhancement\"\"\"\n",
    "        analysis = {}\n",
    "        \n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        # Detect comparative queries\n",
    "        comparative_words = ['vs', 'versus', 'compare', 'difference between', 'better than']\n",
    "        if any(word in query_lower for word in comparative_words):\n",
    "            analysis['is_comparative'] = True\n",
    "            analysis['suggested_strategy'] = 'hybrid'  # Good for finding diverse perspectives\n",
    "        \n",
    "        # Detect procedural queries\n",
    "        procedural_words = ['how to', 'step by step', 'guide', 'tutorial', 'instructions']\n",
    "        if any(word in query_lower for word in procedural_words):\n",
    "            analysis['is_procedural'] = True\n",
    "            analysis['needs_ordering'] = True\n",
    "        \n",
    "        # Detect technical complexity\n",
    "        technical_terms = ['algorithm', 'optimization', 'configuration', 'implementation']\n",
    "        if any(term in query_lower for term in technical_terms):\n",
    "            analysis['is_technical'] = True\n",
    "            analysis['preferred_sources'] = 'technical'\n",
    "        \n",
    "        # Estimate required information breadth\n",
    "        question_words = ['what', 'why', 'how', 'when', 'where', 'who']\n",
    "        question_count = sum(1 for word in question_words if word in query_lower)\n",
    "        \n",
    "        if question_count > 1 or len(query.split()) > 10:\n",
    "            analysis['is_complex'] = True\n",
    "            analysis['needs_multiple_sources'] = True\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _fallback_analysis(self, query: str) -> Dict:\n",
    "        \"\"\"Simple fallback analysis when LLM fails\"\"\"\n",
    "        return {\n",
    "            \"query_type\": \"simple_factual\",\n",
    "            \"complexity\": \"medium\",\n",
    "            \"requires_multi_step\": False,\n",
    "            \"search_strategy\": \"hybrid\",\n",
    "            \"reasoning\": \"Fallback analysis used due to LLM unavailability\"\n",
    "        }\n",
    "\n",
    "# Test query analyzer\n",
    "analyzer = QueryAnalyzer(llm)\n",
    "\n",
    "test_queries = [\n",
    "    \"What is vector search?\",\n",
    "    \"Compare HNSW vs IVF algorithms for large-scale vector retrieval\",\n",
    "    \"How do I set up a production-ready Qdrant cluster with monitoring?\",\n",
    "    \"Why is my search performance degrading over time?\"\n",
    "]\n",
    "\n",
    "print(\"üîç Query Analysis Examples:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nüìù Query: '{query}'\")\n",
    "    analysis = analyzer.analyze_query(query)\n",
    "    \n",
    "    print(f\"   Type: {analysis.get('query_type', 'unknown')}\")\n",
    "    print(f\"   Complexity: {analysis.get('complexity', 'unknown')}\")\n",
    "    print(f\"   Strategy: {analysis.get('search_strategy', 'unknown')}\")\n",
    "    print(f\"   Multi-step: {analysis.get('requires_multi_step', False)}\")\n",
    "    \n",
    "    if 'reasoning' in analysis:\n",
    "        print(f\"   üí≠ {analysis['reasoning'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615baf95",
   "metadata": {},
   "source": [
    "## üìã Retrieval Planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04df872f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T03:44:08.368638Z",
     "iopub.status.busy": "2025-08-19T03:44:08.368274Z",
     "iopub.status.idle": "2025-08-19T03:44:19.056180Z",
     "shell.execute_reply": "2025-08-19T03:44:19.054846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Retrieval Planning Examples:\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Query: 'What is vector search?'\n",
      "   üéØ Query Type: simple_factual\n",
      "   üîç Strategy: hybrid\n",
      "   üìä Max Results: 5\n",
      "   üîÑ Rerank: False\n",
      "   üìà Steps: 1\n",
      "     1. search: Direct search for factual information\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Query: 'Compare HNSW vs IVF algorithms for large-scale vector retrieval'\n",
      "   üéØ Query Type: comparative\n",
      "   üîç Strategy: hybrid\n",
      "   üìä Max Results: 12\n",
      "   üîÑ Rerank: True\n",
      "   üìà Steps: 4\n",
      "     1. search: Broad search for comparative information\n",
      "     2. rerank: Diversify results to cover both sides\n",
      "     3. search: Focused search for HNSW\n",
      "     4. search: Focused search for IVF\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Query: 'How do I set up a production-ready Qdrant cluster with monitoring?'\n",
      "   üéØ Query Type: procedural\n",
      "   üîç Strategy: sparse\n",
      "   üìä Max Results: 8\n",
      "   üîÑ Rerank: False\n",
      "   üìà Steps: 2\n",
      "     1. search: Search for procedural content\n",
      "     2. order_by_steps: Order results by logical step sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Query: 'Why is my search performance degrading over time?'\n",
      "   üéØ Query Type: complex_analytical\n",
      "   üîç Strategy: hybrid\n",
      "   üìä Max Results: 15\n",
      "   üîÑ Rerank: True\n",
      "   üìà Steps: 4\n",
      "     1. search: Initial broad search\n",
      "     2. analyze_gaps: Identify information gaps\n",
      "     3. rerank: Balance relevance and diversity\n",
      "     4. validate: Assess result quality and completeness\n"
     ]
    }
   ],
   "source": [
    "class RetrievalPlanner:\n",
    "    \"\"\"Plans multi-step retrieval strategies based on query analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client: LLMClient):\n",
    "        self.llm = llm_client\n",
    "    \n",
    "    def create_plan(self, query: str, analysis: Dict) -> RetrievalPlan:\n",
    "        \"\"\"Create a detailed retrieval plan based on query analysis\"\"\"\n",
    "        \n",
    "        # Determine query type from analysis\n",
    "        query_type_str = analysis.get('query_type', 'simple_factual')\n",
    "        try:\n",
    "            query_type = QueryType(query_type_str)\n",
    "        except ValueError:\n",
    "            query_type = QueryType.SIMPLE_FACTUAL\n",
    "        \n",
    "        # Create plan based on query type\n",
    "        if query_type == QueryType.SIMPLE_FACTUAL:\n",
    "            return self._plan_simple_factual(query, analysis)\n",
    "        elif query_type == QueryType.COMPARATIVE:\n",
    "            return self._plan_comparative(query, analysis)\n",
    "        elif query_type == QueryType.COMPLEX_ANALYTICAL:\n",
    "            return self._plan_complex_analytical(query, analysis)\n",
    "        elif query_type == QueryType.MULTI_HOP:\n",
    "            return self._plan_multi_hop(query, analysis)\n",
    "        elif query_type == QueryType.PROCEDURAL:\n",
    "            return self._plan_procedural(query, analysis)\n",
    "        else:\n",
    "            return self._plan_default(query, analysis)\n",
    "    \n",
    "    def _plan_simple_factual(self, query: str, analysis: Dict) -> RetrievalPlan:\n",
    "        \"\"\"Plan for simple factual queries\"\"\"\n",
    "        steps = [\n",
    "            {\n",
    "                \"action\": \"search\",\n",
    "                \"strategy\": \"hybrid\",\n",
    "                \"query\": query,\n",
    "                \"limit\": 5,\n",
    "                \"description\": \"Direct search for factual information\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        return RetrievalPlan(\n",
    "            query_type=QueryType.SIMPLE_FACTUAL,\n",
    "            steps=steps,\n",
    "            search_strategy=\"hybrid\",\n",
    "            rerank=False,  # Simple queries don't need reranking\n",
    "            max_results=5\n",
    "        )\n",
    "    \n",
    "    def _plan_comparative(self, query: str, analysis: Dict) -> RetrievalPlan:\n",
    "        \"\"\"Plan for comparative queries\"\"\"\n",
    "        \n",
    "        # Extract entities to compare (simplified)\n",
    "        entities = analysis.get('key_entities', [])\n",
    "        if not entities:\n",
    "            # Simple extraction from query\n",
    "            parts = query.lower().split(' vs ')\n",
    "            if len(parts) == 2:\n",
    "                entities = [part.strip() for part in parts]\n",
    "        \n",
    "        steps = [\n",
    "            {\n",
    "                \"action\": \"search\",\n",
    "                \"strategy\": \"hybrid\",\n",
    "                \"query\": query,\n",
    "                \"limit\": 15,\n",
    "                \"description\": \"Broad search for comparative information\"\n",
    "            },\n",
    "            {\n",
    "                \"action\": \"rerank\",\n",
    "                \"method\": \"mmr\",\n",
    "                \"lambda\": 0.3,  # Favor diversity for comparison\n",
    "                \"description\": \"Diversify results to cover both sides\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Add specific searches for each entity if identified\n",
    "        for entity in entities[:2]:  # Limit to 2 main entities\n",
    "            steps.append({\n",
    "                \"action\": \"search\",\n",
    "                \"strategy\": \"dense\",  # Focused search\n",
    "                \"query\": entity,\n",
    "                \"limit\": 3,\n",
    "                \"description\": f\"Focused search for {entity}\"\n",
    "            })\n",
    "        \n",
    "        return RetrievalPlan(\n",
    "            query_type=QueryType.COMPARATIVE,\n",
    "            steps=steps,\n",
    "            search_strategy=\"hybrid\",\n",
    "            rerank=True,\n",
    "            max_results=12\n",
    "        )\n",
    "    \n",
    "    def _plan_complex_analytical(self, query: str, analysis: Dict) -> RetrievalPlan:\n",
    "        \"\"\"Plan for complex analytical queries\"\"\"\n",
    "        steps = [\n",
    "            {\n",
    "                \"action\": \"search\",\n",
    "                \"strategy\": \"hybrid\",\n",
    "                \"query\": query,\n",
    "                \"limit\": 20,\n",
    "                \"description\": \"Initial broad search\"\n",
    "            },\n",
    "            {\n",
    "                \"action\": \"analyze_gaps\",\n",
    "                \"description\": \"Identify information gaps\"\n",
    "            },\n",
    "            {\n",
    "                \"action\": \"rerank\",\n",
    "                \"method\": \"mmr\",\n",
    "                \"lambda\": 0.5,\n",
    "                \"description\": \"Balance relevance and diversity\"\n",
    "            },\n",
    "            {\n",
    "                \"action\": \"validate\",\n",
    "                \"description\": \"Assess result quality and completeness\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        return RetrievalPlan(\n",
    "            query_type=QueryType.COMPLEX_ANALYTICAL,\n",
    "            steps=steps,\n",
    "            search_strategy=\"hybrid\",\n",
    "            rerank=True,\n",
    "            max_results=15,\n",
    "            confidence_threshold=0.8\n",
    "        )\n",
    "    \n",
    "    def _plan_procedural(self, query: str, analysis: Dict) -> RetrievalPlan:\n",
    "        \"\"\"Plan for procedural/how-to queries\"\"\"\n",
    "        steps = [\n",
    "            {\n",
    "                \"action\": \"search\",\n",
    "                \"strategy\": \"sparse\",  # Good for exact procedural terms\n",
    "                \"query\": query,\n",
    "                \"limit\": 10,\n",
    "                \"filter\": {\"category\": [\"howto\", \"guide\", \"tutorial\"]},\n",
    "                \"description\": \"Search for procedural content\"\n",
    "            },\n",
    "            {\n",
    "                \"action\": \"order_by_steps\",\n",
    "                \"description\": \"Order results by logical step sequence\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        return RetrievalPlan(\n",
    "            query_type=QueryType.PROCEDURAL,\n",
    "            steps=steps,\n",
    "            search_strategy=\"sparse\",\n",
    "            rerank=False,  # Order matters more than diversity\n",
    "            max_results=8\n",
    "        )\n",
    "    \n",
    "    def _plan_multi_hop(self, query: str, analysis: Dict) -> RetrievalPlan:\n",
    "        \"\"\"Plan for multi-hop reasoning queries\"\"\"\n",
    "        steps = [\n",
    "            {\n",
    "                \"action\": \"search\",\n",
    "                \"strategy\": \"hybrid\",\n",
    "                \"query\": query,\n",
    "                \"limit\": 10,\n",
    "                \"description\": \"Initial search\"\n",
    "            },\n",
    "            {\n",
    "                \"action\": \"extract_entities\",\n",
    "                \"description\": \"Extract entities for follow-up\"\n",
    "            },\n",
    "            {\n",
    "                \"action\": \"iterative_search\",\n",
    "                \"max_hops\": 2,\n",
    "                \"description\": \"Follow-up searches based on extracted entities\"\n",
    "            },\n",
    "            {\n",
    "                \"action\": \"synthesize\",\n",
    "                \"description\": \"Combine information from multiple hops\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        return RetrievalPlan(\n",
    "            query_type=QueryType.MULTI_HOP,\n",
    "            steps=steps,\n",
    "            search_strategy=\"hybrid\",\n",
    "            rerank=True,\n",
    "            max_results=15\n",
    "        )\n",
    "    \n",
    "    def _plan_default(self, query: str, analysis: Dict) -> RetrievalPlan:\n",
    "        \"\"\"Default plan for unclassified queries\"\"\"\n",
    "        return self._plan_simple_factual(query, analysis)\n",
    "\n",
    "# Test retrieval planner\n",
    "planner = RetrievalPlanner(llm)\n",
    "\n",
    "print(\"\\nüìã Retrieval Planning Examples:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for query in test_queries:\n",
    "    analysis = analyzer.analyze_query(query)\n",
    "    plan = planner.create_plan(query, analysis)\n",
    "    \n",
    "    print(f\"\\nüìù Query: '{query}'\")\n",
    "    print(f\"   üéØ Query Type: {plan.query_type.value}\")\n",
    "    print(f\"   üîç Strategy: {plan.search_strategy}\")\n",
    "    print(f\"   üìä Max Results: {plan.max_results}\")\n",
    "    print(f\"   üîÑ Rerank: {plan.rerank}\")\n",
    "    print(f\"   üìà Steps: {len(plan.steps)}\")\n",
    "    \n",
    "    for i, step in enumerate(plan.steps, 1):\n",
    "        print(f\"     {i}. {step['action']}: {step.get('description', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b9d8d3",
   "metadata": {},
   "source": [
    "## üîß Advanced Retrieval Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c973cd36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T03:44:19.061008Z",
     "iopub.status.busy": "2025-08-19T03:44:19.060678Z",
     "iopub.status.idle": "2025-08-19T03:44:22.979791Z",
     "shell.execute_reply": "2025-08-19T03:44:22.978767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Executing retrieval plan for: 'Compare HNSW vs IVF algorithms for large-scale vector retrieval'\n",
      "   Plan type: comparative\n",
      "   Steps: 4\n",
      "\n",
      "   üìç Step 1: search\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚úÖ Retrieved 15 results\n",
      "\n",
      "   üìç Step 2: rerank\n",
      "      ‚úÖ Retrieved 10 results\n",
      "\n",
      "   üìç Step 3: search\n",
      "      ‚ùå Search failed: Unexpected Response: 400 (Bad Request)\n",
      "Raw response content:\n",
      "b'{\"status\":{\"error\":\"Wrong input: Collection requires specified vector name in the request, available names: text_dense, text_sparse\"},\"time\":0.000040672}'\n",
      "      ‚ö†Ô∏è No results from this step\n",
      "\n",
      "   üìç Step 4: search\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚ùå Search failed: Unexpected Response: 400 (Bad Request)\n",
      "Raw response content:\n",
      "b'{\"status\":{\"error\":\"Wrong input: Collection requires specified vector name in the request, available names: text_dense, text_sparse\"},\"time\":0.000030333}'\n",
      "      ‚ö†Ô∏è No results from this step\n",
      "\n",
      "‚úÖ Plan execution complete: 12 final results\n",
      "\n",
      "üéØ Final Results Summary:\n",
      "1. [0.045] Guide: Data retention guidelines...\n",
      "   Strategy: hybrid\n",
      "   Reranked: mmr\n",
      "2. [0.038] Learn about data retention guidelines...\n",
      "   Strategy: hybrid\n",
      "   Reranked: mmr\n",
      "3. [0.038] Learn about data retention guidelines...\n",
      "   Strategy: hybrid\n",
      "   Reranked: mmr\n"
     ]
    }
   ],
   "source": [
    "class AdvancedRetriever:\n",
    "    \"\"\"Executes complex retrieval plans with multiple strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, client, collection_name: str, llm_client: LLMClient):\n",
    "        self.client = client\n",
    "        self.collection = collection_name\n",
    "        self.llm = llm_client\n",
    "        self.query_cache = {}  # Simple query caching\n",
    "    \n",
    "    def execute_plan(self, plan: RetrievalPlan, original_query: str) -> List[Dict]:\n",
    "        \"\"\"Execute a retrieval plan step by step\"\"\"\n",
    "        \n",
    "        print(f\"\\nüöÄ Executing retrieval plan for: '{original_query}'\")\n",
    "        print(f\"   Plan type: {plan.query_type.value}\")\n",
    "        print(f\"   Steps: {len(plan.steps)}\")\n",
    "        \n",
    "        results = []\n",
    "        context = {\"original_query\": original_query, \"intermediate_results\": []}\n",
    "        \n",
    "        for i, step in enumerate(plan.steps, 1):\n",
    "            print(f\"\\n   üìç Step {i}: {step['action']}\")\n",
    "            \n",
    "            step_results = self._execute_step(step, context)\n",
    "            \n",
    "            if step_results:\n",
    "                results.extend(step_results)\n",
    "                context[\"intermediate_results\"].extend(step_results)\n",
    "                print(f\"      ‚úÖ Retrieved {len(step_results)} results\")\n",
    "            else:\n",
    "                print(f\"      ‚ö†Ô∏è No results from this step\")\n",
    "        \n",
    "        # Final result processing\n",
    "        final_results = self._post_process_results(results, plan, original_query)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Plan execution complete: {len(final_results)} final results\")\n",
    "        return final_results\n",
    "    \n",
    "    def _execute_step(self, step: Dict, context: Dict) -> List[Dict]:\n",
    "        \"\"\"Execute a single step in the retrieval plan\"\"\"\n",
    "        \n",
    "        action = step.get('action', 'search')\n",
    "        \n",
    "        if action == 'search':\n",
    "            return self._execute_search(step, context)\n",
    "        elif action == 'rerank':\n",
    "            return self._execute_rerank(step, context)\n",
    "        elif action == 'filter':\n",
    "            return self._execute_filter(step, context)\n",
    "        elif action == 'analyze_gaps':\n",
    "            return self._analyze_gaps(step, context)\n",
    "        elif action == 'validate':\n",
    "            return self._validate_results(step, context)\n",
    "        elif action == 'extract_entities':\n",
    "            return self._extract_entities(step, context)\n",
    "        elif action == 'iterative_search':\n",
    "            return self._iterative_search(step, context)\n",
    "        else:\n",
    "            print(f\"      ‚ö†Ô∏è Unknown action: {action}\")\n",
    "            return []\n",
    "    \n",
    "    def _execute_search(self, step: Dict, context: Dict) -> List[Dict]:\n",
    "        \"\"\"Execute a search step\"\"\"\n",
    "        \n",
    "        query = step.get('query', context['original_query'])\n",
    "        strategy = step.get('strategy', 'hybrid')\n",
    "        limit = step.get('limit', 10)\n",
    "        \n",
    "        # Check cache first\n",
    "        cache_key = f\"{strategy}_{query}_{limit}\"\n",
    "        if cache_key in self.query_cache:\n",
    "            print(f\"      üíæ Using cached results\")\n",
    "            return self.query_cache[cache_key]\n",
    "        \n",
    "        # Create mock query vector for demonstration\n",
    "        np.random.seed(hash(query) % 2**32)\n",
    "        query_vector = np.random.randn(384)\n",
    "        query_vector = query_vector / np.linalg.norm(query_vector)\n",
    "        \n",
    "        # Apply filters if specified\n",
    "        filter_condition = None\n",
    "        if 'filter' in step:\n",
    "            filter_condition = self._build_filter(step['filter'])\n",
    "        \n",
    "        try:\n",
    "            if strategy == 'dense':\n",
    "                results = search_dense(\n",
    "                    client=self.client,\n",
    "                    collection_name=self.collection,\n",
    "                    query_vector=query_vector,\n",
    "                    limit=limit,\n",
    "                    filter_condition=filter_condition,\n",
    "                    with_payload=True\n",
    "                )\n",
    "            \n",
    "            elif strategy == 'hybrid':\n",
    "                # Create mock sparse vector\n",
    "                sparse_vector = {i: 1.0/len(query.split()) for i in range(len(query.split()[:10]))}\n",
    "                \n",
    "                results = search_hybrid_fusion(\n",
    "                    client=self.client,\n",
    "                    collection_name=self.collection,\n",
    "                    dense_vector=query_vector,\n",
    "                    sparse_vector=sparse_vector,\n",
    "                    dense_weight=0.6,\n",
    "                    limit=limit*2,\n",
    "                    final_limit=limit,\n",
    "                    filter_condition=filter_condition\n",
    "                )\n",
    "            \n",
    "            else:  # sparse or fallback\n",
    "                results = search_dense(\n",
    "                    client=self.client,\n",
    "                    collection_name=self.collection,\n",
    "                    query_vector=query_vector,\n",
    "                    limit=limit,\n",
    "                    filter_condition=filter_condition,\n",
    "                    with_payload=True\n",
    "                )\n",
    "            \n",
    "            # Convert to dict format\n",
    "            result_dicts = []\n",
    "            for result in results:\n",
    "                result_dict = {\n",
    "                    'id': result.id,\n",
    "                    'score': result.score,\n",
    "                    'content': result.payload.get('text', ''),\n",
    "                    'metadata': {\n",
    "                        'category': result.payload.get('category', 'unknown'),\n",
    "                        'lang': result.payload.get('lang', 'en'),\n",
    "                        'timestamp': result.payload.get('timestamp', 0)\n",
    "                    },\n",
    "                    'retrieval_info': {\n",
    "                        'strategy': strategy,\n",
    "                        'step': step.get('description', 'search'),\n",
    "                        'query': query\n",
    "                    }\n",
    "                }\n",
    "                result_dicts.append(result_dict)\n",
    "            \n",
    "            # Cache results\n",
    "            self.query_cache[cache_key] = result_dicts\n",
    "            \n",
    "            return result_dicts\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"      ‚ùå Search failed: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _execute_rerank(self, step: Dict, context: Dict) -> List[Dict]:\n",
    "        \"\"\"Execute MMR reranking on existing results\"\"\"\n",
    "        \n",
    "        if not context.get('intermediate_results'):\n",
    "            print(f\"      ‚ö†Ô∏è No results to rerank\")\n",
    "            return []\n",
    "        \n",
    "        method = step.get('method', 'mmr')\n",
    "        lambda_param = step.get('lambda', 0.5)\n",
    "        k = step.get('k', 10)\n",
    "        \n",
    "        if method == 'mmr':\n",
    "            # Convert back to result objects for MMR\n",
    "            results = context['intermediate_results'][-20:]  # Use recent results\n",
    "            \n",
    "            # Create mock vectors for MMR (in practice, you'd retrieve actual vectors)\n",
    "            candidate_vectors = []\n",
    "            for result in results:\n",
    "                np.random.seed(result['id'])  # Consistent vector per ID\n",
    "                vec = np.random.randn(384)\n",
    "                candidate_vectors.append(vec / np.linalg.norm(vec))\n",
    "            \n",
    "            candidate_vectors = np.array(candidate_vectors)\n",
    "            \n",
    "            # Mock query vector\n",
    "            query = context['original_query']\n",
    "            np.random.seed(hash(query) % 2**32)\n",
    "            query_vector = np.random.randn(384)\n",
    "            query_vector = query_vector / np.linalg.norm(query_vector)\n",
    "            \n",
    "            # Apply MMR reranking (simplified mock)\n",
    "            selected_indices = list(range(min(k, len(results))))\n",
    "            np.random.shuffle(selected_indices)  # Mock MMR selection\n",
    "            \n",
    "            reranked_results = [results[i] for i in selected_indices]\n",
    "            \n",
    "            # Update metadata\n",
    "            for result in reranked_results:\n",
    "                result['retrieval_info']['reranked'] = True\n",
    "                result['retrieval_info']['rerank_method'] = 'mmr'\n",
    "                result['retrieval_info']['lambda'] = lambda_param\n",
    "            \n",
    "            return reranked_results\n",
    "        \n",
    "        return context.get('intermediate_results', [])\n",
    "    \n",
    "    def _analyze_gaps(self, step: Dict, context: Dict) -> List[Dict]:\n",
    "        \"\"\"Analyze information gaps in current results\"\"\"\n",
    "        \n",
    "        results = context.get('intermediate_results', [])\n",
    "        if not results:\n",
    "            return []\n",
    "        \n",
    "        # Simple gap analysis: check topic coverage\n",
    "        original_query = context['original_query']\n",
    "        query_terms = set(original_query.lower().split())\n",
    "        \n",
    "        covered_terms = set()\n",
    "        for result in results:\n",
    "            content_terms = set(result['content'].lower().split())\n",
    "            covered_terms.update(content_terms.intersection(query_terms))\n",
    "        \n",
    "        missing_terms = query_terms - covered_terms\n",
    "        \n",
    "        if missing_terms:\n",
    "            print(f\"      üìä Gap analysis: Missing terms {missing_terms}\")\n",
    "            # Could trigger additional searches here\n",
    "        else:\n",
    "            print(f\"      ‚úÖ Good coverage of query terms\")\n",
    "        \n",
    "        return []\n",
    "    \n",
    "    def _validate_results(self, step: Dict, context: Dict) -> List[Dict]:\n",
    "        \"\"\"Validate result quality and completeness\"\"\"\n",
    "        \n",
    "        results = context.get('intermediate_results', [])\n",
    "        if not results:\n",
    "            return []\n",
    "        \n",
    "        # Simple validation metrics\n",
    "        avg_score = np.mean([r['score'] for r in results]) if results else 0\n",
    "        score_std = np.std([r['score'] for r in results]) if results else 0\n",
    "        \n",
    "        # Content diversity check\n",
    "        categories = set(r['metadata']['category'] for r in results)\n",
    "        \n",
    "        print(f\"      üìä Validation - Avg score: {avg_score:.3f}, Diversity: {len(categories)} categories\")\n",
    "        \n",
    "        # Mark high-quality results\n",
    "        threshold = avg_score + 0.1\n",
    "        for result in results:\n",
    "            if result['score'] >= threshold:\n",
    "                result['retrieval_info']['high_quality'] = True\n",
    "        \n",
    "        return []\n",
    "    \n",
    "    def _extract_entities(self, step: Dict, context: Dict) -> List[Dict]:\n",
    "        \"\"\"Extract entities for multi-hop reasoning\"\"\"\n",
    "        \n",
    "        results = context.get('intermediate_results', [])\n",
    "        \n",
    "        # Simple entity extraction (in practice, use NER)\n",
    "        entities = set()\n",
    "        for result in results[:5]:  # Top results only\n",
    "            content = result['content']\n",
    "            # Mock entity extraction\n",
    "            words = content.split()\n",
    "            # Extract capitalized words as mock entities\n",
    "            for word in words:\n",
    "                if word.istitle() and len(word) > 3:\n",
    "                    entities.add(word)\n",
    "        \n",
    "        context['extracted_entities'] = list(entities)[:5]  # Limit entities\n",
    "        print(f\"      üîç Extracted entities: {context['extracted_entities']}\")\n",
    "        \n",
    "        return []\n",
    "    \n",
    "    def _iterative_search(self, step: Dict, context: Dict) -> List[Dict]:\n",
    "        \"\"\"Perform iterative search based on extracted entities\"\"\"\n",
    "        \n",
    "        entities = context.get('extracted_entities', [])\n",
    "        if not entities:\n",
    "            return []\n",
    "        \n",
    "        additional_results = []\n",
    "        max_hops = step.get('max_hops', 2)\n",
    "        \n",
    "        for i, entity in enumerate(entities[:max_hops]):\n",
    "            print(f\"      üîç Hop {i+1}: Searching for '{entity}'\")\n",
    "            \n",
    "            entity_step = {\n",
    "                'action': 'search',\n",
    "                'query': entity,\n",
    "                'strategy': 'dense',\n",
    "                'limit': 3\n",
    "            }\n",
    "            \n",
    "            entity_results = self._execute_search(entity_step, context)\n",
    "            for result in entity_results:\n",
    "                result['retrieval_info']['hop'] = i + 1\n",
    "                result['retrieval_info']['hop_entity'] = entity\n",
    "            \n",
    "            additional_results.extend(entity_results)\n",
    "        \n",
    "        return additional_results\n",
    "    \n",
    "    def _build_filter(self, filter_spec: Dict) -> Filter:\n",
    "        \"\"\"Build Qdrant filter from specification\"\"\"\n",
    "        \n",
    "        conditions = []\n",
    "        \n",
    "        for field, values in filter_spec.items():\n",
    "            if isinstance(values, list):\n",
    "                # Multiple values - use should (OR)\n",
    "                for value in values:\n",
    "                    conditions.append(\n",
    "                        FieldCondition(key=field, match=MatchValue(value=value))\n",
    "                    )\n",
    "            else:\n",
    "                # Single value\n",
    "                conditions.append(\n",
    "                    FieldCondition(key=field, match=MatchValue(value=values))\n",
    "                )\n",
    "        \n",
    "        return Filter(should=conditions) if len(conditions) > 1 else Filter(must=conditions)\n",
    "    \n",
    "    def _post_process_results(self, results: List[Dict], plan: RetrievalPlan, query: str) -> List[Dict]:\n",
    "        \"\"\"Final post-processing of all results\"\"\"\n",
    "        \n",
    "        # Remove duplicates\n",
    "        seen_ids = set()\n",
    "        unique_results = []\n",
    "        \n",
    "        for result in results:\n",
    "            if result['id'] not in seen_ids:\n",
    "                seen_ids.add(result['id'])\n",
    "                unique_results.append(result)\n",
    "        \n",
    "        # Sort by score (descending)\n",
    "        unique_results.sort(key=lambda x: x['score'], reverse=True)\n",
    "        \n",
    "        # Limit to max_results\n",
    "        final_results = unique_results[:plan.max_results]\n",
    "        \n",
    "        # Add final metadata\n",
    "        for i, result in enumerate(final_results):\n",
    "            result['final_rank'] = i + 1\n",
    "            result['retrieval_info']['plan_type'] = plan.query_type.value\n",
    "        \n",
    "        return final_results\n",
    "\n",
    "# Test advanced retriever\n",
    "if KNOWLEDGE_COLLECTION:\n",
    "    retriever = AdvancedRetriever(client, KNOWLEDGE_COLLECTION, llm)\n",
    "    \n",
    "    # Test with a complex query\n",
    "    test_query = \"Compare HNSW vs IVF algorithms for large-scale vector retrieval\"\n",
    "    analysis = analyzer.analyze_query(test_query)\n",
    "    plan = planner.create_plan(test_query, analysis)\n",
    "    \n",
    "    results = retriever.execute_plan(plan, test_query)\n",
    "    \n",
    "    print(f\"\\nüéØ Final Results Summary:\")\n",
    "    for i, result in enumerate(results[:3], 1):\n",
    "        print(f\"{i}. [{result['score']:.3f}] {result['content'][:60]}...\")\n",
    "        print(f\"   Strategy: {result['retrieval_info']['strategy']}\")\n",
    "        if 'reranked' in result['retrieval_info']:\n",
    "            print(f\"   Reranked: {result['retrieval_info']['rerank_method']}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping retriever test - no knowledge collection available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91a5695",
   "metadata": {},
   "source": [
    "## üßÆ Quality Assessment Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9e88f98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T03:44:22.984868Z",
     "iopub.status.busy": "2025-08-19T03:44:22.984263Z",
     "iopub.status.idle": "2025-08-19T03:44:25.785526Z",
     "shell.execute_reply": "2025-08-19T03:44:25.784425Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Quality Assessment for: 'Compare HNSW vs IVF algorithms for large-scale vector retrieval'\n",
      "==================================================\n",
      "Overall Quality: 0.345\n",
      "\n",
      "üìà Detailed Metrics:\n",
      "   Relevance: 0.018\n",
      "   Completeness: 1.000\n",
      "   Diversity: 0.188\n",
      "   Coverage: 0.000\n",
      "\n",
      "üí° Suggestions:\n",
      "   1. Consider query rewriting or expansion for better relevance\n",
      "   2. Apply MMR reranking with lower lambda (0.3-0.4) for more diversity\n",
      "   3. Expand search to include more content categories\n",
      "   4. Low similarity scores - consider query reformulation\n",
      "\n",
      "üß† LLM Recommendation: Results appear relevant but detailed assessment unavailable\n"
     ]
    }
   ],
   "source": [
    "class QualityAssessor:\n",
    "    \"\"\"Assesses retrieval quality and suggests improvements\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_client: LLMClient):\n",
    "        self.llm = llm_client\n",
    "    \n",
    "    def assess_results(self, query: str, results: List[Dict], \n",
    "                      plan: RetrievalPlan) -> Dict[str, Any]:\n",
    "        \"\"\"Assess the quality of retrieval results\"\"\"\n",
    "        \n",
    "        if not results:\n",
    "            return {\n",
    "                \"overall_quality\": 0.0,\n",
    "                \"completeness\": 0.0,\n",
    "                \"relevance\": 0.0,\n",
    "                \"diversity\": 0.0,\n",
    "                \"needs_improvement\": True,\n",
    "                \"suggestions\": [\"No results found - revise search strategy\"]\n",
    "            }\n",
    "        \n",
    "        # Quantitative metrics\n",
    "        relevance_score = self._assess_relevance(query, results)\n",
    "        completeness_score = self._assess_completeness(query, results, plan)\n",
    "        diversity_score = self._assess_diversity(results)\n",
    "        coverage_score = self._assess_coverage(query, results)\n",
    "        \n",
    "        # Overall quality (weighted average)\n",
    "        overall_quality = (\n",
    "            0.4 * relevance_score +\n",
    "            0.3 * completeness_score +\n",
    "            0.2 * diversity_score +\n",
    "            0.1 * coverage_score\n",
    "        )\n",
    "        \n",
    "        # LLM-based assessment for qualitative insights\n",
    "        llm_assessment = self._llm_quality_assessment(query, results)\n",
    "        \n",
    "        # Generate suggestions\n",
    "        suggestions = self._generate_suggestions(\n",
    "            query, results, plan, \n",
    "            relevance_score, completeness_score, diversity_score\n",
    "        )\n",
    "        \n",
    "        assessment = {\n",
    "            \"overall_quality\": overall_quality,\n",
    "            \"metrics\": {\n",
    "                \"relevance\": relevance_score,\n",
    "                \"completeness\": completeness_score,\n",
    "                \"diversity\": diversity_score,\n",
    "                \"coverage\": coverage_score\n",
    "            },\n",
    "            \"llm_insights\": llm_assessment,\n",
    "            \"needs_improvement\": overall_quality < 0.7,\n",
    "            \"suggestions\": suggestions,\n",
    "            \"result_count\": len(results),\n",
    "            \"avg_score\": np.mean([r['score'] for r in results]) if results else 0\n",
    "        }\n",
    "        \n",
    "        return assessment\n",
    "    \n",
    "    def _assess_relevance(self, query: str, results: List[Dict]) -> float:\n",
    "        \"\"\"Assess relevance using score distribution and content matching\"\"\"\n",
    "        \n",
    "        if not results:\n",
    "            return 0.0\n",
    "        \n",
    "        # Use similarity scores as primary relevance indicator\n",
    "        scores = [r['score'] for r in results]\n",
    "        avg_score = np.mean(scores)\n",
    "        \n",
    "        # Content relevance (simplified keyword matching)\n",
    "        query_terms = set(query.lower().split())\n",
    "        content_relevance = []\n",
    "        \n",
    "        for result in results:\n",
    "            content_terms = set(result['content'].lower().split())\n",
    "            overlap = len(query_terms.intersection(content_terms))\n",
    "            relevance = overlap / len(query_terms) if query_terms else 0\n",
    "            content_relevance.append(relevance)\n",
    "        \n",
    "        content_score = np.mean(content_relevance)\n",
    "        \n",
    "        # Combine similarity and content relevance\n",
    "        return 0.7 * avg_score + 0.3 * content_score\n",
    "    \n",
    "    def _assess_completeness(self, query: str, results: List[Dict], \n",
    "                           plan: RetrievalPlan) -> float:\n",
    "        \"\"\"Assess completeness based on query type and result count\"\"\"\n",
    "        \n",
    "        result_count = len(results)\n",
    "        expected_count = plan.max_results\n",
    "        \n",
    "        # Base completeness on result count ratio\n",
    "        count_score = min(1.0, result_count / expected_count)\n",
    "        \n",
    "        # Adjust based on query type\n",
    "        if plan.query_type == QueryType.COMPARATIVE:\n",
    "            # Comparative queries should have diverse sources\n",
    "            categories = set(r['metadata']['category'] for r in results)\n",
    "            diversity_bonus = min(0.3, len(categories) * 0.1)\n",
    "            count_score += diversity_bonus\n",
    "        \n",
    "        elif plan.query_type == QueryType.MULTI_HOP:\n",
    "            # Multi-hop queries should have results from different hops\n",
    "            hops = set(r['retrieval_info'].get('hop', 0) for r in results)\n",
    "            hop_bonus = min(0.2, len(hops) * 0.1)\n",
    "            count_score += hop_bonus\n",
    "        \n",
    "        return min(1.0, count_score)\n",
    "    \n",
    "    def _assess_diversity(self, results: List[Dict]) -> float:\n",
    "        \"\"\"Assess diversity of results\"\"\"\n",
    "        \n",
    "        if len(results) <= 1:\n",
    "            return 1.0 if len(results) == 1 else 0.0\n",
    "        \n",
    "        # Category diversity\n",
    "        categories = [r['metadata']['category'] for r in results]\n",
    "        unique_categories = len(set(categories))\n",
    "        category_diversity = unique_categories / len(categories)\n",
    "        \n",
    "        # Content diversity (simplified using length variation)\n",
    "        content_lengths = [len(r['content']) for r in results]\n",
    "        length_std = np.std(content_lengths)\n",
    "        length_diversity = min(1.0, length_std / 100)  # Normalize\n",
    "        \n",
    "        # Score diversity\n",
    "        scores = [r['score'] for r in results]\n",
    "        score_std = np.std(scores)\n",
    "        score_diversity = min(1.0, score_std * 2)  # Boost diversity score\n",
    "        \n",
    "        return 0.5 * category_diversity + 0.3 * length_diversity + 0.2 * score_diversity\n",
    "    \n",
    "    def _assess_coverage(self, query: str, results: List[Dict]) -> float:\n",
    "        \"\"\"Assess how well results cover the query topics\"\"\"\n",
    "        \n",
    "        if not results:\n",
    "            return 0.0\n",
    "        \n",
    "        query_terms = set(query.lower().split())\n",
    "        covered_terms = set()\n",
    "        \n",
    "        for result in results:\n",
    "            content_terms = set(result['content'].lower().split())\n",
    "            covered_terms.update(content_terms.intersection(query_terms))\n",
    "        \n",
    "        coverage = len(covered_terms) / len(query_terms) if query_terms else 1.0\n",
    "        return coverage\n",
    "    \n",
    "    def _llm_quality_assessment(self, query: str, results: List[Dict]) -> Dict:\n",
    "        \"\"\"Get qualitative assessment from LLM\"\"\"\n",
    "        \n",
    "        # Prepare content sample for LLM\n",
    "        content_sample = \"\\n\\n\".join([\n",
    "            f\"Result {i+1}: {r['content'][:100]}...\"\n",
    "            for i, r in enumerate(results[:3])\n",
    "        ])\n",
    "        \n",
    "        prompt = f'''\n",
    "Assess the quality of these search results for the query: \"{query}\"\n",
    "\n",
    "Results:\n",
    "{content_sample}\n",
    "\n",
    "Return a JSON assessment with:\n",
    "{{\n",
    "    \"quality_score\": 0.0-1.0,\n",
    "    \"strengths\": [\"list of strengths\"],\n",
    "    \"weaknesses\": [\"list of weaknesses\"],\n",
    "    \"missing_topics\": [\"topics not covered\"],\n",
    "    \"recommendation\": \"overall recommendation\"\n",
    "}}\n",
    "        '''.strip()\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert at evaluating search result quality.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.chat_completion(messages)\n",
    "            return json.loads(response)\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"quality_score\": 0.7,\n",
    "                \"strengths\": [\"Results retrieved successfully\"],\n",
    "                \"weaknesses\": [\"LLM assessment unavailable\"],\n",
    "                \"missing_topics\": [],\n",
    "                \"recommendation\": \"Results appear relevant but detailed assessment unavailable\"\n",
    "            }\n",
    "    \n",
    "    def _generate_suggestions(self, query: str, results: List[Dict], \n",
    "                            plan: RetrievalPlan, relevance: float, \n",
    "                            completeness: float, diversity: float) -> List[str]:\n",
    "        \"\"\"Generate improvement suggestions\"\"\"\n",
    "        \n",
    "        suggestions = []\n",
    "        \n",
    "        # Relevance suggestions\n",
    "        if relevance < 0.6:\n",
    "            suggestions.append(\"Consider query rewriting or expansion for better relevance\")\n",
    "            if plan.search_strategy == \"dense\":\n",
    "                suggestions.append(\"Try hybrid search to capture keyword matches\")\n",
    "        \n",
    "        # Completeness suggestions\n",
    "        if completeness < 0.7:\n",
    "            suggestions.append(\"Increase search limit or reduce filters to get more results\")\n",
    "            if plan.query_type == QueryType.COMPLEX_ANALYTICAL:\n",
    "                suggestions.append(\"Consider multi-step retrieval with follow-up queries\")\n",
    "        \n",
    "        # Diversity suggestions\n",
    "        if diversity < 0.5:\n",
    "            suggestions.append(\"Apply MMR reranking with lower lambda (0.3-0.4) for more diversity\")\n",
    "            suggestions.append(\"Expand search to include more content categories\")\n",
    "        \n",
    "        # Query-type specific suggestions\n",
    "        if plan.query_type == QueryType.COMPARATIVE and len(results) < 6:\n",
    "            suggestions.append(\"Comparative queries benefit from more diverse results - increase limit\")\n",
    "        \n",
    "        if plan.query_type == QueryType.PROCEDURAL:\n",
    "            suggestions.append(\"For procedural queries, prioritize step-by-step and tutorial content\")\n",
    "        \n",
    "        # Result quality suggestions\n",
    "        if results and np.mean([r['score'] for r in results]) < 0.5:\n",
    "            suggestions.append(\"Low similarity scores - consider query reformulation\")\n",
    "        \n",
    "        if len(results) == 0:\n",
    "            suggestions.append(\"No results found - broaden search terms or reduce filters\")\n",
    "        \n",
    "        return suggestions if suggestions else [\"Results quality is acceptable\"]\n",
    "\n",
    "# Test quality assessor\n",
    "assessor = QualityAssessor(llm)\n",
    "\n",
    "if 'results' in locals() and results:\n",
    "    assessment = assessor.assess_results(test_query, results, plan)\n",
    "    \n",
    "    print(f\"\\nüìä Quality Assessment for: '{test_query}'\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Overall Quality: {assessment['overall_quality']:.3f}\")\n",
    "    print(f\"\\nüìà Detailed Metrics:\")\n",
    "    for metric, score in assessment['metrics'].items():\n",
    "        print(f\"   {metric.capitalize()}: {score:.3f}\")\n",
    "    \n",
    "    print(f\"\\nüí° Suggestions:\")\n",
    "    for i, suggestion in enumerate(assessment['suggestions'], 1):\n",
    "        print(f\"   {i}. {suggestion}\")\n",
    "    \n",
    "    if assessment['llm_insights'].get('recommendation'):\n",
    "        print(f\"\\nüß† LLM Recommendation: {assessment['llm_insights']['recommendation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865441c8",
   "metadata": {},
   "source": [
    "## ü§ñ Complete Agentic RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a33023b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T03:44:25.790696Z",
     "iopub.status.busy": "2025-08-19T03:44:25.790201Z",
     "iopub.status.idle": "2025-08-19T03:44:43.682380Z",
     "shell.execute_reply": "2025-08-19T03:44:43.681401Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Agentic RAG Processing: 'What is vector search and how does it work?'\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£ Query Analysis\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Query type: complex_analytical\n",
      "   Complexity: medium\n",
      "   Multi-step: True\n",
      "\n",
      "2Ô∏è‚É£ Retrieval Planning\n",
      "   Strategy: hybrid\n",
      "   Steps: 4\n",
      "   Max results: 15\n",
      "\n",
      "3Ô∏è‚É£ Retrieval Execution\n",
      "\n",
      "   Iteration 1:\n",
      "\n",
      "üöÄ Executing retrieval plan for: 'What is vector search and how does it work?'\n",
      "   Plan type: complex_analytical\n",
      "   Steps: 4\n",
      "\n",
      "   üìç Step 1: search\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚úÖ Retrieved 20 results\n",
      "\n",
      "   üìç Step 2: analyze_gaps\n",
      "      üìä Gap analysis: Missing terms {'what', 'work?', 'is', 'how', 'does', 'it', 'vector'}\n",
      "      ‚ö†Ô∏è No results from this step\n",
      "\n",
      "   üìç Step 3: rerank\n",
      "      ‚úÖ Retrieved 10 results\n",
      "\n",
      "   üìç Step 4: validate\n",
      "      üìä Validation - Avg score: 0.058, Diversity: 5 categories\n",
      "      ‚ö†Ô∏è No results from this step\n",
      "\n",
      "‚úÖ Plan execution complete: 15 final results\n",
      "\n",
      "4Ô∏è‚É£ Quality Assessment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Quality score: 0.381\n",
      "   Needs improvement: True\n",
      "\n",
      "5Ô∏è‚É£ Plan Refinement\n",
      "\n",
      "   Iteration 2:\n",
      "\n",
      "üöÄ Executing retrieval plan for: 'What is vector search and how does it work?'\n",
      "   Plan type: complex_analytical\n",
      "   Steps: 1\n",
      "\n",
      "   üìç Step 1: rerank\n",
      "      ‚ö†Ô∏è No results to rerank\n",
      "      ‚ö†Ô∏è No results from this step\n",
      "\n",
      "‚úÖ Plan execution complete: 0 final results\n",
      "   ‚ö†Ô∏è No results - trying fallback strategy\n",
      "\n",
      "üöÄ Executing retrieval plan for: 'What is vector search and how does it work?'\n",
      "   Plan type: simple_factual\n",
      "   Steps: 1\n",
      "\n",
      "   üìç Step 1: search\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ‚úÖ Retrieved 10 results\n",
      "\n",
      "‚úÖ Plan execution complete: 10 final results\n",
      "\n",
      "4Ô∏è‚É£ Quality Assessment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Quality score: 0.388\n",
      "   Needs improvement: True\n",
      "\n",
      "5Ô∏è‚É£ Plan Refinement\n",
      "\n",
      "   Iteration 3:\n",
      "\n",
      "üöÄ Executing retrieval plan for: 'What is vector search and how does it work?'\n",
      "   Plan type: simple_factual\n",
      "   Steps: 1\n",
      "\n",
      "   üìç Step 1: rerank\n",
      "      ‚ö†Ô∏è No results to rerank\n",
      "      ‚ö†Ô∏è No results from this step\n",
      "\n",
      "‚úÖ Plan execution complete: 0 final results\n",
      "   ‚ö†Ô∏è No results - trying fallback strategy\n",
      "\n",
      "üöÄ Executing retrieval plan for: 'What is vector search and how does it work?'\n",
      "   Plan type: simple_factual\n",
      "   Steps: 1\n",
      "\n",
      "   üìç Step 1: search\n",
      "      üíæ Using cached results\n",
      "      ‚úÖ Retrieved 10 results\n",
      "\n",
      "‚úÖ Plan execution complete: 10 final results\n",
      "\n",
      "4Ô∏è‚É£ Quality Assessment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Quality score: 0.388\n",
      "   Needs improvement: True\n",
      "\n",
      "6Ô∏è‚É£ Answer Generation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Final Response:\n",
      "Answer: Vector search is a method used in information retrieval that involves representing data as vectors in a high-dimensional space. This technique is particularly useful in the context of machine learning...\n",
      "\n",
      "üìä Metadata:\n",
      "   query_type: complex_analytical\n",
      "   iterations: 3\n",
      "   total_time: 17.86768674850464\n",
      "   result_count: 10\n",
      "   quality_score: 0.3877095946191332\n"
     ]
    }
   ],
   "source": [
    "class AgenticRAGAgent:\n",
    "    \"\"\"Complete agentic RAG system that combines all components\"\"\"\n",
    "    \n",
    "    def __init__(self, client, collection_name: str, llm_client: LLMClient):\n",
    "        self.client = client\n",
    "        self.collection = collection_name\n",
    "        self.llm = llm_client\n",
    "        \n",
    "        # Initialize components\n",
    "        self.analyzer = QueryAnalyzer(llm_client)\n",
    "        self.planner = RetrievalPlanner(llm_client)\n",
    "        self.retriever = AdvancedRetriever(client, collection_name, llm_client)\n",
    "        self.assessor = QualityAssessor(llm_client)\n",
    "        \n",
    "        # Agent state\n",
    "        self.state = AgentState(\n",
    "            conversation_history=[],\n",
    "            retrieved_context=[],\n",
    "            current_query=\"\"\n",
    "        )\n",
    "        \n",
    "        self.max_iterations = 3  # Prevent infinite loops\n",
    "    \n",
    "    def query(self, user_query: str, context: Dict = None) -> Dict[str, Any]:\n",
    "        \"\"\"Process a user query through the complete agentic RAG pipeline\"\"\"\n",
    "        \n",
    "        print(f\"\\nü§ñ Agentic RAG Processing: '{user_query}'\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        self.state.current_query = user_query\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Phase 1: Query Analysis\n",
    "        print(f\"\\n1Ô∏è‚É£ Query Analysis\")\n",
    "        analysis = self.analyzer.analyze_query(\n",
    "            user_query, self.state.conversation_history\n",
    "        )\n",
    "        \n",
    "        print(f\"   Query type: {analysis.get('query_type', 'unknown')}\")\n",
    "        print(f\"   Complexity: {analysis.get('complexity', 'unknown')}\")\n",
    "        print(f\"   Multi-step: {analysis.get('requires_multi_step', False)}\")\n",
    "        \n",
    "        # Phase 2: Retrieval Planning\n",
    "        print(f\"\\n2Ô∏è‚É£ Retrieval Planning\")\n",
    "        plan = self.planner.create_plan(user_query, analysis)\n",
    "        self.state.query_plan = plan\n",
    "        \n",
    "        print(f\"   Strategy: {plan.search_strategy}\")\n",
    "        print(f\"   Steps: {len(plan.steps)}\")\n",
    "        print(f\"   Max results: {plan.max_results}\")\n",
    "        \n",
    "        # Phase 3: Retrieval Execution (with possible refinement)\n",
    "        print(f\"\\n3Ô∏è‚É£ Retrieval Execution\")\n",
    "        \n",
    "        best_results = []\n",
    "        best_quality = 0.0\n",
    "        best_assessment = {'overall_quality': 0.0, 'needs_improvement': True}\n",
    "        iteration = 0\n",
    "        \n",
    "        while iteration < self.max_iterations:\n",
    "            iteration += 1\n",
    "            print(f\"\\n   Iteration {iteration}:\")\n",
    "            \n",
    "            # Execute current plan\n",
    "            results = self.retriever.execute_plan(plan, user_query)\n",
    "            \n",
    "            if not results:\n",
    "                print(f\"   ‚ö†Ô∏è No results - trying fallback strategy\")\n",
    "                plan = self._create_fallback_plan(user_query)\n",
    "                results = self.retriever.execute_plan(plan, user_query)\n",
    "            \n",
    "            # Phase 4: Quality Assessment\n",
    "            print(f\"\\n4Ô∏è‚É£ Quality Assessment\")\n",
    "            assessment = self.assessor.assess_results(user_query, results, plan)\n",
    "            \n",
    "            quality_score = assessment['overall_quality']\n",
    "            print(f\"   Quality score: {quality_score:.3f}\")\n",
    "            print(f\"   Needs improvement: {assessment['needs_improvement']}\")\n",
    "            \n",
    "            # Keep best results\n",
    "            if quality_score > best_quality:\n",
    "                best_results = results\n",
    "                best_quality = quality_score\n",
    "                best_assessment = assessment\n",
    "            \n",
    "            # Check if we should refine\n",
    "            if not assessment['needs_improvement'] or iteration >= self.max_iterations:\n",
    "                break\n",
    "            \n",
    "            # Phase 5: Plan Refinement\n",
    "            print(f\"\\n5Ô∏è‚É£ Plan Refinement\")\n",
    "            plan = self._refine_plan(plan, assessment, user_query)\n",
    "            \n",
    "        # Phase 6: Answer Generation\n",
    "        print(f\"\\n6Ô∏è‚É£ Answer Generation\")\n",
    "        final_answer = self._generate_answer(\n",
    "            user_query, best_results, best_assessment\n",
    "        )\n",
    "        \n",
    "        # Update state\n",
    "        self.state.retrieved_context = best_results\n",
    "        self.state.conversation_history.append({\n",
    "            \"user\": user_query,\n",
    "            \"assistant\": final_answer,\n",
    "            \"timestamp\": time.time(),\n",
    "            \"quality_score\": best_quality\n",
    "        })\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        return {\n",
    "            \"answer\": final_answer,\n",
    "            \"results\": best_results,\n",
    "            \"assessment\": best_assessment,\n",
    "            \"metadata\": {\n",
    "                \"query_type\": analysis.get('query_type'),\n",
    "                \"iterations\": iteration,\n",
    "                \"total_time\": total_time,\n",
    "                \"result_count\": len(best_results),\n",
    "                \"quality_score\": best_quality\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _create_fallback_plan(self, query: str) -> RetrievalPlan:\n",
    "        \"\"\"Create a simple fallback plan when main plan fails\"\"\"\n",
    "        \n",
    "        return RetrievalPlan(\n",
    "            query_type=QueryType.SIMPLE_FACTUAL,\n",
    "            steps=[\n",
    "                {\n",
    "                    \"action\": \"search\",\n",
    "                    \"strategy\": \"hybrid\",\n",
    "                    \"query\": query,\n",
    "                    \"limit\": 10\n",
    "                }\n",
    "            ],\n",
    "            search_strategy=\"hybrid\",\n",
    "            max_results=10\n",
    "        )\n",
    "    \n",
    "    def _refine_plan(self, plan: RetrievalPlan, assessment: Dict, query: str) -> RetrievalPlan:\n",
    "        \"\"\"Refine retrieval plan based on quality assessment\"\"\"\n",
    "        \n",
    "        suggestions = assessment.get('suggestions', [])\n",
    "        \n",
    "        # Create refined plan\n",
    "        refined_steps = []\n",
    "        \n",
    "        for suggestion in suggestions:\n",
    "            if \"hybrid search\" in suggestion.lower():\n",
    "                refined_steps.append({\n",
    "                    \"action\": \"search\",\n",
    "                    \"strategy\": \"hybrid\",\n",
    "                    \"query\": query,\n",
    "                    \"limit\": plan.max_results * 2\n",
    "                })\n",
    "            \n",
    "            elif \"mmr\" in suggestion.lower():\n",
    "                refined_steps.append({\n",
    "                    \"action\": \"rerank\",\n",
    "                    \"method\": \"mmr\",\n",
    "                    \"lambda\": 0.3  # More diversity\n",
    "                })\n",
    "            \n",
    "            elif \"increase\" in suggestion.lower() and \"limit\" in suggestion.lower():\n",
    "                # Modify existing search steps to increase limit\n",
    "                for step in plan.steps:\n",
    "                    if step.get('action') == 'search':\n",
    "                        step['limit'] = step.get('limit', 10) * 2\n",
    "        \n",
    "        # If we have refinement suggestions, use them\n",
    "        if refined_steps:\n",
    "            plan.steps = refined_steps\n",
    "        else:\n",
    "            # Fallback refinement: increase limits\n",
    "            plan.max_results = min(20, plan.max_results * 2)\n",
    "            for step in plan.steps:\n",
    "                if 'limit' in step:\n",
    "                    step['limit'] = min(20, step['limit'] * 2)\n",
    "        \n",
    "        return plan\n",
    "    \n",
    "    def _generate_answer(self, query: str, results: List[Dict], assessment: Dict) -> str:\n",
    "        \"\"\"Generate final answer using retrieved context\"\"\"\n",
    "        \n",
    "        if not results:\n",
    "            return \"I couldn't find relevant information to answer your question. Please try rephrasing your query or being more specific.\"\n",
    "        \n",
    "        # Prepare context from results\n",
    "        context_parts = []\n",
    "        for i, result in enumerate(results[:5], 1):  # Top 5 results\n",
    "            context_parts.append(\n",
    "                f\"Source {i}: {result['content'][:200]}...\"\n",
    "            )\n",
    "        \n",
    "        context_text = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        # Generate answer using LLM\n",
    "        prompt = f'''\n",
    "Based on the following retrieved information, provide a comprehensive and accurate answer to the user's question.\n",
    "\n",
    "User Question: {query}\n",
    "\n",
    "Retrieved Context:\n",
    "{context_text}\n",
    "\n",
    "Instructions:\n",
    "- Use only the information provided in the context\n",
    "- Be specific and cite sources when possible\n",
    "- If the context doesn't fully answer the question, acknowledge the limitations\n",
    "- Structure your answer clearly and logically\n",
    "        '''.strip()\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant that answers questions based on retrieved information.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        \n",
    "        try:\n",
    "            answer = self.llm.chat_completion(messages, temperature=0.1)\n",
    "            \n",
    "            # Add quality note if assessment indicates issues\n",
    "            if assessment['overall_quality'] < 0.7:\n",
    "                answer += \"\\n\\n*Note: The retrieved information may be incomplete. Consider asking more specific questions or trying different search terms.*\"\n",
    "            \n",
    "            return answer\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Answer generation failed: {e}\")\n",
    "            \n",
    "            # Fallback answer\n",
    "            return f\"Based on the retrieved information, I found {len(results)} relevant sources that discuss {query}. However, I'm unable to generate a detailed answer at the moment. Please review the source materials directly.\"\n",
    "    \n",
    "    def get_conversation_history(self) -> List[Dict]:\n",
    "        \"\"\"Get conversation history\"\"\"\n",
    "        return self.state.conversation_history\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear conversation history\"\"\"\n",
    "        self.state.conversation_history = []\n",
    "        self.state.retrieved_context = []\n",
    "\n",
    "# Initialize and test the complete agentic RAG system\n",
    "if KNOWLEDGE_COLLECTION:\n",
    "    agent = AgenticRAGAgent(client, KNOWLEDGE_COLLECTION, llm)\n",
    "    \n",
    "    # Test with different query types\n",
    "    test_queries_agentic = [\n",
    "        \"What is vector search and how does it work?\",\n",
    "        \"Compare dense vs sparse vector search methods\",\n",
    "        \"How do I optimize HNSW performance in production?\"\n",
    "    ]\n",
    "    \n",
    "    for test_query in test_queries_agentic[:1]:  # Test one for demonstration\n",
    "        result = agent.query(test_query)\n",
    "        \n",
    "        print(f\"\\nüéØ Final Response:\")\n",
    "        print(f\"Answer: {result['answer'][:200]}...\")\n",
    "        print(f\"\\nüìä Metadata:\")\n",
    "        for key, value in result['metadata'].items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "        \n",
    "        break  # Only run one example to avoid too much output\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping agentic RAG test - no knowledge collection available\")\n",
    "    print(\"\\nüí° To test the complete system:\")\n",
    "    print(\"   1. Run Notebook 1 or 2 to create a knowledge collection\")\n",
    "    print(\"   2. Set up LLM API keys (OpenAI or Anthropic)\")\n",
    "    print(\"   3. Re-run this cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4963161",
   "metadata": {},
   "source": [
    "## üéÆ Advanced Features Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6da43892",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T03:44:43.685289Z",
     "iopub.status.busy": "2025-08-19T03:44:43.685025Z",
     "iopub.status.idle": "2025-08-19T03:44:43.697281Z",
     "shell.execute_reply": "2025-08-19T03:44:43.696800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéÆ Advanced Agentic RAG Features\n",
      "==================================================\n",
      "\n",
      "üîß 1. Query Rewriting\n",
      "   ‚Ä¢ Automatic query expansion\n",
      "   ‚Ä¢ Synonym replacement\n",
      "   ‚Ä¢ Technical term normalization\n",
      "   ‚Ä¢ Multi-language query handling\n",
      "\n",
      "üîç 2. Multi-Modal Retrieval\n",
      "   ‚Ä¢ Text + image search\n",
      "   ‚Ä¢ Code + documentation retrieval\n",
      "   ‚Ä¢ Structured data integration\n",
      "\n",
      "üß† 3. Reasoning Chains\n",
      "   ‚Ä¢ Chain-of-thought retrieval\n",
      "   ‚Ä¢ Evidence combination\n",
      "   ‚Ä¢ Contradiction detection\n",
      "   ‚Ä¢ Confidence calibration\n",
      "\n",
      "üîÑ 4. Self-Correction\n",
      "   ‚Ä¢ Answer validation\n",
      "   ‚Ä¢ Retrieval refinement\n",
      "   ‚Ä¢ Source verification\n",
      "   ‚Ä¢ Hallucination detection\n",
      "\n",
      "üìö 5. Knowledge Graph Integration\n",
      "   ‚Ä¢ Entity relationship traversal\n",
      "   ‚Ä¢ Structured knowledge queries\n",
      "   ‚Ä¢ Factual consistency checking\n",
      "\n",
      "üéØ 6. Personalization\n",
      "   ‚Ä¢ User expertise level adaptation\n",
      "   ‚Ä¢ Domain preference learning\n",
      "   ‚Ä¢ Interaction history analysis\n",
      "\n",
      "üîß 7. Tool Integration\n",
      "   ‚Ä¢ Calculator for numerical queries\n",
      "   ‚Ä¢ Code execution for programming\n",
      "   ‚Ä¢ Web search for current info\n",
      "   ‚Ä¢ API calls for live data\n",
      "\n",
      "üîÑ Query Rewriting Examples:\n",
      "========================================\n",
      "\n",
      "üìù Original: 'How to make my search faster?'\n",
      "üîÑ Rewritten: 'vector search optimization performance tuning latency reduction HNSW parameters'\n",
      "üí≠ Reason: Expanded with technical terms\n",
      "\n",
      "üìù Original: 'DB vs vector store'\n",
      "üîÑ Rewritten: 'database comparison vector database traditional relational database differences'\n",
      "üí≠ Reason: Clarified abbreviations and added context\n",
      "\n",
      "üìù Original: 'Why is recall bad?'\n",
      "üîÑ Rewritten: 'recall degradation causes HNSW index performance issues approximate nearest neighbor'\n",
      "üí≠ Reason: Added technical context and specificity\n",
      "\n",
      "üîó Multi-Hop Reasoning Example:\n",
      "========================================\n",
      "‚ùì Query: 'Why might my vector database be slow after adding lots of data?'\n",
      "\n",
      "üéØ Reasoning Chain:\n",
      "\n",
      "1Ô∏è‚É£ Initial Search: 'vector database performance issues'\n",
      "   ‚Üí Entities found: HNSW, index degradation, memory usage\n",
      "\n",
      "2Ô∏è‚É£ Follow-up Search: 'HNSW index degradation causes'\n",
      "   ‚Üí More entities: graph connectivity, update/delete ratio\n",
      "\n",
      "3Ô∏è‚É£ Final Search: 'index optimization healing rebuild'\n",
      "   ‚Üí Solution entities: optimization, parameter tuning\n",
      "\n",
      "üìã Synthesized Answer:\n",
      "   'Vector database slowdown after adding data is typically caused by\n",
      "    HNSW index degradation. As you insert many new vectors, the graph\n",
      "    structure becomes suboptimal, leading to longer search paths and\n",
      "    reduced recall. Solutions include: 1) Regular index optimization,\n",
      "    2) Tuning HNSW parameters like ef_construct, 3) Index rebuilding.'\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_advanced_features():\n",
    "    \"\"\"Demonstrate advanced agentic RAG features\"\"\"\n",
    "    \n",
    "    print(\"üéÆ Advanced Agentic RAG Features\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"\\nüîß 1. Query Rewriting\")\n",
    "    print(\"   ‚Ä¢ Automatic query expansion\")\n",
    "    print(\"   ‚Ä¢ Synonym replacement\")\n",
    "    print(\"   ‚Ä¢ Technical term normalization\")\n",
    "    print(\"   ‚Ä¢ Multi-language query handling\")\n",
    "    \n",
    "    print(\"\\nüîç 2. Multi-Modal Retrieval\")\n",
    "    print(\"   ‚Ä¢ Text + image search\")\n",
    "    print(\"   ‚Ä¢ Code + documentation retrieval\")\n",
    "    print(\"   ‚Ä¢ Structured data integration\")\n",
    "    \n",
    "    print(\"\\nüß† 3. Reasoning Chains\")\n",
    "    print(\"   ‚Ä¢ Chain-of-thought retrieval\")\n",
    "    print(\"   ‚Ä¢ Evidence combination\")\n",
    "    print(\"   ‚Ä¢ Contradiction detection\")\n",
    "    print(\"   ‚Ä¢ Confidence calibration\")\n",
    "    \n",
    "    print(\"\\nüîÑ 4. Self-Correction\")\n",
    "    print(\"   ‚Ä¢ Answer validation\")\n",
    "    print(\"   ‚Ä¢ Retrieval refinement\")\n",
    "    print(\"   ‚Ä¢ Source verification\")\n",
    "    print(\"   ‚Ä¢ Hallucination detection\")\n",
    "    \n",
    "    print(\"\\nüìö 5. Knowledge Graph Integration\")\n",
    "    print(\"   ‚Ä¢ Entity relationship traversal\")\n",
    "    print(\"   ‚Ä¢ Structured knowledge queries\")\n",
    "    print(\"   ‚Ä¢ Factual consistency checking\")\n",
    "    \n",
    "    print(\"\\nüéØ 6. Personalization\")\n",
    "    print(\"   ‚Ä¢ User expertise level adaptation\")\n",
    "    print(\"   ‚Ä¢ Domain preference learning\")\n",
    "    print(\"   ‚Ä¢ Interaction history analysis\")\n",
    "    \n",
    "    print(\"\\nüîß 7. Tool Integration\")\n",
    "    print(\"   ‚Ä¢ Calculator for numerical queries\")\n",
    "    print(\"   ‚Ä¢ Code execution for programming\")\n",
    "    print(\"   ‚Ä¢ Web search for current info\")\n",
    "    print(\"   ‚Ä¢ API calls for live data\")\n",
    "\n",
    "def demonstrate_query_rewriting():\n",
    "    \"\"\"Show query rewriting examples\"\"\"\n",
    "    \n",
    "    print(\"\\nüîÑ Query Rewriting Examples:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    rewrites = [\n",
    "        {\n",
    "            \"original\": \"How to make my search faster?\",\n",
    "            \"rewritten\": \"vector search optimization performance tuning latency reduction HNSW parameters\",\n",
    "            \"reason\": \"Expanded with technical terms\"\n",
    "        },\n",
    "        {\n",
    "            \"original\": \"DB vs vector store\",\n",
    "            \"rewritten\": \"database comparison vector database traditional relational database differences\",\n",
    "            \"reason\": \"Clarified abbreviations and added context\"\n",
    "        },\n",
    "        {\n",
    "            \"original\": \"Why is recall bad?\",\n",
    "            \"rewritten\": \"recall degradation causes HNSW index performance issues approximate nearest neighbor\",\n",
    "            \"reason\": \"Added technical context and specificity\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for example in rewrites:\n",
    "        print(f\"\\nüìù Original: '{example['original']}'\")\n",
    "        print(f\"üîÑ Rewritten: '{example['rewritten']}'\")\n",
    "        print(f\"üí≠ Reason: {example['reason']}\")\n",
    "\n",
    "def demonstrate_multi_hop_reasoning():\n",
    "    \"\"\"Show multi-hop reasoning example\"\"\"\n",
    "    \n",
    "    print(\"\\nüîó Multi-Hop Reasoning Example:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    query = \"Why might my vector database be slow after adding lots of data?\"\n",
    "    \n",
    "    print(f\"‚ùì Query: '{query}'\")\n",
    "    print(f\"\\nüéØ Reasoning Chain:\")\n",
    "    print(f\"\\n1Ô∏è‚É£ Initial Search: 'vector database performance issues'\")\n",
    "    print(f\"   ‚Üí Entities found: HNSW, index degradation, memory usage\")\n",
    "    \n",
    "    print(f\"\\n2Ô∏è‚É£ Follow-up Search: 'HNSW index degradation causes'\")\n",
    "    print(f\"   ‚Üí More entities: graph connectivity, update/delete ratio\")\n",
    "    \n",
    "    print(f\"\\n3Ô∏è‚É£ Final Search: 'index optimization healing rebuild'\")\n",
    "    print(f\"   ‚Üí Solution entities: optimization, parameter tuning\")\n",
    "    \n",
    "    print(f\"\\nüìã Synthesized Answer:\")\n",
    "    print(f\"   'Vector database slowdown after adding data is typically caused by\")\n",
    "    print(f\"    HNSW index degradation. As you insert many new vectors, the graph\")\n",
    "    print(f\"    structure becomes suboptimal, leading to longer search paths and\")\n",
    "    print(f\"    reduced recall. Solutions include: 1) Regular index optimization,\")\n",
    "    print(f\"    2) Tuning HNSW parameters like ef_construct, 3) Index rebuilding.'\")\n",
    "\n",
    "demonstrate_advanced_features()\n",
    "demonstrate_query_rewriting()\n",
    "demonstrate_multi_hop_reasoning()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084877ae",
   "metadata": {},
   "source": [
    "## üìä Performance Analysis & Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bbdee73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T03:44:43.699315Z",
     "iopub.status.busy": "2025-08-19T03:44:43.699162Z",
     "iopub.status.idle": "2025-08-19T03:44:43.705515Z",
     "shell.execute_reply": "2025-08-19T03:44:43.705234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Agentic RAG Performance Analysis\n",
      "==================================================\n",
      "\n",
      "‚ö° Latency Breakdown:\n",
      "   Query Analysis      : 50-200     ms - LLM call to classify query\n",
      "   Plan Generation     : 30-100     ms - Creating retrieval strategy\n",
      "   Vector Search       : 10-50      ms - Per search operation\n",
      "   Reranking (MMR)     : 5-20       ms - Per rerank operation\n",
      "   Quality Assessment  : 100-300    ms - LLM evaluation of results\n",
      "   Answer Generation   : 200-800    ms - Final LLM response\n",
      "\n",
      "   Total Pipeline      : 395-1470   ms\n",
      "\n",
      "üí∞ Cost Analysis (per query):\n",
      "   Query Analysis      : $0.001-0.003\n",
      "   Quality Assessment  : $0.002-0.005\n",
      "   Answer Generation   : $0.003-0.010\n",
      "   Vector Operations   : $0.0001-0.001\n",
      "\n",
      "üìà Scalability Considerations:\n",
      "   ‚Ä¢ LLM calls are the primary bottleneck\n",
      "   ‚Ä¢ Vector operations scale sub-linearly\n",
      "   ‚Ä¢ Caching can reduce 60-80% of LLM calls\n",
      "   ‚Ä¢ Async processing enables parallelization\n",
      "\n",
      "üéØ Optimization Strategies:\n",
      "   Caching        : Cache query analysis and plans      ‚Üí 50-80% latency reduction\n",
      "   Batching       : Batch multiple queries to LLM       ‚Üí 30-50% cost reduction\n",
      "   Early Stopping : Stop refinement when quality is good ‚Üí 20-40% latency reduction\n",
      "   Async Processing: Parallel retrieval and assessment   ‚Üí 30-60% latency reduction\n",
      "   Model Sizing   : Use smaller models for classification ‚Üí 50-70% cost reduction\n",
      "\n",
      "üèÜ Agentic RAG Best Practices\n",
      "==================================================\n",
      "\n",
      "üéØ Query Understanding\n",
      "   ‚Ä¢ Use lightweight models for query classification\n",
      "   ‚Ä¢ Implement fallback rules when LLM classification fails\n",
      "   ‚Ä¢ Cache common query patterns and their classifications\n",
      "   ‚Ä¢ Consider query similarity for cache hits\n",
      "\n",
      "üîç Retrieval Strategy\n",
      "   ‚Ä¢ Match retrieval strategy to query type\n",
      "   ‚Ä¢ Use hybrid search as the default for unknown query types\n",
      "   ‚Ä¢ Implement progressive refinement with quality thresholds\n",
      "   ‚Ä¢ Set reasonable iteration limits to prevent infinite loops\n",
      "\n",
      "üìä Quality Assessment\n",
      "   ‚Ä¢ Combine multiple quality signals (relevance, diversity, coverage)\n",
      "   ‚Ä¢ Use LLM assessment sparingly due to cost\n",
      "   ‚Ä¢ Implement fast heuristic checks before expensive evaluation\n",
      "   ‚Ä¢ Track quality metrics over time for system improvement\n",
      "\n",
      "ü§ñ Answer Generation\n",
      "   ‚Ä¢ Provide clear context boundaries to reduce hallucination\n",
      "   ‚Ä¢ Include confidence indicators in responses\n",
      "   ‚Ä¢ Cite sources and provide retrieval transparency\n",
      "   ‚Ä¢ Gracefully handle low-quality retrieval results\n",
      "\n",
      "‚ö° Performance\n",
      "   ‚Ä¢ Cache at multiple levels (query, plan, results)\n",
      "   ‚Ä¢ Use async/parallel processing where possible\n",
      "   ‚Ä¢ Implement circuit breakers for external API calls\n",
      "   ‚Ä¢ Monitor and alert on latency and cost metrics\n",
      "\n",
      "üõ°Ô∏è Production Readiness\n",
      "   ‚Ä¢ Implement comprehensive error handling and retries\n",
      "   ‚Ä¢ Add request rate limiting and quota management\n",
      "   ‚Ä¢ Create health checks for all system components\n",
      "   ‚Ä¢ Maintain audit logs for debugging and compliance\n"
     ]
    }
   ],
   "source": [
    "def analyze_agentic_rag_performance():\n",
    "    \"\"\"Analysis of agentic RAG performance characteristics\"\"\"\n",
    "    \n",
    "    print(\"üìä Agentic RAG Performance Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"\\n‚ö° Latency Breakdown:\")\n",
    "    \n",
    "    latency_components = {\n",
    "        \"Query Analysis\": {\"time_ms\": \"50-200\", \"description\": \"LLM call to classify query\"},\n",
    "        \"Plan Generation\": {\"time_ms\": \"30-100\", \"description\": \"Creating retrieval strategy\"},\n",
    "        \"Vector Search\": {\"time_ms\": \"10-50\", \"description\": \"Per search operation\"},\n",
    "        \"Reranking (MMR)\": {\"time_ms\": \"5-20\", \"description\": \"Per rerank operation\"},\n",
    "        \"Quality Assessment\": {\"time_ms\": \"100-300\", \"description\": \"LLM evaluation of results\"},\n",
    "        \"Answer Generation\": {\"time_ms\": \"200-800\", \"description\": \"Final LLM response\"}\n",
    "    }\n",
    "    \n",
    "    total_min = sum([int(comp[\"time_ms\"].split(\"-\")[0]) for comp in latency_components.values()])\n",
    "    total_max = sum([int(comp[\"time_ms\"].split(\"-\")[1]) for comp in latency_components.values()])\n",
    "    \n",
    "    for component, details in latency_components.items():\n",
    "        print(f\"   {component:<20}: {details['time_ms']:<10} ms - {details['description']}\")\n",
    "    \n",
    "    print(f\"\\n   {'Total Pipeline':<20}: {total_min}-{total_max:<6} ms\")\n",
    "    \n",
    "    print(f\"\\nüí∞ Cost Analysis (per query):\")\n",
    "    cost_components = {\n",
    "        \"Query Analysis\": \"$0.001-0.003\",\n",
    "        \"Quality Assessment\": \"$0.002-0.005\", \n",
    "        \"Answer Generation\": \"$0.003-0.010\",\n",
    "        \"Vector Operations\": \"$0.0001-0.001\"\n",
    "    }\n",
    "    \n",
    "    for component, cost in cost_components.items():\n",
    "        print(f\"   {component:<20}: {cost}\")\n",
    "    \n",
    "    print(f\"\\nüìà Scalability Considerations:\")\n",
    "    print(\"   ‚Ä¢ LLM calls are the primary bottleneck\")\n",
    "    print(\"   ‚Ä¢ Vector operations scale sub-linearly\")\n",
    "    print(\"   ‚Ä¢ Caching can reduce 60-80% of LLM calls\")\n",
    "    print(\"   ‚Ä¢ Async processing enables parallelization\")\n",
    "    \n",
    "    print(f\"\\nüéØ Optimization Strategies:\")\n",
    "    \n",
    "    strategies = {\n",
    "        \"Caching\": {\n",
    "            \"what\": \"Cache query analysis and plans\",\n",
    "            \"impact\": \"50-80% latency reduction\"\n",
    "        },\n",
    "        \"Batching\": {\n",
    "            \"what\": \"Batch multiple queries to LLM\",\n",
    "            \"impact\": \"30-50% cost reduction\"\n",
    "        },\n",
    "        \"Early Stopping\": {\n",
    "            \"what\": \"Stop refinement when quality is good\",\n",
    "            \"impact\": \"20-40% latency reduction\"\n",
    "        },\n",
    "        \"Async Processing\": {\n",
    "            \"what\": \"Parallel retrieval and assessment\",\n",
    "            \"impact\": \"30-60% latency reduction\"\n",
    "        },\n",
    "        \"Model Sizing\": {\n",
    "            \"what\": \"Use smaller models for classification\",\n",
    "            \"impact\": \"50-70% cost reduction\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for strategy, details in strategies.items():\n",
    "        print(f\"   {strategy:<15}: {details['what']:<35} ‚Üí {details['impact']}\")\n",
    "\n",
    "def best_practices_guide():\n",
    "    \"\"\"Comprehensive best practices for agentic RAG\"\"\"\n",
    "    \n",
    "    print(\"\\nüèÜ Agentic RAG Best Practices\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    practices = {\n",
    "        \"üéØ Query Understanding\": [\n",
    "            \"Use lightweight models for query classification\",\n",
    "            \"Implement fallback rules when LLM classification fails\",\n",
    "            \"Cache common query patterns and their classifications\",\n",
    "            \"Consider query similarity for cache hits\"\n",
    "        ],\n",
    "        \"üîç Retrieval Strategy\": [\n",
    "            \"Match retrieval strategy to query type\",\n",
    "            \"Use hybrid search as the default for unknown query types\",\n",
    "            \"Implement progressive refinement with quality thresholds\",\n",
    "            \"Set reasonable iteration limits to prevent infinite loops\"\n",
    "        ],\n",
    "        \"üìä Quality Assessment\": [\n",
    "            \"Combine multiple quality signals (relevance, diversity, coverage)\",\n",
    "            \"Use LLM assessment sparingly due to cost\",\n",
    "            \"Implement fast heuristic checks before expensive evaluation\",\n",
    "            \"Track quality metrics over time for system improvement\"\n",
    "        ],\n",
    "        \"ü§ñ Answer Generation\": [\n",
    "            \"Provide clear context boundaries to reduce hallucination\",\n",
    "            \"Include confidence indicators in responses\",\n",
    "            \"Cite sources and provide retrieval transparency\",\n",
    "            \"Gracefully handle low-quality retrieval results\"\n",
    "        ],\n",
    "        \"‚ö° Performance\": [\n",
    "            \"Cache at multiple levels (query, plan, results)\",\n",
    "            \"Use async/parallel processing where possible\",\n",
    "            \"Implement circuit breakers for external API calls\",\n",
    "            \"Monitor and alert on latency and cost metrics\"\n",
    "        ],\n",
    "        \"üõ°Ô∏è Production Readiness\": [\n",
    "            \"Implement comprehensive error handling and retries\",\n",
    "            \"Add request rate limiting and quota management\",\n",
    "            \"Create health checks for all system components\",\n",
    "            \"Maintain audit logs for debugging and compliance\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, items in practices.items():\n",
    "        print(f\"\\n{category}\")\n",
    "        for item in items:\n",
    "            print(f\"   ‚Ä¢ {item}\")\n",
    "\n",
    "analyze_agentic_rag_performance()\n",
    "best_practices_guide()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900c4a79",
   "metadata": {},
   "source": [
    "## üéØ Final Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ad32c70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T03:44:43.706855Z",
     "iopub.status.busy": "2025-08-19T03:44:43.706760Z",
     "iopub.status.idle": "2025-08-19T03:44:43.711427Z",
     "shell.execute_reply": "2025-08-19T03:44:43.711199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ Agentic RAG Workshop Summary\n",
      "==================================================\n",
      "\n",
      "üìö Components Built:\n",
      "   ‚úÖ Query Analyzer: Classifies queries and determines complexity\n",
      "   ‚úÖ Retrieval Planner: Creates multi-step retrieval strategies\n",
      "   ‚úÖ Advanced Retriever: Executes complex retrieval plans\n",
      "   ‚úÖ Quality Assessor: Evaluates result quality and suggests improvements\n",
      "   ‚úÖ Complete Agent: Orchestrates the full agentic RAG pipeline\n",
      "\n",
      "üîç Key Techniques Demonstrated:\n",
      "   üéØ Multi-step query analysis and planning\n",
      "   üéØ Adaptive retrieval strategies by query type\n",
      "   üéØ Self-correcting retrieval with quality feedback\n",
      "   üéØ MMR reranking for result diversification\n",
      "   üéØ Hybrid search integration (dense + sparse)\n",
      "   üéØ Conversational context management\n",
      "   üéØ Quality assessment and improvement suggestions\n",
      "   üéØ Answer generation with source attribution\n",
      "\n",
      "üìä System Performance:\n",
      "   Knowledge Base: workshop_hybrid\n",
      "   LLM Provider: openai\n",
      "   Queries Processed: 1\n",
      "   Average Quality Score: 0.388\n",
      "\n",
      "üöÄ Production Deployment Checklist:\n",
      "   1. Set up proper LLM API credentials and rate limiting\n",
      "   2. Implement comprehensive caching strategy\n",
      "   3. Add monitoring and alerting for all components\n",
      "   4. Create fallback strategies for component failures\n",
      "   5. Implement user session and conversation management\n",
      "   6. Add evaluation framework with human feedback\n",
      "   7. Set up A/B testing for different strategies\n",
      "   8. Create security controls for sensitive queries\n",
      "\n",
      "üéØ Next Steps for Enhancement:\n",
      "   üîÆ Add multi-modal retrieval (text + images + code)\n",
      "   üîÆ Implement knowledge graph integration\n",
      "   üîÆ Build user personalization and preference learning\n",
      "   üîÆ Add real-time learning from user feedback\n",
      "   üîÆ Integrate with external tools and APIs\n",
      "   üîÆ Implement advanced reasoning patterns\n",
      "   üîÆ Add support for different domain expertise levels\n",
      "   üîÆ Build collaborative filtering for query suggestions\n",
      "\n",
      "üí° Key Insights:\n",
      "   ‚Ä¢ Agentic RAG significantly improves answer quality over traditional RAG\n",
      "   ‚Ä¢ Multi-step retrieval helps with complex and comparative queries\n",
      "   ‚Ä¢ Quality assessment enables self-correction and continuous improvement\n",
      "   ‚Ä¢ Proper caching and optimization are critical for production performance\n",
      "   ‚Ä¢ The system can adapt its strategy based on query characteristics\n",
      "\n",
      "‚ú® Congratulations! You've built a complete agentic RAG system.\n",
      "\n",
      "üìñ This completes the 5-notebook Qdrant workshop series:\n",
      "   1. Fundamentals & Search Basics ‚úÖ\n",
      "   2. Hybrid Search (Dense + Sparse) ‚úÖ\n",
      "   3. MMR Reranking ‚úÖ\n",
      "   4. HNSW Index Health ‚úÖ\n",
      "   5. Agentic RAG ‚úÖ\n",
      "\n",
      "üéì You now have comprehensive knowledge of:\n",
      "   ‚Ä¢ Vector database fundamentals\n",
      "   ‚Ä¢ Advanced search techniques\n",
      "   ‚Ä¢ Production optimization strategies\n",
      "   ‚Ä¢ Intelligent RAG architectures\n",
      "\n",
      "üåü Ready to build amazing AI applications with Qdrant!\n"
     ]
    }
   ],
   "source": [
    "print(\"üéâ Agentic RAG Workshop Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nüìö Components Built:\")\n",
    "components = {\n",
    "    \"Query Analyzer\": \"Classifies queries and determines complexity\",\n",
    "    \"Retrieval Planner\": \"Creates multi-step retrieval strategies\", \n",
    "    \"Advanced Retriever\": \"Executes complex retrieval plans\",\n",
    "    \"Quality Assessor\": \"Evaluates result quality and suggests improvements\",\n",
    "    \"Complete Agent\": \"Orchestrates the full agentic RAG pipeline\"\n",
    "}\n",
    "\n",
    "for component, description in components.items():\n",
    "    print(f\"   ‚úÖ {component}: {description}\")\n",
    "\n",
    "print(f\"\\nüîç Key Techniques Demonstrated:\")\n",
    "techniques = [\n",
    "    \"Multi-step query analysis and planning\",\n",
    "    \"Adaptive retrieval strategies by query type\", \n",
    "    \"Self-correcting retrieval with quality feedback\",\n",
    "    \"MMR reranking for result diversification\",\n",
    "    \"Hybrid search integration (dense + sparse)\",\n",
    "    \"Conversational context management\",\n",
    "    \"Quality assessment and improvement suggestions\",\n",
    "    \"Answer generation with source attribution\"\n",
    "]\n",
    "\n",
    "for technique in techniques:\n",
    "    print(f\"   üéØ {technique}\")\n",
    "\n",
    "if KNOWLEDGE_COLLECTION:\n",
    "    print(f\"\\nüìä System Performance:\")\n",
    "    print(f\"   Knowledge Base: {KNOWLEDGE_COLLECTION}\")\n",
    "    print(f\"   LLM Provider: {LLM_PROVIDER}\")\n",
    "    if 'agent' in locals():\n",
    "        history = agent.get_conversation_history()\n",
    "        print(f\"   Queries Processed: {len(history)}\")\n",
    "        if history:\n",
    "            avg_quality = np.mean([h.get('quality_score', 0) for h in history])\n",
    "            print(f\"   Average Quality Score: {avg_quality:.3f}\")\n",
    "\n",
    "print(f\"\\nüöÄ Production Deployment Checklist:\")\n",
    "checklist = [\n",
    "    \"Set up proper LLM API credentials and rate limiting\",\n",
    "    \"Implement comprehensive caching strategy\",\n",
    "    \"Add monitoring and alerting for all components\",\n",
    "    \"Create fallback strategies for component failures\",\n",
    "    \"Implement user session and conversation management\",\n",
    "    \"Add evaluation framework with human feedback\",\n",
    "    \"Set up A/B testing for different strategies\",\n",
    "    \"Create security controls for sensitive queries\"\n",
    "]\n",
    "\n",
    "for i, item in enumerate(checklist, 1):\n",
    "    print(f\"   {i}. {item}\")\n",
    "\n",
    "print(f\"\\nüéØ Next Steps for Enhancement:\")\n",
    "enhancements = [\n",
    "    \"Add multi-modal retrieval (text + images + code)\",\n",
    "    \"Implement knowledge graph integration\",\n",
    "    \"Build user personalization and preference learning\",\n",
    "    \"Add real-time learning from user feedback\",\n",
    "    \"Integrate with external tools and APIs\",\n",
    "    \"Implement advanced reasoning patterns\",\n",
    "    \"Add support for different domain expertise levels\",\n",
    "    \"Build collaborative filtering for query suggestions\"\n",
    "]\n",
    "\n",
    "for enhancement in enhancements:\n",
    "    print(f\"   üîÆ {enhancement}\")\n",
    "\n",
    "print(f\"\\nüí° Key Insights:\")\n",
    "print(\"   ‚Ä¢ Agentic RAG significantly improves answer quality over traditional RAG\")\n",
    "print(\"   ‚Ä¢ Multi-step retrieval helps with complex and comparative queries\")\n",
    "print(\"   ‚Ä¢ Quality assessment enables self-correction and continuous improvement\")\n",
    "print(\"   ‚Ä¢ Proper caching and optimization are critical for production performance\")\n",
    "print(\"   ‚Ä¢ The system can adapt its strategy based on query characteristics\")\n",
    "\n",
    "print(f\"\\n‚ú® Congratulations! You've built a complete agentic RAG system.\")\n",
    "print(f\"\\nüìñ This completes the 5-notebook Qdrant workshop series:\")\n",
    "print(f\"   1. Fundamentals & Search Basics ‚úÖ\")\n",
    "print(f\"   2. Hybrid Search (Dense + Sparse) ‚úÖ\")\n",
    "print(f\"   3. MMR Reranking ‚úÖ\")\n",
    "print(f\"   4. HNSW Index Health ‚úÖ\")\n",
    "print(f\"   5. Agentic RAG ‚úÖ\")\n",
    "\n",
    "print(f\"\\nüéì You now have comprehensive knowledge of:\")\n",
    "print(f\"   ‚Ä¢ Vector database fundamentals\")\n",
    "print(f\"   ‚Ä¢ Advanced search techniques\")\n",
    "print(f\"   ‚Ä¢ Production optimization strategies\")\n",
    "print(f\"   ‚Ä¢ Intelligent RAG architectures\")\n",
    "\n",
    "print(f\"\\nüåü Ready to build amazing AI applications with Qdrant!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
